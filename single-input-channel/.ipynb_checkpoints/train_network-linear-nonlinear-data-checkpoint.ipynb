{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collect-receiver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Loading training and test data\n",
    "import torch\n",
    "\n",
    "data_tr = torch.load(\"training-test-data/training_data_nonlinear.txt\")\n",
    "target_tr = torch.load(\"training-test-data/training_targets_nonlinear.txt\")\n",
    "data_test = torch.load(\"training-test-data/test_data_nonlinear.txt\")\n",
    "target_test = torch.load(\"training-test-data/test_targets_nonlinear.txt\")\n",
    "\n",
    "num_samples = 512\n",
    "num_samples_test = 100 # number of test samples\n",
    "# new_dim1 = 28 * 1 # first dimension\n",
    "# new_dim2 = 28 * 1 # second dimension\n",
    "# old_dim = 28 # MNIST original dimension\n",
    "\n",
    "new_dim1 = 32 # first dimension\n",
    "new_dim2 = 32 # second dimension\n",
    "old_dim = 32 # CIFAR original dimension\n",
    "\n",
    "print(data_tr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "understanding-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Two-layer linear convolutional neural network\n",
    "output_channels = 1\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, ker_size1, ker_size2, output_channels):\n",
    "        super(Net, self).__init__()\n",
    "        self.ker_size1 = ker_size1\n",
    "        self.ker_size2 = ker_size2\n",
    "        self.output_channels = output_channels\n",
    "        self.conv1 = nn.Conv2d(1, output_channels, kernel_size=(self.ker_size1, self.ker_size2), bias=False) \n",
    "        self.fc1 = nn.Linear(int(new_dim1 * new_dim2 * output_channels), 1, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = F.pad(x, (0,self.ker_size2-1,0,self.ker_size1-1), mode='circular') # Circular padding \n",
    "        y1 = self.conv1(y1)\n",
    "#         y1 = F.relu(y1) # ReLU activations\n",
    "        y1 = y1.reshape(y1.size(0), -1)\n",
    "        y1 = self.fc1(y1) \n",
    "        return y1\n",
    "\n",
    "    def initialize(self, initialization_scale, ker_size1):\n",
    "        nn.init.normal_(self.fc1.weight, mean=0.0, std=initialization_scale/np.sqrt(new_dim1))\n",
    "        nn.init.normal_(self.conv1.weight, mean=0.0, std=initialization_scale/np.sqrt(ker_size1))\n",
    "\n",
    "\n",
    "output = torch.zeros((num_samples, 1))\n",
    "output = output.float()\n",
    "output_test = torch.zeros((num_samples_test, 1))\n",
    "output_test = output.float()\n",
    "\n",
    "\n",
    "# Batch gradient descent\n",
    "def train_minibatch(network, optimizer):\n",
    "    minibatch_size = 32\n",
    "    num_batch = int(num_samples/minibatch_size)\n",
    "    for i in range(num_batch):\n",
    "        network.train()\n",
    "        optimizer.zero_grad()\n",
    "        start_index = i * minibatch_size\n",
    "        end_index = start_index + minibatch_size\n",
    "        output = network(data_tr[start_index:end_index])\n",
    "        loss = torch.sum(torch.exp(-1 * torch.mul(output.flatten(), target_tr[start_index:end_index]))) / minibatch_size\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate training data loss\n",
    "def train_eval(network):\n",
    "    network.eval()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        output = network(data_tr)\n",
    "        train_loss = torch.sum(torch.exp(-1 * torch.mul(output.flatten(), target_tr)))\n",
    "        pred = output.apply_(lambda x: 1 if x > 0 else -1)\n",
    "        correct += pred.eq(target_tr.data.view_as(pred)).sum()\n",
    "    train_loss /= num_samples\n",
    "    print('\\nTrain set: Avg. loss: {:.9f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    train_loss, correct, num_samples,\n",
    "    100. * correct / num_samples))\n",
    "    return train_loss\n",
    "\n",
    "def test(network):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        output_test = network(data_test)\n",
    "        test_loss = torch.sum(torch.exp(-1 * torch.mul(output_test.flatten(), target_test)))\n",
    "        pred = output_test.apply_(lambda x: 1 if x > 0 else -1)\n",
    "        correct += pred.eq(target_test.data.view_as(pred)).sum()\n",
    "    test_loss /= num_samples_test\n",
    "    accuracy = 100. * correct / num_samples_test\n",
    "    losses = test_loss\n",
    "    return (accuracy, losses)\n",
    "\n",
    "\n",
    "# Get the information about beta\n",
    "def extract_info(network, show_photo): \n",
    "\n",
    "  # Compute beta for linear CNNs\n",
    "    beta_test = np.zeros((new_dim1,new_dim2))\n",
    "    for i in range(new_dim1):\n",
    "        for j in range(new_dim2):\n",
    "            tempimg = torch.zeros((1,1,new_dim1, new_dim2))\n",
    "            tempimg[0,0,i,j]=1\n",
    "            beta_test[i,j] = network(tempimg)  \n",
    "\n",
    "  # Compute margin\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        output_np = np.ndarray.flatten(network(data_tr).data.numpy())\n",
    "        target_np = np.ndarray.flatten(target_tr.data.numpy())\n",
    "        margins = [target_np[i] * output_np[i] for i in range(num_samples)]\n",
    "        min_margin = min(margins) # get the minimum margin for any datapoint \n",
    "\n",
    "\n",
    "    # Compute R(beta)\n",
    "    w1 = network.conv1.weight.detach().numpy()\n",
    "    w2 = network.fc1.weight.detach().numpy()\n",
    "    w1_norm_sq = np.sum(np.square(w1))\n",
    "    w2_norm_sq = np.sum(np.square(w2))\n",
    "    print(w1_norm_sq, w2_norm_sq)\n",
    "    Rbeta = (np.sum(np.square(w1)) + np.sum(np.square(w2))) * np.sqrt(new_dim1 * new_dim2)\n",
    "\n",
    "\n",
    "    # Normalize by margin \n",
    "    beta_test = beta_test / min_margin # normalize to have margin 1\n",
    "    hat_beta = np.absolute(np.fft.fft2(beta_test,norm='ortho'))\n",
    "    Rbeta = Rbeta / min_margin\n",
    "\n",
    "    print(\"l2 norm: \" + str(2 * np.sqrt(new_dim1 * new_dim2)* np.linalg.norm(beta_test, ord=\"fro\")))\n",
    "    print(\"l1 norm: \" + str(2 * np.sum(hat_beta)))\n",
    "    print(\"Rbeta: \" + str(Rbeta))\n",
    "\n",
    "    if show_photo:\n",
    "        print(\"Time domain:\")\n",
    "        plt.imshow(np.absolute(beta_test), cmap='gray')\n",
    "        plt.show()\n",
    "        print(\"Frequency domain:\")\n",
    "        plt.imshow(np.absolute(hat_beta), cmap='gray', norm=LogNorm(vmin=0.0001, vmax=0.08))\n",
    "        plt.show()\n",
    "  \n",
    "    return (Rbeta, beta_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pleased-projection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "Before training:\n",
      "\n",
      "Train set: Avg. loss: 1.000001907, Accuracy: 269/512 (53%)\n",
      "\n",
      "0.0004523121 0.0029802877\n",
      "l2 norm: 60.26730238387065\n",
      "l1 norm: 53.42195031975183\n",
      "Rbeta: -89.0895449459224\n",
      "Start training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-84acd1ba8b83>:21: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm(range(1, n_epochs + 1)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbbafcc0e4f4ee0b4283bb5e6acc8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.970371246, Accuracy: 298/512 (58%)\n",
      "\n",
      "0.029607642 0.032219507\n",
      "l2 norm: 3.884661991474231\n",
      "l1 norm: 1.6204781059843354\n",
      "Rbeta: -3.888133059499572\n",
      "\n",
      "Train set: Avg. loss: 0.946800828, Accuracy: 314/512 (61%)\n",
      "\n",
      "0.08489104 0.08828798\n",
      "l2 norm: 6.005642108609489\n",
      "l1 norm: 3.1935040865593063\n",
      "Rbeta: -6.006798180157201\n",
      "\n",
      "Train set: Avg. loss: 0.916748583, Accuracy: 345/512 (67%)\n",
      "\n",
      "0.19335179 0.19863716\n",
      "l2 norm: 14.930128042849178\n",
      "l1 norm: 9.384505894979222\n",
      "Rbeta: -14.931485046907213\n",
      "\n",
      "Train set: Avg. loss: 0.872991383, Accuracy: 370/512 (72%)\n",
      "\n",
      "0.40824756 0.4175468\n",
      "l2 norm: 27.007420684464414\n",
      "l1 norm: 18.357450459146932\n",
      "Rbeta: -27.009131852605652\n",
      "\n",
      "Train set: Avg. loss: 0.819148898, Accuracy: 377/512 (74%)\n",
      "\n",
      "0.7422014 0.75860846\n",
      "l2 norm: 53.66121576384612\n",
      "l1 norm: 38.50670550495505\n",
      "Rbeta: -53.66442513532641\n",
      "Learning rate change\n",
      "\n",
      "Train set: Avg. loss: 0.313120693, Accuracy: 499/512 (97%)\n",
      "\n",
      "8.257196 10.511462\n",
      "l2 norm: 814.4612578111305\n",
      "l1 norm: 666.5235576046871\n",
      "Rbeta: -820.4002634048925\n",
      "\n",
      "Train set: Avg. loss: 0.117254660, Accuracy: 511/512 (100%)\n",
      "\n",
      "18.260372 21.838041\n",
      "l2 norm: 87398.46218069614\n",
      "l1 norm: 72300.8921733429\n",
      "Rbeta: -87748.41964016402\n",
      "\n",
      "Train set: Avg. loss: 0.058272988, Accuracy: 512/512 (100%)\n",
      "\n",
      "25.491013 29.45364\n",
      "l2 norm: 1944.2638442482673\n",
      "l1 norm: 1612.9478843611842\n",
      "Rbeta: 1949.34006009359\n",
      "\n",
      "Train set: Avg. loss: 0.035347454, Accuracy: 512/512 (100%)\n",
      "\n",
      "30.667835 34.778534\n",
      "l2 norm: 1434.132251221298\n",
      "l1 norm: 1191.8351928758693\n",
      "Rbeta: 1436.9695182718274\n",
      "\n",
      "Train set: Avg. loss: 0.024513206, Accuracy: 512/512 (100%)\n",
      "\n",
      "34.60447 38.786636\n",
      "l2 norm: 1364.9364905730035\n",
      "l1 norm: 1135.6229162398981\n",
      "Rbeta: 1367.158054742464\n",
      "\n",
      "Train set: Avg. loss: 0.018397342, Accuracy: 512/512 (100%)\n",
      "\n",
      "37.752354 41.975212\n",
      "l2 norm: 1315.3049078077252\n",
      "l1 norm: 1095.1866585164198\n",
      "Rbeta: 1317.1538903900237\n",
      "\n",
      "Train set: Avg. loss: 0.014532367, Accuracy: 512/512 (100%)\n",
      "\n",
      "40.3634 44.612206\n",
      "l2 norm: 1277.2583197709034\n",
      "l1 norm: 1064.1205987222402\n",
      "Rbeta: 1278.8579126391312\n",
      "\n",
      "Train set: Avg. loss: 0.011899658, Accuracy: 512/512 (100%)\n",
      "\n",
      "42.58839 46.854935\n",
      "l2 norm: 1247.1854616745215\n",
      "l1 norm: 1039.5312366525181\n",
      "Rbeta: 1248.6068491526446\n",
      "\n",
      "Train set: Avg. loss: 0.010007170, Accuracy: 512/512 (100%)\n",
      "\n",
      "44.52347 48.80283\n",
      "l2 norm: 1222.8188432703098\n",
      "l1 norm: 1019.5897388729743\n",
      "Rbeta: 1224.1063629950218\n",
      "\n",
      "Train set: Avg. loss: 0.008590260, Accuracy: 512/512 (100%)\n",
      "\n",
      "46.23347 50.522438\n",
      "l2 norm: 1202.6454730050953\n",
      "l1 norm: 1003.069842277433\n",
      "Rbeta: 1203.828726199638\n",
      "\n",
      "Train set: Avg. loss: 0.007495083, Accuracy: 512/512 (100%)\n",
      "\n",
      "47.763863 52.060326\n",
      "l2 norm: 1185.6339839127388\n",
      "l1 norm: 989.1330670369675\n",
      "Rbeta: 1186.733677001513\n",
      "\n",
      "Train set: Avg. loss: 0.006626569, Accuracy: 512/512 (100%)\n",
      "\n",
      "49.147938 53.450344\n",
      "l2 norm: 1171.0614074401649\n",
      "l1 norm: 977.1905661183434\n",
      "Rbeta: 1172.0924190975409\n",
      "\n",
      "Train set: Avg. loss: 0.005923159, Accuracy: 512/512 (100%)\n",
      "\n",
      "50.410492 54.717766\n",
      "l2 norm: 1158.4080995266504\n",
      "l1 norm: 966.8183425738426\n",
      "Rbeta: 1159.3815611009284\n",
      "\n",
      "Train set: Avg. loss: 0.005343311, Accuracy: 512/512 (100%)\n",
      "\n",
      "51.570763 55.88201\n",
      "l2 norm: 1147.2955864154026\n",
      "l1 norm: 957.7073397450797\n",
      "Rbeta: 1148.2201158437397\n",
      "\n",
      "Train set: Avg. loss: 0.004858144, Accuracy: 512/512 (100%)\n",
      "\n",
      "52.643654 56.95822\n",
      "l2 norm: 1137.4385354934943\n",
      "l1 norm: 949.6244278585381\n",
      "Rbeta: 1138.3208527125485\n",
      "\n",
      "Train set: Avg. loss: 0.004446969, Accuracy: 512/512 (100%)\n",
      "\n",
      "53.641052 57.958496\n",
      "l2 norm: 1128.6161477807152\n",
      "l1 norm: 942.3889784851044\n",
      "Rbeta: 1129.4617191141228\n",
      "\n",
      "Train set: Avg. loss: 0.004094571, Accuracy: 512/512 (100%)\n",
      "\n",
      "54.57278 58.892628\n",
      "l2 norm: 1120.6636089488006\n",
      "l1 norm: 935.866210445891\n",
      "Rbeta: 1121.47670113117\n",
      "\n",
      "Train set: Avg. loss: 0.003789615, Accuracy: 512/512 (100%)\n",
      "\n",
      "55.446644 59.768654\n",
      "l2 norm: 1113.446970473372\n",
      "l1 norm: 929.9465101156475\n",
      "Rbeta: 1114.2312212281236\n",
      "\n",
      "Train set: Avg. loss: 0.003523406, Accuracy: 512/512 (100%)\n",
      "\n",
      "56.269398 60.593243\n",
      "l2 norm: 1106.8591338194174\n",
      "l1 norm: 924.5422110930059\n",
      "Rbeta: 1107.6175145080947\n",
      "\n",
      "Train set: Avg. loss: 0.003289249, Accuracy: 512/512 (100%)\n",
      "\n",
      "57.04652 61.371964\n",
      "l2 norm: 1100.8110596536076\n",
      "l1 norm: 919.5803315745372\n",
      "Rbeta: 1101.5462363090405\n",
      "\n",
      "Train set: Avg. loss: 0.003081874, Accuracy: 512/512 (100%)\n",
      "\n",
      "57.782627 62.109577\n",
      "l2 norm: 1095.2344884877748\n",
      "l1 norm: 915.005005164475\n",
      "Rbeta: 1095.9484846992693\n",
      "\n",
      "Train set: Avg. loss: 0.002897086, Accuracy: 512/512 (100%)\n",
      "\n",
      "58.4818 62.81007\n",
      "l2 norm: 1090.0735020085308\n",
      "l1 norm: 910.7704596448492\n",
      "Rbeta: 1090.7682124139521\n",
      "\n",
      "Train set: Avg. loss: 0.002731479, Accuracy: 512/512 (100%)\n",
      "\n",
      "59.147587 63.47698\n",
      "l2 norm: 1085.274001924671\n",
      "l1 norm: 906.8323112744902\n",
      "Rbeta: 1085.9510399327937\n",
      "\n",
      "Train set: Avg. loss: 0.002582324, Accuracy: 512/512 (100%)\n",
      "\n",
      "59.7829 64.11333\n",
      "l2 norm: 1080.7962115955745\n",
      "l1 norm: 903.1579760319271\n",
      "Rbeta: 1081.4569844470243\n",
      "\n",
      "Train set: Avg. loss: 0.002447372, Accuracy: 512/512 (100%)\n",
      "\n",
      "60.390263 64.7217\n",
      "l2 norm: 1076.6080584178612\n",
      "l1 norm: 899.7211896545182\n",
      "Rbeta: 1077.2538780768039\n",
      "\n",
      "Train set: Avg. loss: 0.002324743, Accuracy: 512/512 (100%)\n",
      "\n",
      "60.972088 65.30443\n",
      "l2 norm: 1072.6766812678843\n",
      "l1 norm: 896.4950589222688\n",
      "Rbeta: 1073.3085429952637\n",
      "\n",
      "Train set: Avg. loss: 0.002212863, Accuracy: 512/512 (100%)\n",
      "\n",
      "61.530495 65.86355\n",
      "l2 norm: 1068.9753904507895\n",
      "l1 norm: 893.4575727205854\n",
      "Rbeta: 1069.5942487194268\n",
      "\n",
      "Train set: Avg. loss: 0.002110445, Accuracy: 512/512 (100%)\n",
      "\n",
      "62.06705 66.40088\n",
      "l2 norm: 1065.4853153436932\n",
      "l1 norm: 890.5933622249092\n",
      "Rbeta: 1066.0921101752856\n",
      "\n",
      "Train set: Avg. loss: 0.002016343, Accuracy: 512/512 (100%)\n",
      "\n",
      "62.583706 66.918015\n",
      "l2 norm: 1062.1854886016235\n",
      "l1 norm: 887.8852457022655\n",
      "Rbeta: 1062.7809335880047\n",
      "\n",
      "Train set: Avg. loss: 0.001929673, Accuracy: 512/512 (100%)\n",
      "\n",
      "63.08119 67.41641\n",
      "l2 norm: 1059.0617152030682\n",
      "l1 norm: 885.3216022592073\n",
      "Rbeta: 1059.6466278851549\n",
      "\n",
      "Train set: Avg. loss: 0.001849547, Accuracy: 512/512 (100%)\n",
      "\n",
      "63.56171 67.89732\n",
      "l2 norm: 1056.0935271794588\n",
      "l1 norm: 882.8855577983702\n",
      "Rbeta: 1056.668378579569\n",
      "\n",
      "Train set: Avg. loss: 0.001775335, Accuracy: 512/512 (100%)\n",
      "\n",
      "64.02557 68.36192\n",
      "l2 norm: 1053.270691330483\n",
      "l1 norm: 880.568751253551\n",
      "Rbeta: 1053.8362015669757\n",
      "\n",
      "Train set: Avg. loss: 0.001706321, Accuracy: 512/512 (100%)\n",
      "\n",
      "64.47524 68.811295\n",
      "l2 norm: 1050.5793071672858\n",
      "l1 norm: 878.3598076463145\n",
      "Rbeta: 1051.1355789472696\n",
      "\n",
      "Train set: Avg. loss: 0.001642165, Accuracy: 512/512 (100%)\n",
      "\n",
      "64.909256 69.24634\n",
      "l2 norm: 1048.0122962001471\n",
      "l1 norm: 876.2529261054224\n",
      "Rbeta: 1048.5603689504158\n",
      "\n",
      "Train set: Avg. loss: 0.001582212, Accuracy: 512/512 (100%)\n",
      "\n",
      "65.3311 69.66797\n",
      "l2 norm: 1045.5558263187183\n",
      "l1 norm: 874.2366751813258\n",
      "Rbeta: 1046.095795067794\n",
      "\n",
      "Train set: Avg. loss: 0.001526264, Accuracy: 512/512 (100%)\n",
      "\n",
      "65.73867 70.07692\n",
      "l2 norm: 1043.213718559244\n",
      "l1 norm: 872.3143717022294\n",
      "Rbeta: 1043.7462334319964\n",
      "\n",
      "Train set: Avg. loss: 0.001473714, Accuracy: 512/512 (100%)\n",
      "\n",
      "66.13636 70.474\n",
      "l2 norm: 1040.9601331028648\n",
      "l1 norm: 870.4645921919408\n",
      "Rbeta: 1041.485243203476\n",
      "\n",
      "Train set: Avg. loss: 0.001424475, Accuracy: 512/512 (100%)\n",
      "\n",
      "66.52139 70.85983\n",
      "l2 norm: 1038.8035067279923\n",
      "l1 norm: 868.6945112905505\n",
      "Rbeta: 1039.32187251358\n",
      "\n",
      "Train set: Avg. loss: 0.001378167, Accuracy: 512/512 (100%)\n",
      "\n",
      "66.89604 71.234955\n",
      "l2 norm: 1036.7286514847374\n",
      "l1 norm: 866.9914318418614\n",
      "Rbeta: 1037.2405081080935\n",
      "\n",
      "Train set: Avg. loss: 0.001334515, Accuracy: 512/512 (100%)\n",
      "\n",
      "67.26121 71.60002\n",
      "l2 norm: 1034.738307163506\n",
      "l1 norm: 865.3578801350773\n",
      "Rbeta: 1035.2438341311572\n",
      "\n",
      "Train set: Avg. loss: 0.001293342, Accuracy: 512/512 (100%)\n",
      "\n",
      "67.61678 71.95551\n",
      "l2 norm: 1032.817058756441\n",
      "l1 norm: 863.7808966106776\n",
      "Rbeta: 1033.3164440776918\n",
      "\n",
      "Train set: Avg. loss: 0.001254538, Accuracy: 512/512 (100%)\n",
      "\n",
      "67.96152 72.30198\n",
      "l2 norm: 1030.9659238264444\n",
      "l1 norm: 862.261455874075\n",
      "Rbeta: 1031.4598801248237\n",
      "\n",
      "Train set: Avg. loss: 0.001217685, Accuracy: 512/512 (100%)\n",
      "\n",
      "68.300285 72.6398\n",
      "l2 norm: 1029.177282576795\n",
      "l1 norm: 860.7932558688674\n",
      "Rbeta: 1029.6655488939646\n",
      "\n",
      "Train set: Avg. loss: 0.001182814, Accuracy: 512/512 (100%)\n",
      "\n",
      "68.63008 72.969376\n",
      "l2 norm: 1027.4550328143253\n",
      "l1 norm: 859.3795406700538\n",
      "Rbeta: 1027.9377913750866\n",
      "\n",
      "Train set: Avg. loss: 0.001149859, Accuracy: 512/512 (100%)\n",
      "\n",
      "68.94965 73.29109\n",
      "l2 norm: 1025.7919295452987\n",
      "l1 norm: 858.0144243928612\n",
      "Rbeta: 1026.2701072004145\n",
      "\n",
      "Train set: Avg. loss: 0.001118387, Accuracy: 512/512 (100%)\n",
      "\n",
      "69.265236 73.60537\n",
      "l2 norm: 1024.178208713596\n",
      "l1 norm: 856.6898418775911\n",
      "Rbeta: 1024.6511337023462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.001088505, Accuracy: 512/512 (100%)\n",
      "\n",
      "69.57286 73.91246\n",
      "l2 norm: 1022.620917872764\n",
      "l1 norm: 855.41156680021\n",
      "Rbeta: 1023.0889149270471\n",
      "\n",
      "Train set: Avg. loss: 0.001060192, Accuracy: 512/512 (100%)\n",
      "\n",
      "69.87078 74.212746\n",
      "l2 norm: 1021.1109029206381\n",
      "l1 norm: 854.1720831202529\n",
      "Rbeta: 1021.5748753786817\n",
      "\n",
      "Train set: Avg. loss: 0.001033091, Accuracy: 512/512 (100%)\n",
      "\n",
      "70.164825 74.506516\n",
      "l2 norm: 1019.6482593415884\n",
      "l1 norm: 852.971532095604\n",
      "Rbeta: 1020.1077194015103\n",
      "\n",
      "Train set: Avg. loss: 0.001007137, Accuracy: 512/512 (100%)\n",
      "\n",
      "70.45493 74.79401\n",
      "l2 norm: 1018.2269481852112\n",
      "l1 norm: 851.8048278980029\n",
      "Rbeta: 1018.6815573892131\n",
      "\n",
      "Train set: Avg. loss: 0.000982555, Accuracy: 512/512 (100%)\n",
      "\n",
      "70.734436 75.075485\n",
      "l2 norm: 1016.8488998757323\n",
      "l1 norm: 850.6736779177853\n",
      "Rbeta: 1017.2999023260335\n",
      "\n",
      "Train set: Avg. loss: 0.000959100, Accuracy: 512/512 (100%)\n",
      "\n",
      "71.007164 75.35124\n",
      "l2 norm: 1015.507184625201\n",
      "l1 norm: 849.5721694254287\n",
      "Rbeta: 1015.9547623529899\n",
      "\n",
      "Train set: Avg. loss: 0.000936494, Accuracy: 512/512 (100%)\n",
      "\n",
      "71.278465 75.621475\n",
      "l2 norm: 1014.2048227519696\n",
      "l1 norm: 848.5030771887072\n",
      "Rbeta: 1014.6483253078825\n",
      "\n",
      "Train set: Avg. loss: 0.000914799, Accuracy: 512/512 (100%)\n",
      "\n",
      "71.5458 75.88634\n",
      "l2 norm: 1012.9381501659736\n",
      "l1 norm: 847.4632512983746\n",
      "Rbeta: 1013.3773857738405\n",
      "\n",
      "Train set: Avg. loss: 0.000894068, Accuracy: 512/512 (100%)\n",
      "\n",
      "71.806526 76.14607\n",
      "l2 norm: 1011.7071148053109\n",
      "l1 norm: 846.452794343799\n",
      "Rbeta: 1012.1426693717128\n",
      "\n",
      "Train set: Avg. loss: 0.000874358, Accuracy: 512/512 (100%)\n",
      "\n",
      "72.057915 76.40082\n",
      "l2 norm: 1010.51221727883\n",
      "l1 norm: 845.4719660389488\n",
      "Rbeta: 1010.9449035949644\n",
      "\n",
      "Train set: Avg. loss: 0.000855388, Accuracy: 512/512 (100%)\n",
      "\n",
      "72.30585 76.65092\n",
      "l2 norm: 1009.3434073481425\n",
      "l1 norm: 844.5125929613972\n",
      "Rbeta: 1009.7730224646122\n",
      "\n",
      "Train set: Avg. loss: 0.000837044, Accuracy: 512/512 (100%)\n",
      "\n",
      "72.55263 76.89645\n",
      "l2 norm: 1008.2114506299633\n",
      "l1 norm: 843.5835187247676\n",
      "Rbeta: 1008.6375335773125\n",
      "\n",
      "Train set: Avg. loss: 0.000819423, Accuracy: 512/512 (100%)\n",
      "\n",
      "72.79464 77.13759\n",
      "l2 norm: 1007.0999617294088\n",
      "l1 norm: 842.6711100864841\n",
      "Rbeta: 1007.5226381606253\n",
      "\n",
      "Train set: Avg. loss: 0.000802469, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.03264 77.374405\n",
      "l2 norm: 1006.0220331422346\n",
      "l1 norm: 841.7862444339472\n",
      "Rbeta: 1006.4414737835431\n",
      "\n",
      "Train set: Avg. loss: 0.000786163, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.26614 77.60712\n",
      "l2 norm: 1004.9608424706688\n",
      "l1 norm: 840.9150633212836\n",
      "Rbeta: 1005.3770521494756\n",
      "\n",
      "Train set: Avg. loss: 0.000770508, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.4942 77.83587\n",
      "l2 norm: 1003.9303742693282\n",
      "l1 norm: 840.0692282034155\n",
      "Rbeta: 1004.3438570854694\n",
      "\n",
      "Train set: Avg. loss: 0.000755454, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.71742 78.060776\n",
      "l2 norm: 1002.9261746481493\n",
      "l1 norm: 839.2449194965598\n",
      "Rbeta: 1003.3370943139556\n",
      "\n",
      "Train set: Avg. loss: 0.000740949, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.9365 78.282005\n",
      "l2 norm: 1001.9439849945885\n",
      "l1 norm: 838.4386557352682\n",
      "Rbeta: 1002.3525122958589\n",
      "\n",
      "Train set: Avg. loss: 0.000726916, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.15315 78.49965\n",
      "l2 norm: 1000.9863282391728\n",
      "l1 norm: 837.6525831825093\n",
      "Rbeta: 1001.392369506805\n",
      "\n",
      "Train set: Avg. loss: 0.000713274, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.36937 78.71377\n",
      "l2 norm: 1000.0395491349232\n",
      "l1 norm: 836.875277450829\n",
      "Rbeta: 1000.4424072705657\n",
      "\n",
      "Train set: Avg. loss: 0.000700141, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.58076 78.9245\n",
      "l2 norm: 999.1139433229778\n",
      "l1 norm: 836.1153283357693\n",
      "Rbeta: 999.5141817306825\n",
      "\n",
      "Train set: Avg. loss: 0.000687426, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.789696 79.131874\n",
      "l2 norm: 998.2148603647623\n",
      "l1 norm: 835.3773178210463\n",
      "Rbeta: 998.6122869459899\n",
      "\n",
      "Train set: Avg. loss: 0.000675142, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.99502 79.336136\n",
      "l2 norm: 997.3386778724646\n",
      "l1 norm: 834.6580903899862\n",
      "Rbeta: 997.7334367578644\n",
      "\n",
      "Train set: Avg. loss: 0.000663280, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.19649 79.53732\n",
      "l2 norm: 996.470215018239\n",
      "l1 norm: 833.9450100779402\n",
      "Rbeta: 996.8625586702573\n",
      "\n",
      "Train set: Avg. loss: 0.000651887, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.391914 79.73546\n",
      "l2 norm: 995.6275242291391\n",
      "l1 norm: 833.2532339296595\n",
      "Rbeta: 996.0180290128952\n",
      "\n",
      "Train set: Avg. loss: 0.000640841, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.58475 79.93082\n",
      "l2 norm: 994.7977611529271\n",
      "l1 norm: 832.5719653020603\n",
      "Rbeta: 995.186478587868\n",
      "\n",
      "Train set: Avg. loss: 0.000630086, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.77676 80.12334\n",
      "l2 norm: 993.9965318037222\n",
      "l1 norm: 831.9144865646193\n",
      "Rbeta: 994.3830191032048\n",
      "\n",
      "Train set: Avg. loss: 0.000619657, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.966156 80.31311\n",
      "l2 norm: 993.2040490697487\n",
      "l1 norm: 831.2638987512556\n",
      "Rbeta: 993.5884980199709\n",
      "\n",
      "Train set: Avg. loss: 0.000609511, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.15412 80.50025\n",
      "l2 norm: 992.4224547233958\n",
      "l1 norm: 830.6221792569143\n",
      "Rbeta: 992.8045775416018\n",
      "\n",
      "Train set: Avg. loss: 0.000599648, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.340225 80.684814\n",
      "l2 norm: 991.6599015629077\n",
      "l1 norm: 829.9963368755106\n",
      "Rbeta: 992.0396960026375\n",
      "\n",
      "Train set: Avg. loss: 0.000590070, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.52403 80.86684\n",
      "l2 norm: 990.9080257774613\n",
      "l1 norm: 829.3791577977315\n",
      "Rbeta: 991.2854043358075\n",
      "\n",
      "Train set: Avg. loss: 0.000580820, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.70349 81.04638\n",
      "l2 norm: 990.1695161396447\n",
      "l1 norm: 828.7727750679112\n",
      "Rbeta: 990.5450080070437\n",
      "\n",
      "Train set: Avg. loss: 0.000571865, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.87941 81.22351\n",
      "l2 norm: 989.4483200276641\n",
      "l1 norm: 828.1807579507258\n",
      "Rbeta: 989.8220448583556\n",
      "\n",
      "Train set: Avg. loss: 0.000563111, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.05511 81.3983\n",
      "l2 norm: 988.7343153902424\n",
      "l1 norm: 827.594604360261\n",
      "Rbeta: 989.1059335690813\n",
      "\n",
      "Train set: Avg. loss: 0.000554631, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.22739 81.57082\n",
      "l2 norm: 988.0392332146773\n",
      "l1 norm: 827.0239915705567\n",
      "Rbeta: 988.4091043577575\n",
      "\n",
      "Train set: Avg. loss: 0.000546416, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.39612 81.74111\n",
      "l2 norm: 987.3591548513497\n",
      "l1 norm: 826.46591493549\n",
      "Rbeta: 987.7274182445351\n",
      "\n",
      "Train set: Avg. loss: 0.000538394, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.56401 81.9093\n",
      "l2 norm: 986.688217304663\n",
      "l1 norm: 825.9152156215853\n",
      "Rbeta: 987.0547326517457\n",
      "\n",
      "Train set: Avg. loss: 0.000530584, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.72996 82.07538\n",
      "l2 norm: 986.0288312304235\n",
      "l1 norm: 825.3739440959401\n",
      "Rbeta: 986.3934847936331\n",
      "\n",
      "Train set: Avg. loss: 0.000522999, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.893166 82.23936\n",
      "l2 norm: 985.3777651469071\n",
      "l1 norm: 824.8395428385675\n",
      "Rbeta: 985.7408981906684\n",
      "\n",
      "Train set: Avg. loss: 0.000515623, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.05394 82.401344\n",
      "l2 norm: 984.7388586601805\n",
      "l1 norm: 824.3151883576776\n",
      "Rbeta: 985.100531377472\n",
      "\n",
      "Train set: Avg. loss: 0.000508407, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.2141 82.56145\n",
      "l2 norm: 984.1152994600312\n",
      "l1 norm: 823.8033139282342\n",
      "Rbeta: 984.4751833322537\n",
      "\n",
      "Train set: Avg. loss: 0.000501344, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.37388 82.719635\n",
      "l2 norm: 983.493676531853\n",
      "l1 norm: 823.2929434337319\n",
      "Rbeta: 983.8516669269449\n",
      "\n",
      "Train set: Avg. loss: 0.000494474, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.531044 82.875984\n",
      "l2 norm: 982.8730695362493\n",
      "l1 norm: 822.7834061757095\n",
      "Rbeta: 983.2293330481879\n",
      "\n",
      "Train set: Avg. loss: 0.000487816, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.68466 83.03047\n",
      "l2 norm: 982.2831017324312\n",
      "l1 norm: 822.2992346852039\n",
      "Rbeta: 982.6379572393648\n",
      "\n",
      "Train set: Avg. loss: 0.000481311, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.837006 83.18321\n",
      "l2 norm: 981.6927397087978\n",
      "l1 norm: 821.8147302975694\n",
      "Rbeta: 982.0461263027948\n",
      "\n",
      "Train set: Avg. loss: 0.000474927, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.9895 83.33419\n",
      "l2 norm: 981.106657677107\n",
      "l1 norm: 821.3334670929887\n",
      "Rbeta: 981.4583292639549\n",
      "\n",
      "Train set: Avg. loss: 0.000468729, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.13876 83.483475\n",
      "l2 norm: 980.54064913543\n",
      "l1 norm: 820.8689084703637\n",
      "Rbeta: 980.8907712859685\n",
      "\n",
      "Train set: Avg. loss: 0.000462678, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.28645 83.63107\n",
      "l2 norm: 979.97468833951\n",
      "l1 norm: 820.4042954407269\n",
      "Rbeta: 980.3233536187834\n",
      "\n",
      "Train set: Avg. loss: 0.000456738, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.43428 83.77696\n",
      "l2 norm: 979.4228389998307\n",
      "l1 norm: 819.9512943291518\n",
      "Rbeta: 979.7697151562692\n",
      "\n",
      "Train set: Avg. loss: 0.000450943, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.58025 83.9212\n",
      "l2 norm: 978.8734259688241\n",
      "l1 norm: 819.5001355092616\n",
      "Rbeta: 979.2186461034632\n",
      "\n",
      "Train set: Avg. loss: 0.000445322, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.72256 84.06399\n",
      "l2 norm: 978.3359201739278\n",
      "l1 norm: 819.0587778031639\n",
      "Rbeta: 978.6797656766429\n",
      "\n",
      "Train set: Avg. loss: 0.000439911, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.85906 84.205246\n",
      "l2 norm: 977.8046367536596\n",
      "l1 norm: 818.6225850498932\n",
      "Rbeta: 978.1478226105929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000434597, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.99548 84.34506\n",
      "l2 norm: 977.285489745398\n",
      "l1 norm: 818.1964258065693\n",
      "Rbeta: 977.6279848184856\n",
      "\n",
      "Train set: Avg. loss: 0.000429372, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.13201 84.48343\n",
      "l2 norm: 976.762407601126\n",
      "l1 norm: 817.766919318811\n",
      "Rbeta: 977.1038536991797\n",
      "\n",
      "Train set: Avg. loss: 0.000424237, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.26866 84.620255\n",
      "l2 norm: 976.2516672838177\n",
      "l1 norm: 817.3476662078424\n",
      "Rbeta: 976.5918060073747\n",
      "\n",
      "Train set: Avg. loss: 0.000419186, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.40543 84.75574\n",
      "l2 norm: 975.7500005183039\n",
      "l1 norm: 816.9358604140374\n",
      "Rbeta: 976.0886127260937\n",
      "\n",
      "Train set: Avg. loss: 0.000414238, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.54125 84.88984\n",
      "l2 norm: 975.2530857894396\n",
      "l1 norm: 816.5279057496213\n",
      "Rbeta: 975.5902024558186\n",
      "\n",
      "Train set: Avg. loss: 0.000409414, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.67482 85.02256\n",
      "l2 norm: 974.7727878476167\n",
      "l1 norm: 816.1337957280412\n",
      "Rbeta: 975.1085494285321\n",
      "\n",
      "Train set: Avg. loss: 0.000404674, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.80806 85.15401\n",
      "l2 norm: 974.2875067190794\n",
      "l1 norm: 815.7354497955355\n",
      "Rbeta: 974.6217194483816\n",
      "\n",
      "Train set: Avg. loss: 0.000400031, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.94013 85.28426\n",
      "l2 norm: 973.8049240077361\n",
      "l1 norm: 815.3390683232303\n",
      "Rbeta: 974.1376138649207\n",
      "\n",
      "Train set: Avg. loss: 0.000395509, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.06967 85.41313\n",
      "l2 norm: 973.3278373068315\n",
      "l1 norm: 814.9472696016843\n",
      "Rbeta: 973.6592212885276\n",
      "\n",
      "Train set: Avg. loss: 0.000391121, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.19571 85.540665\n",
      "l2 norm: 972.8632751885152\n",
      "l1 norm: 814.565787167609\n",
      "Rbeta: 973.1937355465645\n",
      "\n",
      "Train set: Avg. loss: 0.000386811, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.32122 85.66704\n",
      "l2 norm: 972.4067732483929\n",
      "l1 norm: 814.1910119941551\n",
      "Rbeta: 972.7361982361158\n",
      "\n",
      "Train set: Avg. loss: 0.000382568, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.44683 85.79221\n",
      "l2 norm: 971.9540799809871\n",
      "l1 norm: 813.8192674913985\n",
      "Rbeta: 972.2823390312473\n",
      "\n",
      "Train set: Avg. loss: 0.000378410, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.57123 85.91623\n",
      "l2 norm: 971.5085550488205\n",
      "l1 norm: 813.4535862819623\n",
      "Rbeta: 971.835606735542\n",
      "\n",
      "Train set: Avg. loss: 0.000374348, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.69358 86.039215\n",
      "l2 norm: 971.0770832277921\n",
      "l1 norm: 813.0997616985734\n",
      "Rbeta: 971.4031017291566\n",
      "\n",
      "Train set: Avg. loss: 0.000370365, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.81506 86.160904\n",
      "l2 norm: 970.632996131408\n",
      "l1 norm: 812.7349842366931\n",
      "Rbeta: 970.9579668987972\n",
      "\n",
      "Train set: Avg. loss: 0.000366452, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.935875 86.28154\n",
      "l2 norm: 970.2127323684113\n",
      "l1 norm: 812.3901013428023\n",
      "Rbeta: 970.5366125080976\n",
      "\n",
      "Train set: Avg. loss: 0.000362599, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.05674 86.40098\n",
      "l2 norm: 969.8058204031456\n",
      "l1 norm: 812.0563144195028\n",
      "Rbeta: 970.1284756625317\n",
      "\n",
      "Train set: Avg. loss: 0.000358840, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.17512 86.519485\n",
      "l2 norm: 969.3788702703715\n",
      "l1 norm: 811.705821633366\n",
      "Rbeta: 969.7005346375164\n",
      "\n",
      "Train set: Avg. loss: 0.000355172, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.29131 86.63686\n",
      "l2 norm: 968.9620832880536\n",
      "l1 norm: 811.3635022890364\n",
      "Rbeta: 969.2828457568805\n",
      "\n",
      "Train set: Avg. loss: 0.000351562, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.40728 86.75319\n",
      "l2 norm: 968.5606586958567\n",
      "l1 norm: 811.0341223756816\n",
      "Rbeta: 968.8804133009832\n",
      "\n",
      "Train set: Avg. loss: 0.000348002, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.52333 86.868515\n",
      "l2 norm: 968.1481613507817\n",
      "l1 norm: 810.6954719400264\n",
      "Rbeta: 968.4668442407345\n",
      "\n",
      "Train set: Avg. loss: 0.000344508, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.638504 86.98279\n",
      "l2 norm: 967.7578453634881\n",
      "l1 norm: 810.3753328860377\n",
      "Rbeta: 968.0753913963517\n",
      "\n",
      "Train set: Avg. loss: 0.000341096, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.7514 87.09613\n",
      "l2 norm: 967.3598711231183\n",
      "l1 norm: 810.0486162141781\n",
      "Rbeta: 967.6765026342613\n",
      "\n",
      "Train set: Avg. loss: 0.000337753, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.86311 87.20834\n",
      "l2 norm: 966.9593223307555\n",
      "l1 norm: 809.7197070788404\n",
      "Rbeta: 967.2750746264356\n",
      "\n",
      "Train set: Avg. loss: 0.000334476, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.97338 87.31977\n",
      "l2 norm: 966.5711013887345\n",
      "l1 norm: 809.4010877353865\n",
      "Rbeta: 966.8861078895243\n",
      "\n",
      "Train set: Avg. loss: 0.000331276, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.081276 87.430405\n",
      "l2 norm: 966.1969119037856\n",
      "l1 norm: 809.0942994174959\n",
      "Rbeta: 966.5114182156285\n",
      "\n",
      "Train set: Avg. loss: 0.000328122, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.189095 87.54003\n",
      "l2 norm: 965.8192226073697\n",
      "l1 norm: 808.7843575861287\n",
      "Rbeta: 966.1329819398134\n",
      "\n",
      "Train set: Avg. loss: 0.000325010, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.29699 87.6488\n",
      "l2 norm: 965.4597675426546\n",
      "l1 norm: 808.4894362090979\n",
      "Rbeta: 965.7727944526554\n",
      "\n",
      "Train set: Avg. loss: 0.000321940, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.40495 87.75661\n",
      "l2 norm: 965.1027602460166\n",
      "l1 norm: 808.196649202164\n",
      "Rbeta: 965.4147891515101\n",
      "\n",
      "Train set: Avg. loss: 0.000318912, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.512985 87.86351\n",
      "l2 norm: 964.736623908822\n",
      "l1 norm: 807.8962018227196\n",
      "Rbeta: 965.0476411680913\n",
      "\n",
      "Train set: Avg. loss: 0.000315944, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.61951 87.96959\n",
      "l2 norm: 964.3721612517317\n",
      "l1 norm: 807.5968921744616\n",
      "Rbeta: 964.6822238053845\n",
      "\n",
      "Train set: Avg. loss: 0.000313017, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.72593 88.0748\n",
      "l2 norm: 964.017972511217\n",
      "l1 norm: 807.3061419333757\n",
      "Rbeta: 964.3269424607345\n",
      "\n",
      "Train set: Avg. loss: 0.000310128, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.83243 88.1791\n",
      "l2 norm: 963.660238873105\n",
      "l1 norm: 807.0123500385245\n",
      "Rbeta: 963.9681028277291\n",
      "\n",
      "Train set: Avg. loss: 0.000307299, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.9374 88.28248\n",
      "l2 norm: 963.3045994357827\n",
      "l1 norm: 806.7201475374088\n",
      "Rbeta: 963.6113595193166\n",
      "\n",
      "Train set: Avg. loss: 0.000304528, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.040825 88.38507\n",
      "l2 norm: 962.9487624602209\n",
      "l1 norm: 806.4278486449149\n",
      "Rbeta: 963.2545717584836\n",
      "\n",
      "Train set: Avg. loss: 0.000301817, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.14227 88.48693\n",
      "l2 norm: 962.6127643378787\n",
      "l1 norm: 806.1522709384751\n",
      "Rbeta: 962.9178432642299\n",
      "\n",
      "Train set: Avg. loss: 0.000299164, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.24194 88.587975\n",
      "l2 norm: 962.2639983060545\n",
      "l1 norm: 805.8657523210688\n",
      "Rbeta: 962.568413330069\n",
      "\n",
      "Train set: Avg. loss: 0.000296565, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.340004 88.6883\n",
      "l2 norm: 961.9205159493154\n",
      "l1 norm: 805.5836681804693\n",
      "Rbeta: 962.2243665141535\n",
      "\n",
      "Train set: Avg. loss: 0.000293996, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.438126 88.78802\n",
      "l2 norm: 961.57859020677\n",
      "l1 norm: 805.3028817304491\n",
      "Rbeta: 961.8818839763677\n",
      "\n",
      "Train set: Avg. loss: 0.000291459, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.5363 88.88692\n",
      "l2 norm: 961.2555879136082\n",
      "l1 norm: 805.0376476784807\n",
      "Rbeta: 961.5582063922583\n",
      "\n",
      "Train set: Avg. loss: 0.000288964, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.633575 88.98514\n",
      "l2 norm: 960.9359046797438\n",
      "l1 norm: 804.7753323160396\n",
      "Rbeta: 961.2378763221259\n",
      "\n",
      "Train set: Avg. loss: 0.000286510, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.73011 89.08253\n",
      "l2 norm: 960.6299801937557\n",
      "l1 norm: 804.5245559873913\n",
      "Rbeta: 960.931322603053\n",
      "\n",
      "Train set: Avg. loss: 0.000284084, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.8267 89.179276\n",
      "l2 norm: 960.310933839839\n",
      "l1 norm: 804.2627470506593\n",
      "Rbeta: 960.6115406876977\n",
      "\n",
      "Train set: Avg. loss: 0.000281688, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.92335 89.27524\n",
      "l2 norm: 959.9942659932827\n",
      "l1 norm: 804.0027470672838\n",
      "Rbeta: 960.2939230317056\n",
      "\n",
      "Train set: Avg. loss: 0.000279321, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.02005 89.37042\n",
      "l2 norm: 959.6995714332805\n",
      "l1 norm: 803.7612119584591\n",
      "Rbeta: 959.998314794625\n",
      "\n",
      "Train set: Avg. loss: 0.000276994, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.115746 89.465\n",
      "l2 norm: 959.3909238430857\n",
      "l1 norm: 803.5078681157042\n",
      "Rbeta: 959.6888292269125\n",
      "\n",
      "Train set: Avg. loss: 0.000274709, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.21025 89.55892\n",
      "l2 norm: 959.0822705451563\n",
      "l1 norm: 803.2544947300792\n",
      "Rbeta: 959.3792576045048\n",
      "\n",
      "Train set: Avg. loss: 0.000272468, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.303276 89.65212\n",
      "l2 norm: 958.7928759664185\n",
      "l1 norm: 803.0171694949714\n",
      "Rbeta: 959.0892224668378\n",
      "\n",
      "Train set: Avg. loss: 0.000270269, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.394905 89.74477\n",
      "l2 norm: 958.5189900270926\n",
      "l1 norm: 802.7928829197189\n",
      "Rbeta: 958.8147849393288\n",
      "\n",
      "Train set: Avg. loss: 0.000268096, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.48658 89.836685\n",
      "l2 norm: 958.2193082951065\n",
      "l1 norm: 802.5468950877591\n",
      "Rbeta: 958.5144467852493\n",
      "\n",
      "Train set: Avg. loss: 0.000265948, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.57831 89.92796\n",
      "l2 norm: 957.9118624222028\n",
      "l1 norm: 802.2943019013937\n",
      "Rbeta: 958.2062185637596\n",
      "\n",
      "Train set: Avg. loss: 0.000263823, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.67008 90.01853\n",
      "l2 norm: 957.6301363636617\n",
      "l1 norm: 802.0633796282104\n",
      "Rbeta: 957.9235813419828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000261724, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.76191 90.10843\n",
      "l2 norm: 957.3558103360069\n",
      "l1 norm: 801.8383450045951\n",
      "Rbeta: 957.6482357505048\n",
      "\n",
      "Train set: Avg. loss: 0.000259647, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.85378 90.19777\n",
      "l2 norm: 957.0742796344081\n",
      "l1 norm: 801.6074771245787\n",
      "Rbeta: 957.3656803442965\n",
      "\n",
      "Train set: Avg. loss: 0.000257592, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.94571 90.286606\n",
      "l2 norm: 956.7805974675884\n",
      "l1 norm: 801.3661576083391\n",
      "Rbeta: 957.0710131409667\n",
      "\n",
      "Train set: Avg. loss: 0.000255570, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.036674 90.374855\n",
      "l2 norm: 956.4927358493015\n",
      "l1 norm: 801.1298608552678\n",
      "Rbeta: 956.7820665006349\n",
      "\n",
      "Train set: Avg. loss: 0.000253597, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.12531 90.46245\n",
      "l2 norm: 956.2138779597395\n",
      "l1 norm: 800.9009257813261\n",
      "Rbeta: 956.5024679733303\n",
      "\n",
      "Train set: Avg. loss: 0.000251664, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.21232 90.54945\n",
      "l2 norm: 955.9489257893914\n",
      "l1 norm: 800.6838532992406\n",
      "Rbeta: 956.236821222632\n",
      "\n",
      "Train set: Avg. loss: 0.000249755, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.29912 90.63585\n",
      "l2 norm: 955.6746728976522\n",
      "l1 norm: 800.4587696410623\n",
      "Rbeta: 955.9618507454131\n",
      "\n",
      "Train set: Avg. loss: 0.000247867, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.38596 90.72159\n",
      "l2 norm: 955.4020507025313\n",
      "l1 norm: 800.2350708793699\n",
      "Rbeta: 955.6884317782154\n",
      "\n",
      "Train set: Avg. loss: 0.000245998, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.472855 90.80687\n",
      "l2 norm: 955.1235561113828\n",
      "l1 norm: 800.0062160168079\n",
      "Rbeta: 955.4091128546534\n",
      "\n",
      "Train set: Avg. loss: 0.000244147, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.559784 90.89169\n",
      "l2 norm: 954.8686784764532\n",
      "l1 norm: 799.7971657508364\n",
      "Rbeta: 955.1533265713281\n",
      "\n",
      "Train set: Avg. loss: 0.000242327, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.64578 90.975914\n",
      "l2 norm: 954.590859256941\n",
      "l1 norm: 799.5687976575111\n",
      "Rbeta: 954.8746697188501\n",
      "\n",
      "Train set: Avg. loss: 0.000240550, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.72957 91.0595\n",
      "l2 norm: 954.318133338012\n",
      "l1 norm: 799.3446714577877\n",
      "Rbeta: 954.6012400544219\n",
      "\n",
      "Train set: Avg. loss: 0.000238812, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.81133 91.14261\n",
      "l2 norm: 954.0556303993958\n",
      "l1 norm: 799.129178530918\n",
      "Rbeta: 954.3384081519732\n",
      "\n",
      "Train set: Avg. loss: 0.000237097, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.89287 91.22504\n",
      "l2 norm: 953.7986383602624\n",
      "l1 norm: 798.9182223320068\n",
      "Rbeta: 954.0808927292069\n",
      "\n",
      "Train set: Avg. loss: 0.000235413, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.97289 91.30713\n",
      "l2 norm: 953.5483412466364\n",
      "l1 norm: 798.7127824207498\n",
      "Rbeta: 953.8303140563601\n",
      "\n",
      "Train set: Avg. loss: 0.000233749, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.05265 91.38867\n",
      "l2 norm: 953.3003704858551\n",
      "l1 norm: 798.5093251825424\n",
      "Rbeta: 953.5819118916941\n",
      "\n",
      "Train set: Avg. loss: 0.000232117, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.130974 91.46964\n",
      "l2 norm: 953.058512271872\n",
      "l1 norm: 798.3108075609176\n",
      "Rbeta: 953.3398778744718\n",
      "\n",
      "Train set: Avg. loss: 0.000230503, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.20933 91.55003\n",
      "l2 norm: 952.8086427152168\n",
      "l1 norm: 798.1055328630525\n",
      "Rbeta: 953.0897140716122\n",
      "\n",
      "Train set: Avg. loss: 0.000228908, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.28711 91.63014\n",
      "l2 norm: 952.5635993263221\n",
      "l1 norm: 797.904454662058\n",
      "Rbeta: 952.8443401470771\n",
      "\n",
      "Train set: Avg. loss: 0.000227340, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.363754 91.70979\n",
      "l2 norm: 952.318794640118\n",
      "l1 norm: 797.7035114069703\n",
      "Rbeta: 952.5993893178179\n",
      "\n",
      "Train set: Avg. loss: 0.000225787, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.44043 91.78901\n",
      "l2 norm: 952.0811546583092\n",
      "l1 norm: 797.5083584666637\n",
      "Rbeta: 952.3614739290501\n",
      "\n",
      "Train set: Avg. loss: 0.000224249, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.51714 91.867714\n",
      "l2 norm: 951.8119482995871\n",
      "l1 norm: 797.2866400535099\n",
      "Rbeta: 952.0919919272803\n",
      "\n",
      "Train set: Avg. loss: 0.000222726, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.59389 91.94591\n",
      "l2 norm: 951.5809747824991\n",
      "l1 norm: 797.0971567047523\n",
      "Rbeta: 951.8606939988127\n",
      "\n",
      "Train set: Avg. loss: 0.000221218, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.67066 92.023636\n",
      "l2 norm: 951.3557313764418\n",
      "l1 norm: 796.9123608095442\n",
      "Rbeta: 951.6350038983709\n",
      "\n",
      "Train set: Avg. loss: 0.000219725, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.747475 92.100815\n",
      "l2 norm: 951.1131072471436\n",
      "l1 norm: 796.7130867201959\n",
      "Rbeta: 951.391914589715\n",
      "\n",
      "Train set: Avg. loss: 0.000218247, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.82432 92.17754\n",
      "l2 norm: 950.8997584960237\n",
      "l1 norm: 796.5384473551876\n",
      "Rbeta: 951.1779641611786\n",
      "\n",
      "Train set: Avg. loss: 0.000216782, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.90119 92.253914\n",
      "l2 norm: 950.6537758079099\n",
      "l1 norm: 796.3363124222872\n",
      "Rbeta: 950.9313681073626\n",
      "\n",
      "Train set: Avg. loss: 0.000215341, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.97708 92.32974\n",
      "l2 norm: 950.4207137481301\n",
      "l1 norm: 796.1449403386578\n",
      "Rbeta: 950.6977394955409\n",
      "\n",
      "Train set: Avg. loss: 0.000213921, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.05224 92.405014\n",
      "l2 norm: 950.2048878989413\n",
      "l1 norm: 795.9679637088992\n",
      "Rbeta: 950.4813596952503\n",
      "\n",
      "Train set: Avg. loss: 0.000212518, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.127 92.47994\n",
      "l2 norm: 949.9694209552133\n",
      "l1 norm: 795.7745983304812\n",
      "Rbeta: 950.2454181096693\n",
      "\n",
      "Train set: Avg. loss: 0.000211139, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.200424 92.554474\n",
      "l2 norm: 949.7490917036095\n",
      "l1 norm: 795.5940139093826\n",
      "Rbeta: 950.024730822867\n",
      "\n",
      "Train set: Avg. loss: 0.000209774, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.27388 92.6286\n",
      "l2 norm: 949.5250238557844\n",
      "l1 norm: 795.4101990784845\n",
      "Rbeta: 949.8002746560873\n",
      "\n",
      "Train set: Avg. loss: 0.000208436, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.34565 92.70241\n",
      "l2 norm: 949.3184514150755\n",
      "l1 norm: 795.2412127107426\n",
      "Rbeta: 949.5934883302886\n",
      "\n",
      "Train set: Avg. loss: 0.000207110, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.41737 92.7758\n",
      "l2 norm: 949.1047987182784\n",
      "l1 norm: 795.0660871504724\n",
      "Rbeta: 949.3794569448182\n",
      "\n",
      "Train set: Avg. loss: 0.000205797, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.48913 92.84877\n",
      "l2 norm: 948.8840909798349\n",
      "l1 norm: 794.8848057229723\n",
      "Rbeta: 949.1583835313174\n",
      "\n",
      "Train set: Avg. loss: 0.000204495, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.56091 92.92129\n",
      "l2 norm: 948.6677527285834\n",
      "l1 norm: 794.7070397859909\n",
      "Rbeta: 948.9417537992725\n",
      "\n",
      "Train set: Avg. loss: 0.000203205, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.63273 92.993416\n",
      "l2 norm: 948.4412102035687\n",
      "l1 norm: 794.5210115708195\n",
      "Rbeta: 948.7147130308513\n",
      "\n",
      "Train set: Avg. loss: 0.000201939, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.70311 93.065155\n",
      "l2 norm: 948.2333143011624\n",
      "l1 norm: 794.3505421031259\n",
      "Rbeta: 948.506469502649\n",
      "\n",
      "Train set: Avg. loss: 0.000200688, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.773186 93.13647\n",
      "l2 norm: 948.026638287622\n",
      "l1 norm: 794.1809808256794\n",
      "Rbeta: 948.2994652423033\n",
      "\n",
      "Train set: Avg. loss: 0.000199447, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.843285 93.207504\n",
      "l2 norm: 947.8042330145178\n",
      "l1 norm: 793.9982257405038\n",
      "Rbeta: 948.0766598879028\n",
      "\n",
      "Train set: Avg. loss: 0.000198217, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.913414 93.27809\n",
      "l2 norm: 947.6171912958941\n",
      "l1 norm: 793.8452158412756\n",
      "Rbeta: 947.8891964877753\n",
      "\n",
      "Train set: Avg. loss: 0.000196999, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.98357 93.34808\n",
      "l2 norm: 947.4394500191905\n",
      "l1 norm: 793.6998450591567\n",
      "Rbeta: 947.7109795438574\n",
      "\n",
      "Train set: Avg. loss: 0.000195792, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.05375 93.4177\n",
      "l2 norm: 947.2307407533434\n",
      "l1 norm: 793.528479706554\n",
      "Rbeta: 947.5017344561286\n",
      "\n",
      "Train set: Avg. loss: 0.000194594, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.12396 93.48703\n",
      "l2 norm: 947.0274150259347\n",
      "l1 norm: 793.3615799455595\n",
      "Rbeta: 947.2978536946678\n",
      "\n",
      "Train set: Avg. loss: 0.000193406, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.1942 93.55611\n",
      "l2 norm: 946.8244820623158\n",
      "l1 norm: 793.1951242761348\n",
      "Rbeta: 947.094282721284\n",
      "\n",
      "Train set: Avg. loss: 0.000192226, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.264465 93.625\n",
      "l2 norm: 946.6110000882569\n",
      "l1 norm: 793.0198490796179\n",
      "Rbeta: 946.8801650603616\n",
      "\n",
      "Train set: Avg. loss: 0.000191070, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.333176 93.69357\n",
      "l2 norm: 946.4187555531162\n",
      "l1 norm: 792.8621806441197\n",
      "Rbeta: 946.6874797293381\n",
      "\n",
      "Train set: Avg. loss: 0.000189925, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.401695 93.76176\n",
      "l2 norm: 946.2124334594641\n",
      "l1 norm: 792.6928115447615\n",
      "Rbeta: 946.4806501052134\n",
      "\n",
      "Train set: Avg. loss: 0.000188805, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.46858 93.829445\n",
      "l2 norm: 946.0039618429533\n",
      "l1 norm: 792.5215585651823\n",
      "Rbeta: 946.2718593554725\n",
      "\n",
      "Train set: Avg. loss: 0.000187696, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.53535 93.89678\n",
      "l2 norm: 945.8090023569516\n",
      "l1 norm: 792.3614770276702\n",
      "Rbeta: 946.0765083329811\n",
      "\n",
      "Train set: Avg. loss: 0.000186595, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.602135 93.96384\n",
      "l2 norm: 945.6103352782108\n",
      "l1 norm: 792.198399775622\n",
      "Rbeta: 945.8774327125333\n",
      "\n",
      "Train set: Avg. loss: 0.000185503, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.668945 94.03049\n",
      "l2 norm: 945.4165119008832\n",
      "l1 norm: 792.0394287064089\n",
      "Rbeta: 945.6830722958665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000184420, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.73579 94.09681\n",
      "l2 norm: 945.223178724907\n",
      "l1 norm: 791.8808375109415\n",
      "Rbeta: 945.4892492797394\n",
      "\n",
      "Train set: Avg. loss: 0.000183347, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.80265 94.16278\n",
      "l2 norm: 945.0340743597801\n",
      "l1 norm: 791.7254929889381\n",
      "Rbeta: 945.2995620861825\n",
      "\n",
      "Train set: Avg. loss: 0.000182282, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.869545 94.22839\n",
      "l2 norm: 944.8305732060344\n",
      "l1 norm: 791.558215674141\n",
      "Rbeta: 945.0954935129024\n",
      "\n",
      "Train set: Avg. loss: 0.000181226, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.936455 94.29366\n",
      "l2 norm: 944.6473776091945\n",
      "l1 norm: 791.4078071159923\n",
      "Rbeta: 944.9116832249792\n",
      "\n",
      "Train set: Avg. loss: 0.000180178, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.003395 94.358635\n",
      "l2 norm: 944.4621437882554\n",
      "l1 norm: 791.2557578930878\n",
      "Rbeta: 944.7257904940277\n",
      "\n",
      "Train set: Avg. loss: 0.000179138, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.07036 94.42337\n",
      "l2 norm: 944.2751963225236\n",
      "l1 norm: 791.1022782590903\n",
      "Rbeta: 944.5381340855564\n",
      "\n",
      "Train set: Avg. loss: 0.000178124, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.13521 94.4877\n",
      "l2 norm: 944.085691259718\n",
      "l1 norm: 790.9465643757084\n",
      "Rbeta: 944.3481625931737\n",
      "\n",
      "Train set: Avg. loss: 0.000177129, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.1986 94.551735\n",
      "l2 norm: 943.9065379091402\n",
      "l1 norm: 790.79953020775\n",
      "Rbeta: 944.1686187991551\n",
      "\n",
      "Train set: Avg. loss: 0.000176142, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.26202 94.61547\n",
      "l2 norm: 943.7327445434885\n",
      "l1 norm: 790.6571182168892\n",
      "Rbeta: 943.994524232599\n",
      "\n",
      "Train set: Avg. loss: 0.000175162, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.32545 94.67885\n",
      "l2 norm: 943.553702473005\n",
      "l1 norm: 790.5102885414876\n",
      "Rbeta: 943.8150742779246\n",
      "\n",
      "Train set: Avg. loss: 0.000174190, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.38891 94.74199\n",
      "l2 norm: 943.3868104936442\n",
      "l1 norm: 790.3736268603175\n",
      "Rbeta: 943.647672729068\n",
      "\n",
      "Train set: Avg. loss: 0.000173226, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.452385 94.80479\n",
      "l2 norm: 943.2200042792014\n",
      "l1 norm: 790.2371352857419\n",
      "Rbeta: 943.4804369922941\n",
      "\n",
      "Train set: Avg. loss: 0.000172268, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.515884 94.867485\n",
      "l2 norm: 943.0433561784446\n",
      "l1 norm: 790.0923212279017\n",
      "Rbeta: 943.3032234838234\n",
      "\n",
      "Train set: Avg. loss: 0.000171318, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.57941 94.929825\n",
      "l2 norm: 942.880483364483\n",
      "l1 norm: 789.9588679641245\n",
      "Rbeta: 943.1398357255305\n",
      "\n",
      "Train set: Avg. loss: 0.000170374, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.64296 94.9919\n",
      "l2 norm: 942.7136011980352\n",
      "l1 norm: 789.8218993709029\n",
      "Rbeta: 942.9724166203926\n",
      "\n",
      "Train set: Avg. loss: 0.000169443, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.70603 95.05354\n",
      "l2 norm: 942.5407406225675\n",
      "l1 norm: 789.6799508967299\n",
      "Rbeta: 942.7990027870776\n",
      "\n",
      "Train set: Avg. loss: 0.000168529, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.76781 95.11487\n",
      "l2 norm: 942.3469474685289\n",
      "l1 norm: 789.5205210981171\n",
      "Rbeta: 942.6047137432455\n",
      "\n",
      "Train set: Avg. loss: 0.000167622, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.829605 95.17586\n",
      "l2 norm: 942.1686890618769\n",
      "l1 norm: 789.374321894625\n",
      "Rbeta: 942.4260071992906\n",
      "\n",
      "Train set: Avg. loss: 0.000166721, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.89142 95.23669\n",
      "l2 norm: 941.9939014583307\n",
      "l1 norm: 789.2308460600179\n",
      "Rbeta: 942.2507470923651\n",
      "\n",
      "Train set: Avg. loss: 0.000165826, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.953255 95.29726\n",
      "l2 norm: 941.8089681647027\n",
      "l1 norm: 789.0787782584296\n",
      "Rbeta: 942.065246312423\n",
      "\n",
      "Train set: Avg. loss: 0.000164939, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.015114 95.35753\n",
      "l2 norm: 941.6530352977969\n",
      "l1 norm: 788.9508617608606\n",
      "Rbeta: 941.90882009798\n",
      "\n",
      "Train set: Avg. loss: 0.000164059, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.07699 95.41732\n",
      "l2 norm: 941.5067489477191\n",
      "l1 norm: 788.8312502626148\n",
      "Rbeta: 941.7618129377089\n",
      "\n",
      "Train set: Avg. loss: 0.000163187, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.13867 95.47689\n",
      "l2 norm: 941.337993735216\n",
      "l1 norm: 788.6927349762724\n",
      "Rbeta: 941.5925106624596\n",
      "\n",
      "Train set: Avg. loss: 0.000162333, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.19877 95.53615\n",
      "l2 norm: 941.1669656423908\n",
      "l1 norm: 788.5522138833185\n",
      "Rbeta: 941.4209454890096\n",
      "\n",
      "Train set: Avg. loss: 0.000161485, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.25887 95.59515\n",
      "l2 norm: 941.0044895074852\n",
      "l1 norm: 788.4189952345437\n",
      "Rbeta: 941.25804086884\n",
      "\n",
      "Train set: Avg. loss: 0.000160656, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.317184 95.65397\n",
      "l2 norm: 940.819143751061\n",
      "l1 norm: 788.2666621686546\n",
      "Rbeta: 941.0723579815693\n",
      "\n",
      "Train set: Avg. loss: 0.000159839, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.37477 95.71245\n",
      "l2 norm: 940.6385527813363\n",
      "l1 norm: 788.1183826989703\n",
      "Rbeta: 940.8914804249896\n",
      "\n",
      "Train set: Avg. loss: 0.000159035, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.431305 95.77067\n",
      "l2 norm: 940.4752998191634\n",
      "l1 norm: 787.984330563634\n",
      "Rbeta: 940.7280410373886\n",
      "\n",
      "Train set: Avg. loss: 0.000158235, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.48785 95.82874\n",
      "l2 norm: 940.3015310068724\n",
      "l1 norm: 787.8414515610549\n",
      "Rbeta: 940.5541116627301\n",
      "\n",
      "Train set: Avg. loss: 0.000157442, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.54441 95.886505\n",
      "l2 norm: 940.1341693712485\n",
      "l1 norm: 787.7040453772076\n",
      "Rbeta: 940.3865210527433\n",
      "\n",
      "Train set: Avg. loss: 0.000156656, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.601 95.94386\n",
      "l2 norm: 939.9660701007454\n",
      "l1 norm: 787.5660513285754\n",
      "Rbeta: 940.2182012901075\n",
      "\n",
      "Train set: Avg. loss: 0.000155875, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.65759 96.00091\n",
      "l2 norm: 939.8294551065633\n",
      "l1 norm: 787.4543634011466\n",
      "Rbeta: 940.0813177793102\n",
      "\n",
      "Train set: Avg. loss: 0.000155099, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.71421 96.0578\n",
      "l2 norm: 939.6769696231994\n",
      "l1 norm: 787.3292802480804\n",
      "Rbeta: 939.9284515082069\n",
      "\n",
      "Train set: Avg. loss: 0.000154329, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.77084 96.11447\n",
      "l2 norm: 939.5222518148506\n",
      "l1 norm: 787.2024361425931\n",
      "Rbeta: 939.7734327339439\n",
      "\n",
      "Train set: Avg. loss: 0.000153565, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.82749 96.17074\n",
      "l2 norm: 939.365599516064\n",
      "l1 norm: 787.0739736350581\n",
      "Rbeta: 939.6163890917535\n",
      "\n",
      "Train set: Avg. loss: 0.000152806, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.88416 96.22679\n",
      "l2 norm: 939.2110766319073\n",
      "l1 norm: 786.9474138130827\n",
      "Rbeta: 939.4614684559721\n",
      "\n",
      "Train set: Avg. loss: 0.000152051, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.94085 96.28272\n",
      "l2 norm: 939.0623826026786\n",
      "l1 norm: 786.8253872156704\n",
      "Rbeta: 939.3123219567071\n",
      "\n",
      "Train set: Avg. loss: 0.000151303, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.99755 96.33836\n",
      "l2 norm: 938.9001862990962\n",
      "l1 norm: 786.6919445596213\n",
      "Rbeta: 939.1496097207825\n",
      "\n",
      "Train set: Avg. loss: 0.000150558, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.054276 96.393875\n",
      "l2 norm: 938.7330187162605\n",
      "l1 norm: 786.554660578611\n",
      "Rbeta: 938.9820204445191\n",
      "\n",
      "Train set: Avg. loss: 0.000149819, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.111015 96.44908\n",
      "l2 norm: 938.5718356147179\n",
      "l1 norm: 786.4224521196194\n",
      "Rbeta: 938.8202754350855\n",
      "\n",
      "Train set: Avg. loss: 0.000149086, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.16777 96.50397\n",
      "l2 norm: 938.4029324545216\n",
      "l1 norm: 786.2835091329357\n",
      "Rbeta: 938.6508640686659\n",
      "\n",
      "Train set: Avg. loss: 0.000148357, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.22455 96.55868\n",
      "l2 norm: 938.2331987685519\n",
      "l1 norm: 786.1437989981607\n",
      "Rbeta: 938.4806079628075\n",
      "\n",
      "Train set: Avg. loss: 0.000147633, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.281334 96.61319\n",
      "l2 norm: 938.0848635294183\n",
      "l1 norm: 786.0221489784285\n",
      "Rbeta: 938.3316463617425\n",
      "\n",
      "Train set: Avg. loss: 0.000146918, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.337524 96.66762\n",
      "l2 norm: 937.9593932523627\n",
      "l1 norm: 785.9195995299722\n",
      "Rbeta: 938.2056610903832\n",
      "\n",
      "Train set: Avg. loss: 0.000146214, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.39252 96.72187\n",
      "l2 norm: 937.8108807072567\n",
      "l1 norm: 785.7978012355427\n",
      "Rbeta: 938.0566815683974\n",
      "\n",
      "Train set: Avg. loss: 0.000145515, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.447525 96.775894\n",
      "l2 norm: 937.6734716172537\n",
      "l1 norm: 785.6853608970781\n",
      "Rbeta: 937.91885319332\n",
      "\n",
      "Train set: Avg. loss: 0.000144822, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.50255 96.829544\n",
      "l2 norm: 937.5347256494706\n",
      "l1 norm: 785.5715024819929\n",
      "Rbeta: 937.7796177661353\n",
      "\n",
      "Train set: Avg. loss: 0.000144134, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.557594 96.882904\n",
      "l2 norm: 937.3966641436907\n",
      "l1 norm: 785.458293593464\n",
      "Rbeta: 937.6410910889427\n",
      "\n",
      "Train set: Avg. loss: 0.000143450, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.61265 96.936066\n",
      "l2 norm: 937.2502794388846\n",
      "l1 norm: 785.3380036576232\n",
      "Rbeta: 937.4941195459191\n",
      "\n",
      "Train set: Avg. loss: 0.000142770, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.667725 96.98903\n",
      "l2 norm: 937.1094214600139\n",
      "l1 norm: 785.222272446232\n",
      "Rbeta: 937.3527451022559\n",
      "\n",
      "Train set: Avg. loss: 0.000142096, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.72282 97.04172\n",
      "l2 norm: 936.9910974811054\n",
      "l1 norm: 785.125752708435\n",
      "Rbeta: 937.2338760606904\n",
      "\n",
      "Train set: Avg. loss: 0.000141426, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.77792 97.094124\n",
      "l2 norm: 936.8479156968787\n",
      "l1 norm: 785.008610799205\n",
      "Rbeta: 937.0900461525473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000140760, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.833046 97.14647\n",
      "l2 norm: 936.7137032578918\n",
      "l1 norm: 784.898773242112\n",
      "Rbeta: 936.9552928345864\n",
      "\n",
      "Train set: Avg. loss: 0.000140107, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.88672 97.1986\n",
      "l2 norm: 936.5699706610504\n",
      "l1 norm: 784.7808411192909\n",
      "Rbeta: 936.8110825092804\n",
      "\n",
      "Train set: Avg. loss: 0.000139467, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.93902 97.2505\n",
      "l2 norm: 936.4260019849728\n",
      "l1 norm: 784.6627361341727\n",
      "Rbeta: 936.6666725376195\n",
      "\n",
      "Train set: Avg. loss: 0.000138837, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.99052 97.30206\n",
      "l2 norm: 936.2922537729368\n",
      "l1 norm: 784.553045231391\n",
      "Rbeta: 936.5326360287078\n",
      "\n",
      "Train set: Avg. loss: 0.000138211, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.04202 97.35333\n",
      "l2 norm: 936.1627050787401\n",
      "l1 norm: 784.4469595377188\n",
      "Rbeta: 936.4027849028948\n",
      "\n",
      "Train set: Avg. loss: 0.000137594, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.09296 97.40437\n",
      "l2 norm: 936.0255072720486\n",
      "l1 norm: 784.3345369256327\n",
      "Rbeta: 936.2653637913064\n",
      "\n",
      "Train set: Avg. loss: 0.000136987, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.14265 97.45521\n",
      "l2 norm: 935.885684198144\n",
      "l1 norm: 784.2198181568729\n",
      "Rbeta: 936.125326626053\n",
      "\n",
      "Train set: Avg. loss: 0.000136383, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.19236 97.50595\n",
      "l2 norm: 935.7578100191027\n",
      "l1 norm: 784.1149856840202\n",
      "Rbeta: 935.9972897683489\n",
      "\n",
      "Train set: Avg. loss: 0.000135784, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.24208 97.55652\n",
      "l2 norm: 935.6368431164468\n",
      "l1 norm: 784.0158634642586\n",
      "Rbeta: 935.876130222838\n",
      "\n",
      "Train set: Avg. loss: 0.000135187, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.29181 97.606964\n",
      "l2 norm: 935.5157301258481\n",
      "l1 norm: 783.9165917924886\n",
      "Rbeta: 935.7548043351356\n",
      "\n",
      "Train set: Avg. loss: 0.000134594, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.34156 97.65723\n",
      "l2 norm: 935.3902518211084\n",
      "l1 norm: 783.8137702517204\n",
      "Rbeta: 935.6291268831621\n",
      "\n",
      "Train set: Avg. loss: 0.000134005, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.39132 97.70729\n",
      "l2 norm: 935.2566000035775\n",
      "l1 norm: 783.704136115747\n",
      "Rbeta: 935.4952117219696\n",
      "\n",
      "Train set: Avg. loss: 0.000133419, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.44109 97.75728\n",
      "l2 norm: 935.1390523464293\n",
      "l1 norm: 783.6080216968469\n",
      "Rbeta: 935.3774045986279\n",
      "\n",
      "Train set: Avg. loss: 0.000132836, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.49088 97.80706\n",
      "l2 norm: 935.0127462184129\n",
      "l1 norm: 783.5044522928883\n",
      "Rbeta: 935.2508148429192\n",
      "\n",
      "Train set: Avg. loss: 0.000132257, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.54068 97.85664\n",
      "l2 norm: 934.8674174139701\n",
      "l1 norm: 783.3847786514838\n",
      "Rbeta: 935.105176661176\n",
      "\n",
      "Train set: Avg. loss: 0.000131682, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.5905 97.90603\n",
      "l2 norm: 934.7025857375077\n",
      "l1 norm: 783.2488330724937\n",
      "Rbeta: 934.9399994427222\n",
      "\n",
      "Train set: Avg. loss: 0.000131110, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.64033 97.955215\n",
      "l2 norm: 934.5536098933906\n",
      "l1 norm: 783.126137039786\n",
      "Rbeta: 934.7907469835932\n",
      "\n",
      "Train set: Avg. loss: 0.000130541, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.69016 98.00432\n",
      "l2 norm: 934.4012969109498\n",
      "l1 norm: 783.0006903466848\n",
      "Rbeta: 934.6380584078343\n",
      "\n",
      "Train set: Avg. loss: 0.000129976, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.74002 98.05316\n",
      "l2 norm: 934.2755823584963\n",
      "l1 norm: 782.8975579072358\n",
      "Rbeta: 934.5119343375646\n",
      "\n",
      "Train set: Avg. loss: 0.000129415, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.78989 98.10177\n",
      "l2 norm: 934.1658855100657\n",
      "l1 norm: 782.8078215818132\n",
      "Rbeta: 934.4018835434044\n",
      "\n",
      "Train set: Avg. loss: 0.000128858, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.83977 98.15015\n",
      "l2 norm: 934.0358477951385\n",
      "l1 norm: 782.7010157588018\n",
      "Rbeta: 934.2713462662829\n",
      "\n",
      "Train set: Avg. loss: 0.000128303, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.88966 98.19845\n",
      "l2 norm: 933.9180339559822\n",
      "l1 norm: 782.6045894201139\n",
      "Rbeta: 934.1530701149544\n",
      "\n",
      "Train set: Avg. loss: 0.000127751, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.93957 98.24655\n",
      "l2 norm: 933.7817911951435\n",
      "l1 norm: 782.4927859826612\n",
      "Rbeta: 934.0164068163958\n",
      "\n",
      "Train set: Avg. loss: 0.000127204, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.98949 98.294395\n",
      "l2 norm: 933.6381960837309\n",
      "l1 norm: 782.3747255054052\n",
      "Rbeta: 933.8722357616344\n",
      "\n",
      "Train set: Avg. loss: 0.000126660, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.03942 98.342\n",
      "l2 norm: 933.5272201070569\n",
      "l1 norm: 782.2840074933131\n",
      "Rbeta: 933.7607889865326\n",
      "\n",
      "Train set: Avg. loss: 0.000126119, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.08937 98.38937\n",
      "l2 norm: 933.4092833833624\n",
      "l1 norm: 782.1873223756158\n",
      "Rbeta: 933.6422979386842\n",
      "\n",
      "Train set: Avg. loss: 0.000125581, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.13933 98.43661\n",
      "l2 norm: 933.2994480257738\n",
      "l1 norm: 782.0973998523023\n",
      "Rbeta: 933.5318549539943\n",
      "\n",
      "Train set: Avg. loss: 0.000125053, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.18806 98.483795\n",
      "l2 norm: 933.191500015139\n",
      "l1 norm: 782.009243999372\n",
      "Rbeta: 933.4234999571548\n",
      "\n",
      "Train set: Avg. loss: 0.000124531, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.2362 98.5309\n",
      "l2 norm: 933.0656198847093\n",
      "l1 norm: 781.9059613950872\n",
      "Rbeta: 933.2972811445801\n",
      "\n",
      "Train set: Avg. loss: 0.000124012, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.28434 98.57777\n",
      "l2 norm: 932.9379999044145\n",
      "l1 norm: 781.8011844310561\n",
      "Rbeta: 933.1692389686336\n",
      "\n",
      "Train set: Avg. loss: 0.000123498, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.33222 98.6244\n",
      "l2 norm: 932.8114813185097\n",
      "l1 norm: 781.6974635216036\n",
      "Rbeta: 933.042351827975\n",
      "\n",
      "Train set: Avg. loss: 0.000122996, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.37854 98.67084\n",
      "l2 norm: 932.6867538665066\n",
      "l1 norm: 781.5953286600327\n",
      "Rbeta: 932.9173547374957\n",
      "\n",
      "Train set: Avg. loss: 0.000122496, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.42487 98.71718\n",
      "l2 norm: 932.5611923770048\n",
      "l1 norm: 781.4924628188855\n",
      "Rbeta: 932.7916056454228\n",
      "\n",
      "Train set: Avg. loss: 0.000121999, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.471214 98.76339\n",
      "l2 norm: 932.4245065655266\n",
      "l1 norm: 781.3801635255116\n",
      "Rbeta: 932.6546156704186\n",
      "\n",
      "Train set: Avg. loss: 0.000121507, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.51717 98.80939\n",
      "l2 norm: 932.3002004427632\n",
      "l1 norm: 781.2782852209264\n",
      "Rbeta: 932.5300811659409\n",
      "\n",
      "Train set: Avg. loss: 0.000121025, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.56168 98.855225\n",
      "l2 norm: 932.1790649115292\n",
      "l1 norm: 781.1790911271617\n",
      "Rbeta: 932.4088120612718\n",
      "\n",
      "Train set: Avg. loss: 0.000120546, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.6062 98.900955\n",
      "l2 norm: 932.0697210170749\n",
      "l1 norm: 781.0897110887823\n",
      "Rbeta: 932.2993677027961\n",
      "\n",
      "Train set: Avg. loss: 0.000120070, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.650734 98.94658\n",
      "l2 norm: 931.9259517481867\n",
      "l1 norm: 780.9713089713509\n",
      "Rbeta: 932.1555000641426\n",
      "\n",
      "Train set: Avg. loss: 0.000119595, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.695274 98.992096\n",
      "l2 norm: 931.8039947557198\n",
      "l1 norm: 780.8712686061672\n",
      "Rbeta: 932.0334174625817\n",
      "\n",
      "Train set: Avg. loss: 0.000119125, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.73982 99.03729\n",
      "l2 norm: 931.6748157583219\n",
      "l1 norm: 780.7652775324658\n",
      "Rbeta: 931.9039955925016\n",
      "\n",
      "Train set: Avg. loss: 0.000118658, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.784386 99.08214\n",
      "l2 norm: 931.566995905144\n",
      "l1 norm: 780.6772233499236\n",
      "Rbeta: 931.7959390817863\n",
      "\n",
      "Train set: Avg. loss: 0.000118193, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.82896 99.126816\n",
      "l2 norm: 931.4734244172706\n",
      "l1 norm: 780.6010406154869\n",
      "Rbeta: 931.702219538432\n",
      "\n",
      "Train set: Avg. loss: 0.000117739, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.87207 99.17137\n",
      "l2 norm: 931.3888338864832\n",
      "l1 norm: 780.5322515449884\n",
      "Rbeta: 931.6175084385133\n",
      "\n",
      "Train set: Avg. loss: 0.000117288, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.91481 99.21591\n",
      "l2 norm: 931.2813289404846\n",
      "l1 norm: 780.4441873443232\n",
      "Rbeta: 931.5100113844221\n",
      "\n",
      "Train set: Avg. loss: 0.000116839, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.95755 99.26041\n",
      "l2 norm: 931.1587291763761\n",
      "l1 norm: 780.3433470893193\n",
      "Rbeta: 931.3873075894608\n",
      "\n",
      "Train set: Avg. loss: 0.000116393, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.000305 99.304825\n",
      "l2 norm: 931.0374003962665\n",
      "l1 norm: 780.2435200777531\n",
      "Rbeta: 931.265916926796\n",
      "\n",
      "Train set: Avg. loss: 0.000115948, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.04307 99.349075\n",
      "l2 norm: 930.9255080415181\n",
      "l1 norm: 780.1515663857131\n",
      "Rbeta: 931.1540382039472\n",
      "\n",
      "Train set: Avg. loss: 0.000115506, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.08584 99.39319\n",
      "l2 norm: 930.832805967568\n",
      "l1 norm: 780.0757935766399\n",
      "Rbeta: 931.0612227106946\n",
      "\n",
      "Train set: Avg. loss: 0.000115067, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.128624 99.437195\n",
      "l2 norm: 930.7430320222776\n",
      "l1 norm: 780.002491165657\n",
      "Rbeta: 930.9713894256595\n",
      "\n",
      "Train set: Avg. loss: 0.000114630, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.17142 99.48099\n",
      "l2 norm: 930.6444416735261\n",
      "l1 norm: 779.9217853632649\n",
      "Rbeta: 930.8726053882843\n",
      "\n",
      "Train set: Avg. loss: 0.000114195, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.21422 99.524605\n",
      "l2 norm: 930.5477660934141\n",
      "l1 norm: 779.8426977874354\n",
      "Rbeta: 930.7758524638525\n",
      "\n",
      "Train set: Avg. loss: 0.000113763, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.25703 99.56811\n",
      "l2 norm: 930.454608658055\n",
      "l1 norm: 779.7665568739108\n",
      "Rbeta: 930.6824647505041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000113332, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.29985 99.61154\n",
      "l2 norm: 930.3450628622637\n",
      "l1 norm: 779.6766897522921\n",
      "Rbeta: 930.572805862653\n",
      "\n",
      "Train set: Avg. loss: 0.000112904, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.34268 99.654816\n",
      "l2 norm: 930.2208476831759\n",
      "l1 norm: 779.5745504476969\n",
      "Rbeta: 930.4483838884571\n",
      "\n",
      "Train set: Avg. loss: 0.000112479, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.38552 99.69795\n",
      "l2 norm: 930.1027558772378\n",
      "l1 norm: 779.4775352782507\n",
      "Rbeta: 930.3300611231672\n",
      "\n",
      "Train set: Avg. loss: 0.000112055, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.42837 99.74098\n",
      "l2 norm: 929.9724509975581\n",
      "l1 norm: 779.3703160063387\n",
      "Rbeta: 930.199544729231\n",
      "\n",
      "Train set: Avg. loss: 0.000111634, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.47123 99.78377\n",
      "l2 norm: 929.8605933585429\n",
      "l1 norm: 779.2785304236565\n",
      "Rbeta: 930.0875051612016\n",
      "\n",
      "Train set: Avg. loss: 0.000111218, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.51356 99.82644\n",
      "l2 norm: 929.7451557933593\n",
      "l1 norm: 779.183629382374\n",
      "Rbeta: 929.9718666923379\n",
      "\n",
      "Train set: Avg. loss: 0.000110811, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.55457 99.86891\n",
      "l2 norm: 929.62830794972\n",
      "l1 norm: 779.0875888348372\n",
      "Rbeta: 929.854978157926\n",
      "\n",
      "Train set: Avg. loss: 0.000110406, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.5956 99.911224\n",
      "l2 norm: 929.5186204509122\n",
      "l1 norm: 778.9977979598816\n",
      "Rbeta: 929.7451949637081\n",
      "\n",
      "Train set: Avg. loss: 0.000110003, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.63663 99.95341\n",
      "l2 norm: 929.4113738580597\n",
      "l1 norm: 778.9100013942073\n",
      "Rbeta: 929.6377821857743\n",
      "\n",
      "Train set: Avg. loss: 0.000109605, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.677086 99.99547\n",
      "l2 norm: 929.3154531893787\n",
      "l1 norm: 778.831512101929\n",
      "Rbeta: 929.541831130602\n",
      "\n",
      "Train set: Avg. loss: 0.000109215, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.71627 100.03734\n",
      "l2 norm: 929.2359972883747\n",
      "l1 norm: 778.7667468805646\n",
      "Rbeta: 929.462436295274\n",
      "\n",
      "Train set: Avg. loss: 0.000108828, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.75546 100.07898\n",
      "l2 norm: 929.1393350758639\n",
      "l1 norm: 778.6874737087298\n",
      "Rbeta: 929.3658735418093\n",
      "\n",
      "Train set: Avg. loss: 0.000108442, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.79466 100.12053\n",
      "l2 norm: 929.0291825255562\n",
      "l1 norm: 778.5969639074156\n",
      "Rbeta: 929.2557747154201\n",
      "\n",
      "Train set: Avg. loss: 0.000108058, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.83387 100.162094\n",
      "l2 norm: 928.9456254596992\n",
      "l1 norm: 778.528773196367\n",
      "Rbeta: 929.172258037444\n",
      "\n",
      "Train set: Avg. loss: 0.000107676, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.873085 100.20355\n",
      "l2 norm: 928.8705731813859\n",
      "l1 norm: 778.4677004286784\n",
      "Rbeta: 929.097168524652\n",
      "\n",
      "Train set: Avg. loss: 0.000107295, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.91231 100.24492\n",
      "l2 norm: 928.7629408640313\n",
      "l1 norm: 778.3793833490874\n",
      "Rbeta: 928.9896072819174\n",
      "\n",
      "Train set: Avg. loss: 0.000106917, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.95154 100.28609\n",
      "l2 norm: 928.6609730285642\n",
      "l1 norm: 778.2958089149732\n",
      "Rbeta: 928.8875755696764\n",
      "\n",
      "Train set: Avg. loss: 0.000106540, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.990776 100.3271\n",
      "l2 norm: 928.571405056213\n",
      "l1 norm: 778.2225343217667\n",
      "Rbeta: 928.7979580736776\n",
      "\n",
      "Train set: Avg. loss: 0.000106166, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.03002 100.368\n",
      "l2 norm: 928.4898105550591\n",
      "l1 norm: 778.1558138736252\n",
      "Rbeta: 928.7163677991093\n",
      "\n",
      "Train set: Avg. loss: 0.000105793, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.069275 100.40876\n",
      "l2 norm: 928.3818457021803\n",
      "l1 norm: 778.0669950534685\n",
      "Rbeta: 928.608317306514\n",
      "\n",
      "Train set: Avg. loss: 0.000105423, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.10854 100.44934\n",
      "l2 norm: 928.2711545484003\n",
      "l1 norm: 777.9760011606631\n",
      "Rbeta: 928.497633033592\n",
      "\n",
      "Train set: Avg. loss: 0.000105054, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.14781 100.4899\n",
      "l2 norm: 928.1694957467298\n",
      "l1 norm: 777.8925412685726\n",
      "Rbeta: 928.3958624186959\n",
      "\n",
      "Train set: Avg. loss: 0.000104686, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.18709 100.53032\n",
      "l2 norm: 928.0825040171738\n",
      "l1 norm: 777.8214866969056\n",
      "Rbeta: 928.308764719338\n",
      "\n",
      "Train set: Avg. loss: 0.000104321, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.22638 100.57058\n",
      "l2 norm: 927.9898557592866\n",
      "l1 norm: 777.7456116342611\n",
      "Rbeta: 928.216082807528\n",
      "\n",
      "Train set: Avg. loss: 0.000103957, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.26567 100.610794\n",
      "l2 norm: 927.8965930010675\n",
      "l1 norm: 777.6692815542094\n",
      "Rbeta: 928.1226197644794\n",
      "\n",
      "Train set: Avg. loss: 0.000103602, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.303406 100.65088\n",
      "l2 norm: 927.7899320988205\n",
      "l1 norm: 777.5818138192468\n",
      "Rbeta: 928.016064462306\n",
      "\n",
      "Train set: Avg. loss: 0.000103259, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.33908 100.69072\n",
      "l2 norm: 927.6849479007781\n",
      "l1 norm: 777.4954838073807\n",
      "Rbeta: 927.9112693555115\n",
      "\n",
      "Train set: Avg. loss: 0.000102918, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.37466 100.730354\n",
      "l2 norm: 927.5800073562168\n",
      "l1 norm: 777.4092247493224\n",
      "Rbeta: 927.8065733980064\n",
      "\n",
      "Train set: Avg. loss: 0.000102579, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.41024 100.76993\n",
      "l2 norm: 927.4637544359483\n",
      "l1 norm: 777.3134948673212\n",
      "Rbeta: 927.690578967013\n",
      "\n",
      "Train set: Avg. loss: 0.000102241, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.44582 100.80945\n",
      "l2 norm: 927.33810412736\n",
      "l1 norm: 777.2099473485459\n",
      "Rbeta: 927.5651205049793\n",
      "\n",
      "Train set: Avg. loss: 0.000101904, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.481415 100.84894\n",
      "l2 norm: 927.2274685180449\n",
      "l1 norm: 777.1191048188242\n",
      "Rbeta: 927.4546523288835\n",
      "\n",
      "Train set: Avg. loss: 0.000101569, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.51701 100.88818\n",
      "l2 norm: 927.1325503250687\n",
      "l1 norm: 777.0413434841201\n",
      "Rbeta: 927.3599026021527\n",
      "\n",
      "Train set: Avg. loss: 0.000101237, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.55262 100.927216\n",
      "l2 norm: 927.0513111641932\n",
      "l1 norm: 776.9749801842127\n",
      "Rbeta: 927.278774098223\n",
      "\n",
      "Train set: Avg. loss: 0.000100906, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.588234 100.966064\n",
      "l2 norm: 926.9639558551987\n",
      "l1 norm: 776.9034671687568\n",
      "Rbeta: 927.1916160269003\n",
      "\n",
      "Train set: Avg. loss: 0.000100577, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.623856 101.00472\n",
      "l2 norm: 926.8754507810792\n",
      "l1 norm: 776.8310313098202\n",
      "Rbeta: 927.1032273713864\n",
      "\n",
      "Train set: Avg. loss: 0.000100250, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.65948 101.043304\n",
      "l2 norm: 926.773942009907\n",
      "l1 norm: 776.7476815296479\n",
      "Rbeta: 927.0018760672136\n",
      "\n",
      "Train set: Avg. loss: 0.000099924, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.695114 101.081795\n",
      "l2 norm: 926.681179283011\n",
      "l1 norm: 776.6717856989612\n",
      "Rbeta: 926.9092562816429\n",
      "\n",
      "Train set: Avg. loss: 0.000099599, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.73075 101.120224\n",
      "l2 norm: 926.5918335143098\n",
      "l1 norm: 776.5987992392071\n",
      "Rbeta: 926.8199966968976\n",
      "\n",
      "Train set: Avg. loss: 0.000099276, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.766396 101.1586\n",
      "l2 norm: 926.4857330871055\n",
      "l1 norm: 776.5116702188501\n",
      "Rbeta: 926.7139183383713\n",
      "\n",
      "Train set: Avg. loss: 0.000098954, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.80205 101.19685\n",
      "l2 norm: 926.365812739696\n",
      "l1 norm: 776.4128249637097\n",
      "Rbeta: 926.5940748662565\n",
      "\n",
      "Train set: Avg. loss: 0.000098633, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.83771 101.23503\n",
      "l2 norm: 926.2669656288375\n",
      "l1 norm: 776.3315745077243\n",
      "Rbeta: 926.4953050492963\n",
      "\n",
      "Train set: Avg. loss: 0.000098317, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.872795 101.273026\n",
      "l2 norm: 926.1713524496589\n",
      "l1 norm: 776.2529195670478\n",
      "Rbeta: 926.3997750558007\n",
      "\n",
      "Train set: Avg. loss: 0.000098007, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.906586 101.31098\n",
      "l2 norm: 926.0810111075829\n",
      "l1 norm: 776.1785569693666\n",
      "Rbeta: 926.3096776853197\n",
      "\n",
      "Train set: Avg. loss: 0.000097699, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.940384 101.34882\n",
      "l2 norm: 925.990014030996\n",
      "l1 norm: 776.1035839787597\n",
      "Rbeta: 926.2189774965835\n",
      "\n",
      "Train set: Avg. loss: 0.000097393, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.97419 101.38657\n",
      "l2 norm: 925.9050240174602\n",
      "l1 norm: 776.0336584145455\n",
      "Rbeta: 926.1341763306491\n",
      "\n",
      "Train set: Avg. loss: 0.000097087, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.008 101.42429\n",
      "l2 norm: 925.8369933115493\n",
      "l1 norm: 775.9778885077046\n",
      "Rbeta: 926.0663821453979\n",
      "\n",
      "Train set: Avg. loss: 0.000096782, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.041824 101.461914\n",
      "l2 norm: 925.7687277843347\n",
      "l1 norm: 775.9219979911269\n",
      "Rbeta: 925.9983483878431\n",
      "\n",
      "Train set: Avg. loss: 0.000096480, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.075645 101.49934\n",
      "l2 norm: 925.6852722960019\n",
      "l1 norm: 775.8535659228014\n",
      "Rbeta: 925.9150376705709\n",
      "\n",
      "Train set: Avg. loss: 0.000096179, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.109474 101.53666\n",
      "l2 norm: 925.5895837164643\n",
      "l1 norm: 775.7749460270171\n",
      "Rbeta: 925.8195441308465\n",
      "\n",
      "Train set: Avg. loss: 0.000095879, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.14331 101.57391\n",
      "l2 norm: 925.4884674594098\n",
      "l1 norm: 775.6917852633228\n",
      "Rbeta: 925.718584944586\n",
      "\n",
      "Train set: Avg. loss: 0.000095580, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.177155 101.6111\n",
      "l2 norm: 925.3848747557735\n",
      "l1 norm: 775.6064871089659\n",
      "Rbeta: 925.6151879542012\n",
      "\n",
      "Train set: Avg. loss: 0.000095282, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.211 101.648346\n",
      "l2 norm: 925.2819870184735\n",
      "l1 norm: 775.5219114338615\n",
      "Rbeta: 925.5124287048412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000094985, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.24485 101.6855\n",
      "l2 norm: 925.1802405784593\n",
      "l1 norm: 775.4383855080794\n",
      "Rbeta: 925.4108861948977\n",
      "\n",
      "Train set: Avg. loss: 0.000094691, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.27843 101.72252\n",
      "l2 norm: 925.0703993659134\n",
      "l1 norm: 775.3481875798623\n",
      "Rbeta: 925.301168917228\n",
      "\n",
      "Train set: Avg. loss: 0.000094405, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.31041 101.759285\n",
      "l2 norm: 924.9825733827284\n",
      "l1 norm: 775.276327105587\n",
      "Rbeta: 925.2136731158715\n",
      "\n",
      "Train set: Avg. loss: 0.000094121, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.3424 101.79602\n",
      "l2 norm: 924.8837654618238\n",
      "l1 norm: 775.1952372803137\n",
      "Rbeta: 925.1151613844904\n",
      "\n",
      "Train set: Avg. loss: 0.000093837, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.3744 101.832664\n",
      "l2 norm: 924.7914068212297\n",
      "l1 norm: 775.1195652214601\n",
      "Rbeta: 925.0230980389335\n",
      "\n",
      "Train set: Avg. loss: 0.000093555, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.406395 101.8692\n",
      "l2 norm: 924.6850534078156\n",
      "l1 norm: 775.0321457535038\n",
      "Rbeta: 924.9170658132236\n",
      "\n",
      "Train set: Avg. loss: 0.000093274, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.4384 101.90555\n",
      "l2 norm: 924.5889374554934\n",
      "l1 norm: 774.9532280735309\n",
      "Rbeta: 924.821207054934\n",
      "\n",
      "Train set: Avg. loss: 0.000092995, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.470406 101.94179\n",
      "l2 norm: 924.5109937809149\n",
      "l1 norm: 774.8895661979448\n",
      "Rbeta: 924.7435403587989\n",
      "\n",
      "Train set: Avg. loss: 0.000092723, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.50108 101.977905\n",
      "l2 norm: 924.4302494910985\n",
      "l1 norm: 774.8235627614849\n",
      "Rbeta: 924.6631096063699\n",
      "\n",
      "Train set: Avg. loss: 0.000092453, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.53122 102.01391\n",
      "l2 norm: 924.3514790292253\n",
      "l1 norm: 774.7592959039196\n",
      "Rbeta: 924.5848203389713\n",
      "\n",
      "Train set: Avg. loss: 0.000092185, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.56136 102.04985\n",
      "l2 norm: 924.2801667925647\n",
      "l1 norm: 774.7012736075928\n",
      "Rbeta: 924.5138995096564\n",
      "\n",
      "Train set: Avg. loss: 0.000091918, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.59151 102.08573\n",
      "l2 norm: 924.1951996317641\n",
      "l1 norm: 774.6316184749965\n",
      "Rbeta: 924.4294388277782\n",
      "\n",
      "Train set: Avg. loss: 0.000091652, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.62166 102.12157\n",
      "l2 norm: 924.0952620549017\n",
      "l1 norm: 774.5493163937894\n",
      "Rbeta: 924.3298519049432\n",
      "\n",
      "Train set: Avg. loss: 0.000091387, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.65181 102.15728\n",
      "l2 norm: 924.0006942056451\n",
      "l1 norm: 774.4715893823632\n",
      "Rbeta: 924.2356483198905\n",
      "\n",
      "Train set: Avg. loss: 0.000091123, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.68197 102.19291\n",
      "l2 norm: 923.9020645926843\n",
      "l1 norm: 774.390477014851\n",
      "Rbeta: 924.1374235746064\n",
      "\n",
      "Train set: Avg. loss: 0.000090860, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.712135 102.22845\n",
      "l2 norm: 923.8172836180926\n",
      "l1 norm: 774.3208183810466\n",
      "Rbeta: 924.053052154463\n",
      "\n",
      "Train set: Avg. loss: 0.000090599, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.7423 102.263794\n",
      "l2 norm: 923.7458117412866\n",
      "l1 norm: 774.2622672617538\n",
      "Rbeta: 923.9819625641632\n",
      "\n",
      "Train set: Avg. loss: 0.000090339, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.77248 102.29901\n",
      "l2 norm: 923.6770608766168\n",
      "l1 norm: 774.2060625589995\n",
      "Rbeta: 923.9135386876663\n",
      "\n",
      "Train set: Avg. loss: 0.000090080, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.80266 102.33416\n",
      "l2 norm: 923.6015294321688\n",
      "l1 norm: 774.1441416649949\n",
      "Rbeta: 923.8383426623144\n",
      "\n",
      "Train set: Avg. loss: 0.000089821, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.83284 102.3693\n",
      "l2 norm: 923.5368567125241\n",
      "l1 norm: 774.0913281174085\n",
      "Rbeta: 923.7740759294954\n",
      "\n",
      "Train set: Avg. loss: 0.000089565, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.86303 102.40427\n",
      "l2 norm: 923.472743483784\n",
      "l1 norm: 774.0391006905961\n",
      "Rbeta: 923.7102852401671\n",
      "\n",
      "Train set: Avg. loss: 0.000089309, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.89322 102.43912\n",
      "l2 norm: 923.4199822387595\n",
      "l1 norm: 773.9963269747595\n",
      "Rbeta: 923.657785201442\n",
      "\n",
      "Train set: Avg. loss: 0.000089054, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.923416 102.47393\n",
      "l2 norm: 923.365041814025\n",
      "l1 norm: 773.9518722314285\n",
      "Rbeta: 923.6031547630494\n",
      "\n",
      "Train set: Avg. loss: 0.000088800, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.95361 102.50871\n",
      "l2 norm: 923.2960598387331\n",
      "l1 norm: 773.8955496981481\n",
      "Rbeta: 923.5344841588437\n",
      "\n",
      "Train set: Avg. loss: 0.000088547, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.983826 102.54352\n",
      "l2 norm: 923.2224604746708\n",
      "l1 norm: 773.8352318835073\n",
      "Rbeta: 923.4612560280391\n",
      "\n",
      "Train set: Avg. loss: 0.000088294, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.01403 102.57826\n",
      "l2 norm: 923.1396592998902\n",
      "l1 norm: 773.7670947198993\n",
      "Rbeta: 923.3786783155634\n",
      "\n",
      "Train set: Avg. loss: 0.000088043, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.04425 102.61297\n",
      "l2 norm: 923.0635999808966\n",
      "l1 norm: 773.7045673592206\n",
      "Rbeta: 923.3030059454366\n",
      "\n",
      "Train set: Avg. loss: 0.000087793, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.07447 102.64746\n",
      "l2 norm: 922.9817356449072\n",
      "l1 norm: 773.6371152942186\n",
      "Rbeta: 923.2213364681679\n",
      "\n",
      "Train set: Avg. loss: 0.000087544, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.10469 102.6819\n",
      "l2 norm: 922.9034166407905\n",
      "l1 norm: 773.5727188052596\n",
      "Rbeta: 923.1433251646469\n",
      "\n",
      "Train set: Avg. loss: 0.000087296, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.134926 102.71628\n",
      "l2 norm: 922.8231875807471\n",
      "l1 norm: 773.5067745909123\n",
      "Rbeta: 923.0633267846166\n",
      "\n",
      "Train set: Avg. loss: 0.000087049, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.16515 102.75054\n",
      "l2 norm: 922.7376659003543\n",
      "l1 norm: 773.4365551036178\n",
      "Rbeta: 922.978038033073\n",
      "\n",
      "Train set: Avg. loss: 0.000086803, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.1954 102.78459\n",
      "l2 norm: 922.6445759540463\n",
      "l1 norm: 773.3602318674152\n",
      "Rbeta: 922.8851450364212\n",
      "\n",
      "Train set: Avg. loss: 0.000086559, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.22564 102.81855\n",
      "l2 norm: 922.5567058887909\n",
      "l1 norm: 773.2881963533566\n",
      "Rbeta: 922.7975341355854\n",
      "\n",
      "Train set: Avg. loss: 0.000086315, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.25589 102.85249\n",
      "l2 norm: 922.4778856460418\n",
      "l1 norm: 773.2238888223728\n",
      "Rbeta: 922.7189469091971\n",
      "\n",
      "Train set: Avg. loss: 0.000086072, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.28614 102.88634\n",
      "l2 norm: 922.3971465035725\n",
      "l1 norm: 773.157898508405\n",
      "Rbeta: 922.6384269588283\n",
      "\n",
      "Train set: Avg. loss: 0.000085830, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.3164 102.92009\n",
      "l2 norm: 922.3192502213608\n",
      "l1 norm: 773.0943167818536\n",
      "Rbeta: 922.5606968661871\n",
      "\n",
      "Train set: Avg. loss: 0.000085589, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.34666 102.953766\n",
      "l2 norm: 922.2309610073693\n",
      "l1 norm: 773.0219904482509\n",
      "Rbeta: 922.4725448335689\n",
      "\n",
      "Train set: Avg. loss: 0.000085349, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.37692 102.987404\n",
      "l2 norm: 922.1361647117543\n",
      "l1 norm: 772.9440622474515\n",
      "Rbeta: 922.3779039893147\n",
      "\n",
      "Train set: Avg. loss: 0.000085110, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.407196 103.02098\n",
      "l2 norm: 922.0487224554738\n",
      "l1 norm: 772.8723811596024\n",
      "Rbeta: 922.2906841492497\n",
      "\n",
      "Train set: Avg. loss: 0.000084872, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.43747 103.05445\n",
      "l2 norm: 921.9747890922025\n",
      "l1 norm: 772.8120244007447\n",
      "Rbeta: 922.2168773566691\n",
      "\n",
      "Train set: Avg. loss: 0.000084635, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.46775 103.08776\n",
      "l2 norm: 921.9149330559059\n",
      "l1 norm: 772.7634746733884\n",
      "Rbeta: 922.1572118160158\n",
      "\n",
      "Train set: Avg. loss: 0.000084400, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.49804 103.12095\n",
      "l2 norm: 921.8496364757234\n",
      "l1 norm: 772.7103684970027\n",
      "Rbeta: 922.0920689466234\n",
      "\n",
      "Train set: Avg. loss: 0.000084165, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.52833 103.15396\n",
      "l2 norm: 921.7776342366056\n",
      "l1 norm: 772.6515771200595\n",
      "Rbeta: 922.0201093180876\n",
      "\n",
      "Train set: Avg. loss: 0.000083932, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.558624 103.18689\n",
      "l2 norm: 921.7046519350799\n",
      "l1 norm: 772.5917309041815\n",
      "Rbeta: 921.9472993285859\n",
      "\n",
      "Train set: Avg. loss: 0.000083699, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.58892 103.21972\n",
      "l2 norm: 921.6326379843572\n",
      "l1 norm: 772.5325290042612\n",
      "Rbeta: 921.8753763137329\n",
      "\n",
      "Train set: Avg. loss: 0.000083468, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.619225 103.2525\n",
      "l2 norm: 921.5579345342907\n",
      "l1 norm: 772.4709879979259\n",
      "Rbeta: 921.800798535775\n",
      "\n",
      "Train set: Avg. loss: 0.000083237, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.649536 103.28527\n",
      "l2 norm: 921.4865674277668\n",
      "l1 norm: 772.4122684523109\n",
      "Rbeta: 921.7295324606672\n",
      "\n",
      "Train set: Avg. loss: 0.000083007, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.67985 103.318\n",
      "l2 norm: 921.4281961509902\n",
      "l1 norm: 772.3645111652648\n",
      "Rbeta: 921.6712098492932\n",
      "\n",
      "Train set: Avg. loss: 0.000082777, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.71017 103.350655\n",
      "l2 norm: 921.3645120572502\n",
      "l1 norm: 772.3123035879631\n",
      "Rbeta: 921.6075789993937\n",
      "\n",
      "Train set: Avg. loss: 0.000082549, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.740486 103.38324\n",
      "l2 norm: 921.290289103152\n",
      "l1 norm: 772.2513338716196\n",
      "Rbeta: 921.5333810168769\n",
      "\n",
      "Train set: Avg. loss: 0.000082322, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.77081 103.415695\n",
      "l2 norm: 921.2072355125076\n",
      "l1 norm: 772.1830051675137\n",
      "Rbeta: 921.4504315809904\n",
      "\n",
      "Train set: Avg. loss: 0.000082095, Accuracy: 512/512 (100%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.80115 103.448105\n",
      "l2 norm: 921.1300490773115\n",
      "l1 norm: 772.1196162680817\n",
      "Rbeta: 921.3732819268798\n",
      "\n",
      "Train set: Avg. loss: 0.000081869, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.83148 103.4805\n",
      "l2 norm: 921.0563370577352\n",
      "l1 norm: 772.0590110364869\n",
      "Rbeta: 921.2996245023992\n",
      "\n",
      "Train set: Avg. loss: 0.000081645, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.861824 103.51277\n",
      "l2 norm: 920.9839910865574\n",
      "l1 norm: 771.9995226649071\n",
      "Rbeta: 921.2273504435835\n",
      "\n",
      "Train set: Avg. loss: 0.000081421, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.892166 103.54488\n",
      "l2 norm: 920.90204481242\n",
      "l1 norm: 771.9321095837302\n",
      "Rbeta: 921.1453052326331\n",
      "\n",
      "Train set: Avg. loss: 0.000081198, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.922516 103.57692\n",
      "l2 norm: 920.8211213830843\n",
      "l1 norm: 771.8655338091397\n",
      "Rbeta: 921.0644279809596\n",
      "\n",
      "Train set: Avg. loss: 0.000080976, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.95287 103.6089\n",
      "l2 norm: 920.7345759883975\n",
      "l1 norm: 771.7942280726168\n",
      "Rbeta: 920.9778462990196\n",
      "\n",
      "Train set: Avg. loss: 0.000080754, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.98323 103.6409\n",
      "l2 norm: 920.6614985104934\n",
      "l1 norm: 771.7343215179965\n",
      "Rbeta: 920.904819299384\n",
      "\n",
      "Train set: Avg. loss: 0.000080534, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.013596 103.67279\n",
      "l2 norm: 920.6065153256322\n",
      "l1 norm: 771.6895350483142\n",
      "Rbeta: 920.8498160880134\n",
      "\n",
      "Train set: Avg. loss: 0.000080314, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.04397 103.70459\n",
      "l2 norm: 920.5466769832749\n",
      "l1 norm: 771.6406326476847\n",
      "Rbeta: 920.7900201174999\n",
      "\n",
      "Train set: Avg. loss: 0.000080095, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.07434 103.7364\n",
      "l2 norm: 920.4768460694297\n",
      "l1 norm: 771.5833057046311\n",
      "Rbeta: 920.7201089949223\n",
      "\n",
      "Train set: Avg. loss: 0.000079877, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.10472 103.76809\n",
      "l2 norm: 920.4026106251954\n",
      "l1 norm: 771.5222932548422\n",
      "Rbeta: 920.6458514462042\n",
      "\n",
      "Train set: Avg. loss: 0.000079660, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.1351 103.79971\n",
      "l2 norm: 920.3185658361704\n",
      "l1 norm: 771.4529213356966\n",
      "Rbeta: 920.5617915674593\n",
      "\n",
      "Train set: Avg. loss: 0.000079443, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.16549 103.83127\n",
      "l2 norm: 920.2327138558134\n",
      "l1 norm: 771.3819582781136\n",
      "Rbeta: 920.4759045461686\n",
      "\n",
      "Train set: Avg. loss: 0.000079227, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.195885 103.86278\n",
      "l2 norm: 920.160524928091\n",
      "l1 norm: 771.3223900620105\n",
      "Rbeta: 920.4036100097936\n",
      "\n",
      "Train set: Avg. loss: 0.000079012, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.22628 103.8942\n",
      "l2 norm: 920.0999025312218\n",
      "l1 norm: 771.2724633319426\n",
      "Rbeta: 920.3429627633113\n",
      "\n",
      "Train set: Avg. loss: 0.000078798, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.25668 103.92554\n",
      "l2 norm: 920.0357280599956\n",
      "l1 norm: 771.2196201982326\n",
      "Rbeta: 920.2787159197976\n",
      "\n",
      "Train set: Avg. loss: 0.000078585, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.28709 103.956825\n",
      "l2 norm: 919.9504336868343\n",
      "l1 norm: 771.1491194717773\n",
      "Rbeta: 920.193325855459\n",
      "\n",
      "Train set: Avg. loss: 0.000078372, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.3175 103.98804\n",
      "l2 norm: 919.8836327128095\n",
      "l1 norm: 771.0941613163898\n",
      "Rbeta: 920.1264669746342\n",
      "\n",
      "Train set: Avg. loss: 0.000078161, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.347916 104.019135\n",
      "l2 norm: 919.8183330617993\n",
      "l1 norm: 771.0405308376112\n",
      "Rbeta: 920.0610908843803\n",
      "\n",
      "Train set: Avg. loss: 0.000077950, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.378334 104.05011\n",
      "l2 norm: 919.7491135034783\n",
      "l1 norm: 770.9835632487474\n",
      "Rbeta: 919.9917396618744\n",
      "\n",
      "Train set: Avg. loss: 0.000077741, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.40876 104.08098\n",
      "l2 norm: 919.6913230472095\n",
      "l1 norm: 770.936178796615\n",
      "Rbeta: 919.9338702232502\n",
      "\n",
      "Train set: Avg. loss: 0.000077532, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.439186 104.11189\n",
      "l2 norm: 919.6262304383796\n",
      "l1 norm: 770.8828212141627\n",
      "Rbeta: 919.8686855620206\n",
      "\n",
      "Train set: Avg. loss: 0.000077323, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.46962 104.142654\n",
      "l2 norm: 919.5571484903272\n",
      "l1 norm: 770.8261661240144\n",
      "Rbeta: 919.7993927953721\n",
      "\n",
      "Train set: Avg. loss: 0.000077116, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.50006 104.1734\n",
      "l2 norm: 919.4749436739897\n",
      "l1 norm: 770.7584456911866\n",
      "Rbeta: 919.717032021114\n",
      "\n",
      "Train set: Avg. loss: 0.000076909, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.5305 104.20413\n",
      "l2 norm: 919.4094088436601\n",
      "l1 norm: 770.7045308749948\n",
      "Rbeta: 919.6514300154766\n",
      "\n",
      "Train set: Avg. loss: 0.000076703, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.56095 104.234726\n",
      "l2 norm: 919.3546346076128\n",
      "l1 norm: 770.6595976036858\n",
      "Rbeta: 919.5965175480827\n",
      "\n",
      "Train set: Avg. loss: 0.000076498, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.59141 104.26523\n",
      "l2 norm: 919.2983889124738\n",
      "l1 norm: 770.6133063344903\n",
      "Rbeta: 919.5400502001628\n",
      "\n",
      "Train set: Avg. loss: 0.000076294, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.621864 104.295654\n",
      "l2 norm: 919.2484910341713\n",
      "l1 norm: 770.5724311099557\n",
      "Rbeta: 919.4900206908683\n",
      "\n",
      "Train set: Avg. loss: 0.000076090, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.65233 104.32596\n",
      "l2 norm: 919.191719059378\n",
      "l1 norm: 770.5258490604684\n",
      "Rbeta: 919.4331223996694\n",
      "\n",
      "Train set: Avg. loss: 0.000075888, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.68279 104.35611\n",
      "l2 norm: 919.12923229586\n",
      "l1 norm: 770.4744856453208\n",
      "Rbeta: 919.3704539727594\n",
      "\n",
      "Train set: Avg. loss: 0.000075687, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.713264 104.386154\n",
      "l2 norm: 919.050136545334\n",
      "l1 norm: 770.4092035699937\n",
      "Rbeta: 919.2911228470098\n",
      "\n",
      "Train set: Avg. loss: 0.000075486, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.743744 104.416145\n",
      "l2 norm: 918.9823643683209\n",
      "l1 norm: 770.3533111783063\n",
      "Rbeta: 919.223110236828\n",
      "\n",
      "Train set: Avg. loss: 0.000075286, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.77422 104.446\n",
      "l2 norm: 918.9270398949956\n",
      "l1 norm: 770.3079095897378\n",
      "Rbeta: 919.167544185812\n",
      "\n",
      "Train set: Avg. loss: 0.000075087, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.80471 104.47587\n",
      "l2 norm: 918.8631317895852\n",
      "l1 norm: 770.25540515429\n",
      "Rbeta: 919.1034664701126\n",
      "\n",
      "Train set: Avg. loss: 0.000074888, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.8352 104.505684\n",
      "l2 norm: 918.79843452555\n",
      "l1 norm: 770.2023189528626\n",
      "Rbeta: 919.0385450958105\n",
      "\n",
      "Train set: Avg. loss: 0.000074690, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.86569 104.53547\n",
      "l2 norm: 918.7274123641948\n",
      "l1 norm: 770.1439617971877\n",
      "Rbeta: 918.9672542700222\n",
      "\n",
      "Train set: Avg. loss: 0.000074493, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.896194 104.56523\n",
      "l2 norm: 918.6419610528376\n",
      "l1 norm: 770.0735740751728\n",
      "Rbeta: 918.8816169218331\n",
      "\n",
      "Train set: Avg. loss: 0.000074296, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.9267 104.594986\n",
      "l2 norm: 918.5650168759594\n",
      "l1 norm: 770.0103400800019\n",
      "Rbeta: 918.8044038717569\n",
      "\n",
      "Train set: Avg. loss: 0.000074100, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.95721 104.624664\n",
      "l2 norm: 918.4841704379951\n",
      "l1 norm: 769.9438277124788\n",
      "Rbeta: 918.723347936672\n",
      "\n",
      "Train set: Avg. loss: 0.000073904, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.98772 104.65428\n",
      "l2 norm: 918.4069553827607\n",
      "l1 norm: 769.88039492797\n",
      "Rbeta: 918.6458213708341\n",
      "\n",
      "Train set: Avg. loss: 0.000073709, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.018234 104.68396\n",
      "l2 norm: 918.3282503572174\n",
      "l1 norm: 769.8155897599788\n",
      "Rbeta: 918.5668531282708\n",
      "\n",
      "Train set: Avg. loss: 0.000073515, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.04876 104.71364\n",
      "l2 norm: 918.2496271833853\n",
      "l1 norm: 769.7508530852568\n",
      "Rbeta: 918.4880062718315\n",
      "\n",
      "Train set: Avg. loss: 0.000073321, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.079285 104.74321\n",
      "l2 norm: 918.1723219417307\n",
      "l1 norm: 769.68729201671\n",
      "Rbeta: 918.4104656020145\n",
      "\n",
      "Train set: Avg. loss: 0.000073128, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.10982 104.77264\n",
      "l2 norm: 918.1056406238911\n",
      "l1 norm: 769.6325704893804\n",
      "Rbeta: 918.3434628491913\n",
      "\n",
      "Train set: Avg. loss: 0.000072936, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.14035 104.801956\n",
      "l2 norm: 918.028643438386\n",
      "l1 norm: 769.5691801498509\n",
      "Rbeta: 918.2662279588691\n",
      "\n",
      "Train set: Avg. loss: 0.000072745, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.1709 104.831116\n",
      "l2 norm: 917.9541896822742\n",
      "l1 norm: 769.5079377661984\n",
      "Rbeta: 918.1914930622992\n",
      "\n",
      "Train set: Avg. loss: 0.000072555, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.20144 104.86017\n",
      "l2 norm: 917.8845511501008\n",
      "l1 norm: 769.4506291435284\n",
      "Rbeta: 918.1215421960432\n",
      "\n",
      "Train set: Avg. loss: 0.000072365, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.231995 104.88921\n",
      "l2 norm: 917.8114388428452\n",
      "l1 norm: 769.3904346777801\n",
      "Rbeta: 918.0481526963086\n",
      "\n",
      "Train set: Avg. loss: 0.000072176, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.26255 104.91817\n",
      "l2 norm: 917.7413124079535\n",
      "l1 norm: 769.3327259443636\n",
      "Rbeta: 917.97771271007\n",
      "\n",
      "Train set: Avg. loss: 0.000071988, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.293106 104.947075\n",
      "l2 norm: 917.6867689945605\n",
      "l1 norm: 769.2881746300552\n",
      "Rbeta: 917.9227729629396\n",
      "\n",
      "Train set: Avg. loss: 0.000071800, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.32367 104.97591\n",
      "l2 norm: 917.628704059536\n",
      "l1 norm: 769.2407063938392\n",
      "Rbeta: 917.8643648875286\n",
      "\n",
      "Train set: Avg. loss: 0.000071614, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.35424 105.004555\n",
      "l2 norm: 917.5737865914772\n",
      "l1 norm: 769.1958823960554\n",
      "Rbeta: 917.8091527208321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000071428, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.38481 105.03313\n",
      "l2 norm: 917.5271516110697\n",
      "l1 norm: 769.1579846132388\n",
      "Rbeta: 917.7621309999735\n",
      "\n",
      "Train set: Avg. loss: 0.000071243, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.41539 105.061584\n",
      "l2 norm: 917.4894924442085\n",
      "l1 norm: 769.1275917286879\n",
      "Rbeta: 917.7241310278587\n",
      "\n",
      "Train set: Avg. loss: 0.000071058, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.44598 105.09004\n",
      "l2 norm: 917.4422743913924\n",
      "l1 norm: 769.0892405868543\n",
      "Rbeta: 917.6765094558358\n",
      "\n",
      "Train set: Avg. loss: 0.000070875, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.47656 105.11841\n",
      "l2 norm: 917.4003495210821\n",
      "l1 norm: 769.0552994455782\n",
      "Rbeta: 917.6342554494914\n",
      "\n",
      "Train set: Avg. loss: 0.000070691, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.50716 105.14676\n",
      "l2 norm: 917.3591462393381\n",
      "l1 norm: 769.0218256111816\n",
      "Rbeta: 917.5926935657948\n",
      "\n",
      "Train set: Avg. loss: 0.000070509, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.53776 105.17503\n",
      "l2 norm: 917.2893050591873\n",
      "l1 norm: 768.9642765405065\n",
      "Rbeta: 917.5225301303632\n",
      "\n",
      "Train set: Avg. loss: 0.000070327, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.56836 105.20323\n",
      "l2 norm: 917.2073533036485\n",
      "l1 norm: 768.8964588226753\n",
      "Rbeta: 917.4401235203568\n",
      "\n",
      "Train set: Avg. loss: 0.000070146, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.59896 105.231415\n",
      "l2 norm: 917.1276317101667\n",
      "l1 norm: 768.8306324824227\n",
      "Rbeta: 917.3600329672117\n",
      "\n",
      "Train set: Avg. loss: 0.000069965, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.62957 105.25952\n",
      "l2 norm: 917.0439615360058\n",
      "l1 norm: 768.7616658969366\n",
      "Rbeta: 917.2759475647081\n",
      "\n",
      "Train set: Avg. loss: 0.000069785, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.66019 105.28757\n",
      "l2 norm: 916.9556158260391\n",
      "l1 norm: 768.6889195162801\n",
      "Rbeta: 917.1871495368812\n",
      "\n",
      "Train set: Avg. loss: 0.000069606, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.69081 105.31554\n",
      "l2 norm: 916.8709509976331\n",
      "l1 norm: 768.6191042836467\n",
      "Rbeta: 917.1021019873566\n",
      "\n",
      "Train set: Avg. loss: 0.000069427, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.721436 105.34332\n",
      "l2 norm: 916.8065390109421\n",
      "l1 norm: 768.5662439635776\n",
      "Rbeta: 917.0372226171941\n",
      "\n",
      "Train set: Avg. loss: 0.000069250, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.75207 105.37101\n",
      "l2 norm: 916.749008990102\n",
      "l1 norm: 768.5192537566611\n",
      "Rbeta: 916.9792763143503\n",
      "\n",
      "Train set: Avg. loss: 0.000069073, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.7827 105.39856\n",
      "l2 norm: 916.706922720773\n",
      "l1 norm: 768.485304460198\n",
      "Rbeta: 916.9367401598514\n",
      "\n",
      "Train set: Avg. loss: 0.000068897, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.81334 105.42604\n",
      "l2 norm: 916.6689846120455\n",
      "l1 norm: 768.4549353000799\n",
      "Rbeta: 916.8983340203202\n",
      "\n",
      "Train set: Avg. loss: 0.000068721, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.84398 105.45356\n",
      "l2 norm: 916.6200488954723\n",
      "l1 norm: 768.4153829453995\n",
      "Rbeta: 916.8489838259303\n",
      "\n",
      "Train set: Avg. loss: 0.000068546, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.87463 105.481125\n",
      "l2 norm: 916.5564032752709\n",
      "l1 norm: 768.3633849254129\n",
      "Rbeta: 916.7848787269148\n",
      "\n",
      "Train set: Avg. loss: 0.000068371, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.90528 105.50865\n",
      "l2 norm: 916.4776312988046\n",
      "l1 norm: 768.2985341820447\n",
      "Rbeta: 916.7056379473986\n",
      "\n",
      "Train set: Avg. loss: 0.000068197, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.93594 105.53616\n",
      "l2 norm: 916.3914966271648\n",
      "l1 norm: 768.2274335842103\n",
      "Rbeta: 916.6190506557717\n",
      "\n",
      "Train set: Avg. loss: 0.000068023, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.966606 105.56367\n",
      "l2 norm: 916.3147733734008\n",
      "l1 norm: 768.1641311589084\n",
      "Rbeta: 916.541862252739\n",
      "\n",
      "Train set: Avg. loss: 0.000067849, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.99727 105.59111\n",
      "l2 norm: 916.2520523621567\n",
      "l1 norm: 768.1123543479837\n",
      "Rbeta: 916.4786360087709\n",
      "\n",
      "Train set: Avg. loss: 0.000067677, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.02794 105.618484\n",
      "l2 norm: 916.1966893557345\n",
      "l1 norm: 768.0666630186101\n",
      "Rbeta: 916.4228138273133\n",
      "\n",
      "Train set: Avg. loss: 0.000067504, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.05862 105.64577\n",
      "l2 norm: 916.1384556257044\n",
      "l1 norm: 768.0187375510832\n",
      "Rbeta: 916.3640814398118\n",
      "\n",
      "Train set: Avg. loss: 0.000067333, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.0893 105.67297\n",
      "l2 norm: 916.074398326095\n",
      "l1 norm: 767.9660272688961\n",
      "Rbeta: 916.2995520985122\n",
      "\n",
      "Train set: Avg. loss: 0.000067162, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.11999 105.700066\n",
      "l2 norm: 916.0064956703022\n",
      "l1 norm: 767.910151097243\n",
      "Rbeta: 916.2312076820665\n",
      "\n",
      "Train set: Avg. loss: 0.000066992, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.15067 105.72711\n",
      "l2 norm: 915.9436202215201\n",
      "l1 norm: 767.8585128482086\n",
      "Rbeta: 916.167780267012\n",
      "\n",
      "Train set: Avg. loss: 0.000066823, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.18137 105.75414\n",
      "l2 norm: 915.8765251964094\n",
      "l1 norm: 767.8032303052175\n",
      "Rbeta: 916.1002342627321\n",
      "\n",
      "Train set: Avg. loss: 0.000066654, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.212074 105.78111\n",
      "l2 norm: 915.8123032426428\n",
      "l1 norm: 767.7504465661248\n",
      "Rbeta: 916.0355465426002\n",
      "\n",
      "Train set: Avg. loss: 0.000066485, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.242775 105.80809\n",
      "l2 norm: 915.7449086157937\n",
      "l1 norm: 767.6950224721713\n",
      "Rbeta: 915.9676433816002\n",
      "\n",
      "Train set: Avg. loss: 0.000066317, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.27348 105.83506\n",
      "l2 norm: 915.6773321064235\n",
      "l1 norm: 767.6395602399213\n",
      "Rbeta: 915.899546676887\n",
      "\n",
      "Train set: Avg. loss: 0.000066149, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.3042 105.861984\n",
      "l2 norm: 915.6132290682752\n",
      "l1 norm: 767.5871248964941\n",
      "Rbeta: 915.8349154452515\n",
      "\n",
      "Train set: Avg. loss: 0.000065982, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.334915 105.888794\n",
      "l2 norm: 915.5430412248986\n",
      "l1 norm: 767.5296257451014\n",
      "Rbeta: 915.7641886165499\n",
      "\n",
      "Train set: Avg. loss: 0.000065816, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.36564 105.91551\n",
      "l2 norm: 915.476630914073\n",
      "l1 norm: 767.4752093353852\n",
      "Rbeta: 915.6972738553293\n",
      "\n",
      "Train set: Avg. loss: 0.000065650, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.39637 105.94211\n",
      "l2 norm: 915.4125586985217\n",
      "l1 norm: 767.4227142285688\n",
      "Rbeta: 915.6326313937918\n",
      "\n",
      "Train set: Avg. loss: 0.000065485, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.4271 105.96865\n",
      "l2 norm: 915.3549601960943\n",
      "l1 norm: 767.3755240141185\n",
      "Rbeta: 915.574524227434\n",
      "\n",
      "Train set: Avg. loss: 0.000065321, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.45783 105.99518\n",
      "l2 norm: 915.2862549938351\n",
      "l1 norm: 767.3189436858019\n",
      "Rbeta: 915.5051840508024\n",
      "\n",
      "Train set: Avg. loss: 0.000065157, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.48858 106.0217\n",
      "l2 norm: 915.2091501760198\n",
      "l1 norm: 767.255238773367\n",
      "Rbeta: 915.4276626163752\n",
      "\n",
      "Train set: Avg. loss: 0.000064994, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.519325 106.04809\n",
      "l2 norm: 915.1387154441029\n",
      "l1 norm: 767.1971329617359\n",
      "Rbeta: 915.3566219314495\n",
      "\n",
      "Train set: Avg. loss: 0.000064831, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.55007 106.074356\n",
      "l2 norm: 915.0843471030418\n",
      "l1 norm: 767.1525467240664\n",
      "Rbeta: 915.3016278756895\n",
      "\n",
      "Train set: Avg. loss: 0.000064669, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.580826 106.100464\n",
      "l2 norm: 915.0293235584493\n",
      "l1 norm: 767.1074142610518\n",
      "Rbeta: 915.2460654071434\n",
      "\n",
      "Train set: Avg. loss: 0.000064508, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.61159 106.126526\n",
      "l2 norm: 914.9870181929757\n",
      "l1 norm: 767.0728766372354\n",
      "Rbeta: 915.2031956392926\n",
      "\n",
      "Train set: Avg. loss: 0.000064347, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.64236 106.1526\n",
      "l2 norm: 914.9458675815791\n",
      "l1 norm: 767.0392247172246\n",
      "Rbeta: 915.1614415399885\n",
      "\n",
      "Train set: Avg. loss: 0.000064187, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.67313 106.17861\n",
      "l2 norm: 914.904218751631\n",
      "l1 norm: 767.005181475126\n",
      "Rbeta: 915.1192723917512\n",
      "\n",
      "Train set: Avg. loss: 0.000064027, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.703896 106.20454\n",
      "l2 norm: 914.855871082499\n",
      "l1 norm: 766.9654282857517\n",
      "Rbeta: 915.0702984582155\n",
      "\n",
      "Train set: Avg. loss: 0.000063868, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.73468 106.2304\n",
      "l2 norm: 914.8018140767476\n",
      "l1 norm: 766.9208580378433\n",
      "Rbeta: 915.0157029727404\n",
      "\n",
      "Train set: Avg. loss: 0.000063710, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.76546 106.25627\n",
      "l2 norm: 914.749349103784\n",
      "l1 norm: 766.8776638080628\n",
      "Rbeta: 914.9625758132064\n",
      "\n",
      "Train set: Avg. loss: 0.000063552, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.79625 106.282104\n",
      "l2 norm: 914.7014482050037\n",
      "l1 norm: 766.8383730440917\n",
      "Rbeta: 914.9140923443351\n",
      "\n",
      "Train set: Avg. loss: 0.000063394, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.82704 106.307884\n",
      "l2 norm: 914.6519028885092\n",
      "l1 norm: 766.7977714262659\n",
      "Rbeta: 914.8639339711757\n",
      "\n",
      "Train set: Avg. loss: 0.000063237, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.85784 106.33365\n",
      "l2 norm: 914.5967466386704\n",
      "l1 norm: 766.7524312359873\n",
      "Rbeta: 914.8082448862322\n",
      "\n",
      "Train set: Avg. loss: 0.000063080, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.88864 106.35939\n",
      "l2 norm: 914.5421967133952\n",
      "l1 norm: 766.7074711780522\n",
      "Rbeta: 914.7529905095439\n",
      "\n",
      "Train set: Avg. loss: 0.000062924, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.91945 106.38512\n",
      "l2 norm: 914.4866160432625\n",
      "l1 norm: 766.6616751888996\n",
      "Rbeta: 914.6968151674273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000062768, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.95026 106.41075\n",
      "l2 norm: 914.4341054535087\n",
      "l1 norm: 766.6185283528054\n",
      "Rbeta: 914.6436855216183\n",
      "\n",
      "Train set: Avg. loss: 0.000062612, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.98107 106.43634\n",
      "l2 norm: 914.378923790487\n",
      "l1 norm: 766.5731162179009\n",
      "Rbeta: 914.5879450854484\n",
      "\n",
      "Train set: Avg. loss: 0.000062458, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.011894 106.461815\n",
      "l2 norm: 914.3138197345307\n",
      "l1 norm: 766.5193513566275\n",
      "Rbeta: 914.5222009559369\n",
      "\n",
      "Train set: Avg. loss: 0.000062304, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.042725 106.487175\n",
      "l2 norm: 914.2581849499614\n",
      "l1 norm: 766.4734942300936\n",
      "Rbeta: 914.4659600363193\n",
      "\n",
      "Train set: Avg. loss: 0.000062151, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.07355 106.512474\n",
      "l2 norm: 914.1840679705917\n",
      "l1 norm: 766.4121999062222\n",
      "Rbeta: 914.3911438574451\n",
      "\n",
      "Train set: Avg. loss: 0.000061998, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.104385 106.537766\n",
      "l2 norm: 914.0966840096827\n",
      "l1 norm: 766.3397300742901\n",
      "Rbeta: 914.3031258299128\n",
      "\n",
      "Train set: Avg. loss: 0.000061845, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.13522 106.56303\n",
      "l2 norm: 914.0089835378168\n",
      "l1 norm: 766.2670534430664\n",
      "Rbeta: 914.2147400077561\n",
      "\n",
      "Train set: Avg. loss: 0.000061693, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.16607 106.58821\n",
      "l2 norm: 913.9303125089407\n",
      "l1 norm: 766.2019193203537\n",
      "Rbeta: 914.1353970434221\n",
      "\n",
      "Train set: Avg. loss: 0.000061542, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.196915 106.6133\n",
      "l2 norm: 913.8601352357545\n",
      "l1 norm: 766.1438564565376\n",
      "Rbeta: 914.0645935629783\n",
      "\n",
      "Train set: Avg. loss: 0.000061391, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.22777 106.63838\n",
      "l2 norm: 913.8015998907098\n",
      "l1 norm: 766.0955682173653\n",
      "Rbeta: 914.0054013082288\n",
      "\n",
      "Train set: Avg. loss: 0.000061241, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.25863 106.66339\n",
      "l2 norm: 913.7446083529484\n",
      "l1 norm: 766.0485473863282\n",
      "Rbeta: 913.9477687138207\n",
      "\n",
      "Train set: Avg. loss: 0.000061091, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.28949 106.68833\n",
      "l2 norm: 913.6926883547166\n",
      "l1 norm: 766.0057005235244\n",
      "Rbeta: 913.8951336781098\n",
      "\n",
      "Train set: Avg. loss: 0.000060942, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.32036 106.7133\n",
      "l2 norm: 913.6328003327477\n",
      "l1 norm: 765.9561764383511\n",
      "Rbeta: 913.8346438806645\n",
      "\n",
      "Train set: Avg. loss: 0.000060793, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.351234 106.73822\n",
      "l2 norm: 913.562132418671\n",
      "l1 norm: 765.8976809868924\n",
      "Rbeta: 913.763276091239\n",
      "\n",
      "Train set: Avg. loss: 0.000060644, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.38211 106.76308\n",
      "l2 norm: 913.4962694549683\n",
      "l1 norm: 765.8432632371332\n",
      "Rbeta: 913.6967538839202\n",
      "\n",
      "Train set: Avg. loss: 0.000060496, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.41299 106.78792\n",
      "l2 norm: 913.4445957267495\n",
      "l1 norm: 765.8007146316535\n",
      "Rbeta: 913.6443537865465\n",
      "\n",
      "Train set: Avg. loss: 0.000060348, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.44388 106.81275\n",
      "l2 norm: 913.3974926393886\n",
      "l1 norm: 765.7620039845992\n",
      "Rbeta: 913.5966307654995\n",
      "\n",
      "Train set: Avg. loss: 0.000060201, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.47476 106.8375\n",
      "l2 norm: 913.3522062070383\n",
      "l1 norm: 765.72484203108\n",
      "Rbeta: 913.5506180464048\n",
      "\n",
      "Train set: Avg. loss: 0.000060054, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.50566 106.86231\n",
      "l2 norm: 913.307080336491\n",
      "l1 norm: 765.6878502912605\n",
      "Rbeta: 913.5049150851073\n",
      "\n",
      "Train set: Avg. loss: 0.000059908, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.53656 106.88696\n",
      "l2 norm: 913.255826233672\n",
      "l1 norm: 765.6458154040133\n",
      "Rbeta: 913.4529209071657\n",
      "\n",
      "Train set: Avg. loss: 0.000059762, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.56747 106.91156\n",
      "l2 norm: 913.2090603883003\n",
      "l1 norm: 765.6075751683958\n",
      "Rbeta: 913.4055156232894\n",
      "\n",
      "Train set: Avg. loss: 0.000059617, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.59837 106.93605\n",
      "l2 norm: 913.1612744609986\n",
      "l1 norm: 765.5685328760862\n",
      "Rbeta: 913.3570150092897\n",
      "\n",
      "Train set: Avg. loss: 0.000059473, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.62929 106.9604\n",
      "l2 norm: 913.1174937611568\n",
      "l1 norm: 765.5328392038203\n",
      "Rbeta: 913.3125203769812\n",
      "\n",
      "Train set: Avg. loss: 0.000059329, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.66021 106.98465\n",
      "l2 norm: 913.0711620749425\n",
      "l1 norm: 765.4949907166415\n",
      "Rbeta: 913.2655205161377\n",
      "\n",
      "Train set: Avg. loss: 0.000059185, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.69113 107.00888\n",
      "l2 norm: 913.0276079189869\n",
      "l1 norm: 765.4594897086986\n",
      "Rbeta: 913.2212026094761\n",
      "\n",
      "Train set: Avg. loss: 0.000059042, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.72205 107.0331\n",
      "l2 norm: 912.9742021404703\n",
      "l1 norm: 765.4157520082227\n",
      "Rbeta: 913.167078517772\n",
      "\n",
      "Train set: Avg. loss: 0.000058900, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.75299 107.05721\n",
      "l2 norm: 912.920988733439\n",
      "l1 norm: 765.3722214617812\n",
      "Rbeta: 913.1131830064919\n",
      "\n",
      "Train set: Avg. loss: 0.000058758, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.78393 107.081314\n",
      "l2 norm: 912.8682311659753\n",
      "l1 norm: 765.3290922346466\n",
      "Rbeta: 913.0596629077995\n",
      "\n",
      "Train set: Avg. loss: 0.000058616, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.814865 107.1054\n",
      "l2 norm: 912.8340277979379\n",
      "l1 norm: 765.3014798553143\n",
      "Rbeta: 913.0246982159817\n",
      "\n",
      "Train set: Avg. loss: 0.000058475, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.84582 107.12938\n",
      "l2 norm: 912.7999191163249\n",
      "l1 norm: 765.2737786084692\n",
      "Rbeta: 912.9898888313757\n",
      "\n",
      "Train set: Avg. loss: 0.000058334, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.87676 107.153305\n",
      "l2 norm: 912.7539429638362\n",
      "l1 norm: 765.2361027696445\n",
      "Rbeta: 912.9431857515643\n",
      "\n",
      "Train set: Avg. loss: 0.000058194, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.90772 107.17712\n",
      "l2 norm: 912.7055780468297\n",
      "l1 norm: 765.1963198991058\n",
      "Rbeta: 912.894104242861\n",
      "\n",
      "Train set: Avg. loss: 0.000058055, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.93868 107.20086\n",
      "l2 norm: 912.6549071622578\n",
      "l1 norm: 765.1546761014298\n",
      "Rbeta: 912.8427131565572\n",
      "\n",
      "Train set: Avg. loss: 0.000057915, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.96965 107.22464\n",
      "l2 norm: 912.6023006031762\n",
      "l1 norm: 765.1115122418508\n",
      "Rbeta: 912.7893679280324\n",
      "\n",
      "Train set: Avg. loss: 0.000057776, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.00062 107.24839\n",
      "l2 norm: 912.5619552515286\n",
      "l1 norm: 765.0786559479096\n",
      "Rbeta: 912.7482646098006\n",
      "\n",
      "Train set: Avg. loss: 0.000057638, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.03159 107.27208\n",
      "l2 norm: 912.5212756506388\n",
      "l1 norm: 765.0455700289933\n",
      "Rbeta: 912.706869662279\n",
      "\n",
      "Train set: Avg. loss: 0.000057500, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.06257 107.2957\n",
      "l2 norm: 912.4781797640404\n",
      "l1 norm: 765.0104559362929\n",
      "Rbeta: 912.662985819978\n",
      "\n",
      "Train set: Avg. loss: 0.000057363, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.09355 107.319336\n",
      "l2 norm: 912.4293858798746\n",
      "l1 norm: 764.9704757173488\n",
      "Rbeta: 912.6134148499477\n",
      "\n",
      "Train set: Avg. loss: 0.000057225, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.12454 107.34296\n",
      "l2 norm: 912.3601553733339\n",
      "l1 norm: 764.9133490444806\n",
      "Rbeta: 912.5434750866333\n",
      "\n",
      "Train set: Avg. loss: 0.000057089, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.15553 107.366516\n",
      "l2 norm: 912.307735230775\n",
      "l1 norm: 764.8701921116171\n",
      "Rbeta: 912.490290961771\n",
      "\n",
      "Train set: Avg. loss: 0.000056952, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.18653 107.39002\n",
      "l2 norm: 912.2617477955481\n",
      "l1 norm: 764.8323364133933\n",
      "Rbeta: 912.4435406097991\n",
      "\n",
      "Train set: Avg. loss: 0.000056816, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.21754 107.4135\n",
      "l2 norm: 912.2131217983781\n",
      "l1 norm: 764.7922666147836\n",
      "Rbeta: 912.394220741385\n",
      "\n",
      "Train set: Avg. loss: 0.000056681, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.24854 107.43682\n",
      "l2 norm: 912.1685085623049\n",
      "l1 norm: 764.7556059449697\n",
      "Rbeta: 912.3487968539357\n",
      "\n",
      "Train set: Avg. loss: 0.000056547, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.27956 107.46004\n",
      "l2 norm: 912.115158954357\n",
      "l1 norm: 764.7116835217457\n",
      "Rbeta: 912.2947066825475\n",
      "\n",
      "Train set: Avg. loss: 0.000056412, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.31057 107.483185\n",
      "l2 norm: 912.0569698658105\n",
      "l1 norm: 764.6637144490821\n",
      "Rbeta: 912.2357392572267\n",
      "\n",
      "Train set: Avg. loss: 0.000056278, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.34159 107.50633\n",
      "l2 norm: 911.9920268797272\n",
      "l1 norm: 764.6100053338955\n",
      "Rbeta: 912.1699885032441\n",
      "\n",
      "Train set: Avg. loss: 0.000056145, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.37262 107.52942\n",
      "l2 norm: 911.923144446158\n",
      "l1 norm: 764.5530368918597\n",
      "Rbeta: 912.1003139192162\n",
      "\n",
      "Train set: Avg. loss: 0.000056012, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.40365 107.55244\n",
      "l2 norm: 911.8580075910588\n",
      "l1 norm: 764.4993051135702\n",
      "Rbeta: 912.0344172118013\n",
      "\n",
      "Train set: Avg. loss: 0.000055879, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.434685 107.575386\n",
      "l2 norm: 911.7990313761667\n",
      "l1 norm: 764.4507701236646\n",
      "Rbeta: 911.9746431095989\n",
      "\n",
      "Train set: Avg. loss: 0.000055747, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.46572 107.59827\n",
      "l2 norm: 911.742457418527\n",
      "l1 norm: 764.4043606795866\n",
      "Rbeta: 911.9172856791425\n",
      "\n",
      "Train set: Avg. loss: 0.000055616, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.496765 107.62105\n",
      "l2 norm: 911.6869301891118\n",
      "l1 norm: 764.3587869040209\n",
      "Rbeta: 911.8609706267895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000055484, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.52782 107.64374\n",
      "l2 norm: 911.6329594969543\n",
      "l1 norm: 764.314500336481\n",
      "Rbeta: 911.8061804595357\n",
      "\n",
      "Train set: Avg. loss: 0.000055354, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.55887 107.6664\n",
      "l2 norm: 911.5746108270519\n",
      "l1 norm: 764.2666573831475\n",
      "Rbeta: 911.7470119967799\n",
      "\n",
      "Train set: Avg. loss: 0.000055224, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.58993 107.68902\n",
      "l2 norm: 911.5107203886905\n",
      "l1 norm: 764.2142573597873\n",
      "Rbeta: 911.6823564678136\n",
      "\n",
      "Train set: Avg. loss: 0.000055094, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.62099 107.71166\n",
      "l2 norm: 911.4521067667233\n",
      "l1 norm: 764.1662076770665\n",
      "Rbeta: 911.6228481820051\n",
      "\n",
      "Train set: Avg. loss: 0.000054964, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.652054 107.734314\n",
      "l2 norm: 911.3942317586426\n",
      "l1 norm: 764.1187565227922\n",
      "Rbeta: 911.5642123999576\n",
      "\n",
      "Train set: Avg. loss: 0.000054835, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.68313 107.75687\n",
      "l2 norm: 911.3392190469792\n",
      "l1 norm: 764.0737112379118\n",
      "Rbeta: 911.5084397166811\n",
      "\n",
      "Train set: Avg. loss: 0.000054706, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.7142 107.77935\n",
      "l2 norm: 911.2875275168683\n",
      "l1 norm: 764.0314452903347\n",
      "Rbeta: 911.4559437899995\n",
      "\n",
      "Train set: Avg. loss: 0.000054578, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.745285 107.80175\n",
      "l2 norm: 911.2392922326582\n",
      "l1 norm: 763.9920781892133\n",
      "Rbeta: 911.4068307424175\n",
      "\n",
      "Train set: Avg. loss: 0.000054450, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.776375 107.824104\n",
      "l2 norm: 911.1908474827915\n",
      "l1 norm: 763.9524540182172\n",
      "Rbeta: 911.3576235938082\n",
      "\n",
      "Train set: Avg. loss: 0.000054323, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.807465 107.84651\n",
      "l2 norm: 911.1395816596987\n",
      "l1 norm: 763.9104504684535\n",
      "Rbeta: 911.3055448659507\n",
      "\n",
      "Train set: Avg. loss: 0.000054195, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.83856 107.868904\n",
      "l2 norm: 911.086806402074\n",
      "l1 norm: 763.867195580532\n",
      "Rbeta: 911.2519131070038\n",
      "\n",
      "Train set: Avg. loss: 0.000054068, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.86966 107.89125\n",
      "l2 norm: 911.0306782446644\n",
      "l1 norm: 763.8211683124541\n",
      "Rbeta: 911.194970056824\n",
      "\n",
      "Train set: Avg. loss: 0.000053942, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.900764 107.91351\n",
      "l2 norm: 910.9691476258233\n",
      "l1 norm: 763.7706176960235\n",
      "Rbeta: 911.1326464303208\n",
      "\n",
      "Train set: Avg. loss: 0.000053816, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.93188 107.9357\n",
      "l2 norm: 910.9126152326656\n",
      "l1 norm: 763.7242869790448\n",
      "Rbeta: 911.0753616602287\n",
      "\n",
      "Train set: Avg. loss: 0.000053690, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.96299 107.957855\n",
      "l2 norm: 910.8618308686647\n",
      "l1 norm: 763.6827805063124\n",
      "Rbeta: 911.0236863639002\n",
      "\n",
      "Train set: Avg. loss: 0.000053565, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.99411 107.98003\n",
      "l2 norm: 910.8153998879404\n",
      "l1 norm: 763.6449007597515\n",
      "Rbeta: 910.9764982704032\n",
      "\n",
      "Train set: Avg. loss: 0.000053439, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.02523 108.00219\n",
      "l2 norm: 910.7731607329324\n",
      "l1 norm: 763.610490672661\n",
      "Rbeta: 910.9334137764207\n",
      "\n",
      "Train set: Avg. loss: 0.000053315, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.05636 108.02434\n",
      "l2 norm: 910.7341081189631\n",
      "l1 norm: 763.5787225429656\n",
      "Rbeta: 910.8935541478043\n",
      "\n",
      "Train set: Avg. loss: 0.000053190, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.087494 108.04645\n",
      "l2 norm: 910.689584196818\n",
      "l1 norm: 763.5422732001027\n",
      "Rbeta: 910.8482326518981\n",
      "\n",
      "Train set: Avg. loss: 0.000053066, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.11863 108.06852\n",
      "l2 norm: 910.6469592793464\n",
      "l1 norm: 763.5073774555175\n",
      "Rbeta: 910.8047809254216\n",
      "\n",
      "Train set: Avg. loss: 0.000052942, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.14977 108.09061\n",
      "l2 norm: 910.6172312914298\n",
      "l1 norm: 763.4833781939577\n",
      "Rbeta: 910.77429847504\n",
      "\n",
      "Train set: Avg. loss: 0.000052819, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.180916 108.11272\n",
      "l2 norm: 910.5723469807738\n",
      "l1 norm: 763.4467057496036\n",
      "Rbeta: 910.7285870875058\n",
      "\n",
      "Train set: Avg. loss: 0.000052696, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.21207 108.13478\n",
      "l2 norm: 910.5185549156615\n",
      "l1 norm: 763.4024911156205\n",
      "Rbeta: 910.6739772683555\n",
      "\n",
      "Train set: Avg. loss: 0.000052573, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.243225 108.15686\n",
      "l2 norm: 910.4626795123581\n",
      "l1 norm: 763.3565014089626\n",
      "Rbeta: 910.6172633558721\n",
      "\n",
      "Train set: Avg. loss: 0.000052450, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.27438 108.17888\n",
      "l2 norm: 910.4159661439152\n",
      "l1 norm: 763.3182574266168\n",
      "Rbeta: 910.5697442599767\n",
      "\n",
      "Train set: Avg. loss: 0.000052328, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.30555 108.20089\n",
      "l2 norm: 910.3591585968569\n",
      "l1 norm: 763.2715523935758\n",
      "Rbeta: 910.5121387225665\n",
      "\n",
      "Train set: Avg. loss: 0.000052206, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.33672 108.2229\n",
      "l2 norm: 910.3097910712776\n",
      "l1 norm: 763.2311005583884\n",
      "Rbeta: 910.4620147644779\n",
      "\n",
      "Train set: Avg. loss: 0.000052084, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.3679 108.24481\n",
      "l2 norm: 910.2517992477441\n",
      "l1 norm: 763.1833800789841\n",
      "Rbeta: 910.4031498952872\n",
      "\n",
      "Train set: Avg. loss: 0.000051963, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.39907 108.266624\n",
      "l2 norm: 910.191683965512\n",
      "l1 norm: 763.1337769265614\n",
      "Rbeta: 910.3421953327571\n",
      "\n",
      "Train set: Avg. loss: 0.000051843, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.43026 108.28845\n",
      "l2 norm: 910.1324390345795\n",
      "l1 norm: 763.0848378870926\n",
      "Rbeta: 910.282222522065\n",
      "\n",
      "Train set: Avg. loss: 0.000051722, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.46145 108.310265\n",
      "l2 norm: 910.0764118636945\n",
      "l1 norm: 763.0386199485453\n",
      "Rbeta: 910.2253482286394\n",
      "\n",
      "Train set: Avg. loss: 0.000051602, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.49264 108.33199\n",
      "l2 norm: 910.0338620687179\n",
      "l1 norm: 763.0036103662812\n",
      "Rbeta: 910.1819877199775\n",
      "\n",
      "Train set: Avg. loss: 0.000051483, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.523834 108.35361\n",
      "l2 norm: 909.9950881360488\n",
      "l1 norm: 762.9717268039033\n",
      "Rbeta: 910.1423710715925\n",
      "\n",
      "Train set: Avg. loss: 0.000051364, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.55504 108.375145\n",
      "l2 norm: 909.9533715702411\n",
      "l1 norm: 762.9374103707876\n",
      "Rbeta: 910.0997850586599\n",
      "\n",
      "Train set: Avg. loss: 0.000051245, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.58625 108.39671\n",
      "l2 norm: 909.9103196292909\n",
      "l1 norm: 762.9020571786009\n",
      "Rbeta: 910.0560363017803\n",
      "\n",
      "Train set: Avg. loss: 0.000051127, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.61746 108.41819\n",
      "l2 norm: 909.8807004542746\n",
      "l1 norm: 762.8779696231411\n",
      "Rbeta: 910.0255202915172\n",
      "\n",
      "Train set: Avg. loss: 0.000051009, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.648674 108.43957\n",
      "l2 norm: 909.8477425652327\n",
      "l1 norm: 762.851105166794\n",
      "Rbeta: 909.9917423019904\n",
      "\n",
      "Train set: Avg. loss: 0.000050891, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.67989 108.46086\n",
      "l2 norm: 909.8036328194684\n",
      "l1 norm: 762.8149187117033\n",
      "Rbeta: 909.9467675132026\n",
      "\n",
      "Train set: Avg. loss: 0.000050774, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.71112 108.48207\n",
      "l2 norm: 909.7586813552329\n",
      "l1 norm: 762.7779151507066\n",
      "Rbeta: 909.9010380151027\n",
      "\n",
      "Train set: Avg. loss: 0.000050658, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.742355 108.50319\n",
      "l2 norm: 909.7101736252761\n",
      "l1 norm: 762.7378904179544\n",
      "Rbeta: 909.8517055269488\n",
      "\n",
      "Train set: Avg. loss: 0.000050542, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.77359 108.5242\n",
      "l2 norm: 909.6613030407125\n",
      "l1 norm: 762.6976353714274\n",
      "Rbeta: 909.801947006105\n",
      "\n",
      "Train set: Avg. loss: 0.000050428, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.80377 108.545235\n",
      "l2 norm: 909.6011660012698\n",
      "l1 norm: 762.6479676673051\n",
      "Rbeta: 909.7410306126242\n",
      "\n",
      "Train set: Avg. loss: 0.000050317, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.83307 108.56621\n",
      "l2 norm: 909.5455277760082\n",
      "l1 norm: 762.6020698954028\n",
      "Rbeta: 909.6847358392073\n",
      "\n",
      "Train set: Avg. loss: 0.000050207, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.86236 108.5871\n",
      "l2 norm: 909.500599325949\n",
      "l1 norm: 762.5651048346645\n",
      "Rbeta: 909.6391173338396\n",
      "\n",
      "Train set: Avg. loss: 0.000050096, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.89166 108.60785\n",
      "l2 norm: 909.4578408994284\n",
      "l1 norm: 762.530019660964\n",
      "Rbeta: 909.5956538121038\n",
      "\n",
      "Train set: Avg. loss: 0.000049987, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.92097 108.628525\n",
      "l2 norm: 909.4288858200104\n",
      "l1 norm: 762.506383622444\n",
      "Rbeta: 909.5660403799559\n",
      "\n",
      "Train set: Avg. loss: 0.000049877, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.95027 108.6492\n",
      "l2 norm: 909.3941629272298\n",
      "l1 norm: 762.4778030211207\n",
      "Rbeta: 909.5305489675981\n",
      "\n",
      "Train set: Avg. loss: 0.000049768, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.97958 108.66988\n",
      "l2 norm: 909.3568963947143\n",
      "l1 norm: 762.4470789867828\n",
      "Rbeta: 909.4926106840111\n",
      "\n",
      "Train set: Avg. loss: 0.000049659, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.0089 108.69054\n",
      "l2 norm: 909.298919999691\n",
      "l1 norm: 762.3989660201611\n",
      "Rbeta: 909.4338517460203\n",
      "\n",
      "Train set: Avg. loss: 0.000049550, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.03822 108.711136\n",
      "l2 norm: 909.2380158004668\n",
      "l1 norm: 762.3485012585602\n",
      "Rbeta: 909.3722884724157\n",
      "\n",
      "Train set: Avg. loss: 0.000049442, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.06754 108.73172\n",
      "l2 norm: 909.178123693782\n",
      "l1 norm: 762.2989645432635\n",
      "Rbeta: 909.3116121737445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000049334, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.09687 108.75232\n",
      "l2 norm: 909.1241984903394\n",
      "l1 norm: 762.2543800160388\n",
      "Rbeta: 909.2569780548057\n",
      "\n",
      "Train set: Avg. loss: 0.000049226, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.126205 108.77294\n",
      "l2 norm: 909.0740136408018\n",
      "l1 norm: 762.2128981957921\n",
      "Rbeta: 909.2061337802556\n",
      "\n",
      "Train set: Avg. loss: 0.000049118, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.15554 108.79346\n",
      "l2 norm: 909.0168179343524\n",
      "l1 norm: 762.165523525359\n",
      "Rbeta: 909.148309547502\n",
      "\n",
      "Train set: Avg. loss: 0.000049011, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.18488 108.813896\n",
      "l2 norm: 908.9438041169626\n",
      "l1 norm: 762.1049629693396\n",
      "Rbeta: 909.0745357164855\n",
      "\n",
      "Train set: Avg. loss: 0.000048904, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.214226 108.83426\n",
      "l2 norm: 908.8628604628041\n",
      "l1 norm: 762.0378246835711\n",
      "Rbeta: 908.992898559804\n",
      "\n",
      "Train set: Avg. loss: 0.000048798, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.243576 108.85461\n",
      "l2 norm: 908.7866555543991\n",
      "l1 norm: 761.9746699287654\n",
      "Rbeta: 908.9158992408582\n",
      "\n",
      "Train set: Avg. loss: 0.000048692, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.27293 108.87489\n",
      "l2 norm: 908.7256698826253\n",
      "l1 norm: 761.9242997159292\n",
      "Rbeta: 908.8542802493351\n",
      "\n",
      "Train set: Avg. loss: 0.000048586, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.302284 108.895195\n",
      "l2 norm: 908.6718972767601\n",
      "l1 norm: 761.8800603859472\n",
      "Rbeta: 908.799765316838\n",
      "\n",
      "Train set: Avg. loss: 0.000048480, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.33164 108.91547\n",
      "l2 norm: 908.6263380737236\n",
      "l1 norm: 761.8427370068231\n",
      "Rbeta: 908.7534237025332\n",
      "\n",
      "Train set: Avg. loss: 0.000048375, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.36101 108.93573\n",
      "l2 norm: 908.5838484549977\n",
      "l1 norm: 761.8079728952138\n",
      "Rbeta: 908.7102735897568\n",
      "\n",
      "Train set: Avg. loss: 0.000048270, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.39038 108.95596\n",
      "l2 norm: 908.5386640080972\n",
      "l1 norm: 761.7709804409083\n",
      "Rbeta: 908.6643777824986\n",
      "\n",
      "Train set: Avg. loss: 0.000048165, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.419754 108.97622\n",
      "l2 norm: 908.4953912572834\n",
      "l1 norm: 761.7355813274531\n",
      "Rbeta: 908.6204087035358\n",
      "\n",
      "Train set: Avg. loss: 0.000048060, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.44913 108.996445\n",
      "l2 norm: 908.4543610334629\n",
      "l1 norm: 761.7020873985686\n",
      "Rbeta: 908.5786367076611\n",
      "\n",
      "Train set: Avg. loss: 0.000047956, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.47851 109.01667\n",
      "l2 norm: 908.4190219751183\n",
      "l1 norm: 761.6733705867049\n",
      "Rbeta: 908.5426223035059\n",
      "\n",
      "Train set: Avg. loss: 0.000047852, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.5079 109.03676\n",
      "l2 norm: 908.390656692372\n",
      "l1 norm: 761.6504995720525\n",
      "Rbeta: 908.5134984273426\n",
      "\n",
      "Train set: Avg. loss: 0.000047748, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.537285 109.05682\n",
      "l2 norm: 908.3561608214288\n",
      "l1 norm: 761.6224547541086\n",
      "Rbeta: 908.4783722396561\n",
      "\n",
      "Train set: Avg. loss: 0.000047645, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.56668 109.07689\n",
      "l2 norm: 908.3132965148541\n",
      "l1 norm: 761.5874161423492\n",
      "Rbeta: 908.434778457301\n",
      "\n",
      "Train set: Avg. loss: 0.000047542, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.59608 109.09693\n",
      "l2 norm: 908.2689570320973\n",
      "l1 norm: 761.5511940719748\n",
      "Rbeta: 908.3897125608528\n",
      "\n",
      "Train set: Avg. loss: 0.000047439, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.62548 109.11699\n",
      "l2 norm: 908.2354683719698\n",
      "l1 norm: 761.5240678332369\n",
      "Rbeta: 908.3555002502668\n",
      "\n",
      "Train set: Avg. loss: 0.000047336, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.654884 109.13703\n",
      "l2 norm: 908.1969145579806\n",
      "l1 norm: 761.4926853610846\n",
      "Rbeta: 908.3162693907674\n",
      "\n",
      "Train set: Avg. loss: 0.000047234, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.684296 109.15698\n",
      "l2 norm: 908.1629014980958\n",
      "l1 norm: 761.4651480270959\n",
      "Rbeta: 908.2815365477733\n",
      "\n",
      "Train set: Avg. loss: 0.000047132, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.71371 109.17687\n",
      "l2 norm: 908.1438386351642\n",
      "l1 norm: 761.4500872813726\n",
      "Rbeta: 908.2617803012411\n",
      "\n",
      "Train set: Avg. loss: 0.000047030, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.743126 109.196655\n",
      "l2 norm: 908.1239110948319\n",
      "l1 norm: 761.4343316521333\n",
      "Rbeta: 908.2411896096916\n",
      "\n",
      "Train set: Avg. loss: 0.000046929, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.77255 109.21639\n",
      "l2 norm: 908.0953901850767\n",
      "l1 norm: 761.4114115310373\n",
      "Rbeta: 908.2119544574113\n",
      "\n",
      "Train set: Avg. loss: 0.000046828, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.80198 109.2361\n",
      "l2 norm: 908.0609628498097\n",
      "l1 norm: 761.3835168281757\n",
      "Rbeta: 908.176833090676\n",
      "\n",
      "Train set: Avg. loss: 0.000046727, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.831406 109.25572\n",
      "l2 norm: 908.016439460392\n",
      "l1 norm: 761.3471249010227\n",
      "Rbeta: 908.1315165820124\n",
      "\n",
      "Train set: Avg. loss: 0.000046627, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.86084 109.27533\n",
      "l2 norm: 907.9700550656092\n",
      "l1 norm: 761.3091288687546\n",
      "Rbeta: 908.0844546478058\n",
      "\n",
      "Train set: Avg. loss: 0.000046527, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.89028 109.29492\n",
      "l2 norm: 907.9255474849501\n",
      "l1 norm: 761.2726347097464\n",
      "Rbeta: 908.0392461547013\n",
      "\n",
      "Train set: Avg. loss: 0.000046427, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.91972 109.31453\n",
      "l2 norm: 907.8681478765304\n",
      "l1 norm: 761.2253658448653\n",
      "Rbeta: 907.9811060614376\n",
      "\n",
      "Train set: Avg. loss: 0.000046327, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.94917 109.33411\n",
      "l2 norm: 907.8065978429024\n",
      "l1 norm: 761.174563537774\n",
      "Rbeta: 907.9188274704804\n",
      "\n",
      "Train set: Avg. loss: 0.000046229, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.9778 109.35362\n",
      "l2 norm: 907.7552156704326\n",
      "l1 norm: 761.1322134773495\n",
      "Rbeta: 907.8668283707177\n",
      "\n",
      "Train set: Avg. loss: 0.000046134, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.00529 109.373116\n",
      "l2 norm: 907.7200838175654\n",
      "l1 norm: 761.103384123627\n",
      "Rbeta: 907.8310674718634\n",
      "\n",
      "Train set: Avg. loss: 0.000046039, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.03278 109.39259\n",
      "l2 norm: 907.6873861339812\n",
      "l1 norm: 761.0765265318786\n",
      "Rbeta: 907.7978331334417\n",
      "\n",
      "Train set: Avg. loss: 0.000045945, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.06028 109.41205\n",
      "l2 norm: 907.6465559527505\n",
      "l1 norm: 761.0428341942911\n",
      "Rbeta: 907.7563308021382\n",
      "\n",
      "Train set: Avg. loss: 0.000045850, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.08778 109.43146\n",
      "l2 norm: 907.6028590790547\n",
      "l1 norm: 761.0068458774335\n",
      "Rbeta: 907.7121085736101\n",
      "\n",
      "Train set: Avg. loss: 0.000045756, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.11529 109.450806\n",
      "l2 norm: 907.5620325851119\n",
      "l1 norm: 760.9732349322722\n",
      "Rbeta: 907.6707282135815\n",
      "\n",
      "Train set: Avg. loss: 0.000045662, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.1428 109.470085\n",
      "l2 norm: 907.5228397083532\n",
      "l1 norm: 760.9410164465532\n",
      "Rbeta: 907.6309272333895\n",
      "\n",
      "Train set: Avg. loss: 0.000045569, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.17031 109.4893\n",
      "l2 norm: 907.4914850700875\n",
      "l1 norm: 760.9153549979146\n",
      "Rbeta: 907.5989722382415\n",
      "\n",
      "Train set: Avg. loss: 0.000045476, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.19783 109.50847\n",
      "l2 norm: 907.4635962532711\n",
      "l1 norm: 760.8926203660671\n",
      "Rbeta: 907.5704896677198\n",
      "\n",
      "Train set: Avg. loss: 0.000045383, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.22535 109.52769\n",
      "l2 norm: 907.4312280630527\n",
      "l1 norm: 760.8661073907675\n",
      "Rbeta: 907.5375492287372\n",
      "\n",
      "Train set: Avg. loss: 0.000045290, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.25287 109.54685\n",
      "l2 norm: 907.3903180010644\n",
      "l1 norm: 760.8323547788125\n",
      "Rbeta: 907.4960092812019\n",
      "\n",
      "Train set: Avg. loss: 0.000045197, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.280396 109.565994\n",
      "l2 norm: 907.3463608181711\n",
      "l1 norm: 760.7960383454788\n",
      "Rbeta: 907.4514780974649\n",
      "\n",
      "Train set: Avg. loss: 0.000045105, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.30793 109.585144\n",
      "l2 norm: 907.292906253814\n",
      "l1 norm: 760.7517294874995\n",
      "Rbeta: 907.3974230214526\n",
      "\n",
      "Train set: Avg. loss: 0.000045013, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.335464 109.60428\n",
      "l2 norm: 907.2260648249433\n",
      "l1 norm: 760.6962600855375\n",
      "Rbeta: 907.3300474080355\n",
      "\n",
      "Train set: Avg. loss: 0.000044921, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.363 109.62336\n",
      "l2 norm: 907.1567927949324\n",
      "l1 norm: 760.6387573431341\n",
      "Rbeta: 907.2601830018492\n",
      "\n",
      "Train set: Avg. loss: 0.000044830, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.39054 109.642334\n",
      "l2 norm: 907.0934269720095\n",
      "l1 norm: 760.586226101253\n",
      "Rbeta: 907.1961553864637\n",
      "\n",
      "Train set: Avg. loss: 0.000044738, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.41808 109.66128\n",
      "l2 norm: 907.0408683981208\n",
      "l1 norm: 760.542771255683\n",
      "Rbeta: 907.1430527347995\n",
      "\n",
      "Train set: Avg. loss: 0.000044647, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.44563 109.68017\n",
      "l2 norm: 906.9927040137837\n",
      "l1 norm: 760.5029748946736\n",
      "Rbeta: 907.094261410791\n",
      "\n",
      "Train set: Avg. loss: 0.000044557, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.47318 109.699005\n",
      "l2 norm: 906.9457698626825\n",
      "l1 norm: 760.4641561293665\n",
      "Rbeta: 907.0467220299943\n",
      "\n",
      "Train set: Avg. loss: 0.000044466, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.50074 109.71784\n",
      "l2 norm: 906.8872025884665\n",
      "l1 norm: 760.4155395838606\n",
      "Rbeta: 906.9875931166325\n",
      "\n",
      "Train set: Avg. loss: 0.000044376, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.5283 109.73665\n",
      "l2 norm: 906.8343819836659\n",
      "l1 norm: 760.3717331287185\n",
      "Rbeta: 906.9342067086769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000044286, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.55586 109.75542\n",
      "l2 norm: 906.7907480472591\n",
      "l1 norm: 760.3356369345896\n",
      "Rbeta: 906.8899491328622\n",
      "\n",
      "Train set: Avg. loss: 0.000044198, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.5827 109.774155\n",
      "l2 norm: 906.7510185866479\n",
      "l1 norm: 760.3027919437926\n",
      "Rbeta: 906.8497175139831\n",
      "\n",
      "Train set: Avg. loss: 0.000044112, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.6083 109.79286\n",
      "l2 norm: 906.7029025372191\n",
      "l1 norm: 760.2629745134584\n",
      "Rbeta: 906.801077838314\n",
      "\n",
      "Train set: Avg. loss: 0.000044027, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.6339 109.81152\n",
      "l2 norm: 906.645533466393\n",
      "l1 norm: 760.2153682063996\n",
      "Rbeta: 906.7432861677739\n",
      "\n",
      "Train set: Avg. loss: 0.000043941, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.65951 109.83015\n",
      "l2 norm: 906.598565162186\n",
      "l1 norm: 760.1764081250211\n",
      "Rbeta: 906.695794682636\n",
      "\n",
      "Train set: Avg. loss: 0.000043859, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.683525 109.84874\n",
      "l2 norm: 906.5640125131816\n",
      "l1 norm: 760.1478811888167\n",
      "Rbeta: 906.6609017995535\n",
      "\n",
      "Train set: Avg. loss: 0.000043778, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.70716 109.86725\n",
      "l2 norm: 906.5294260287072\n",
      "l1 norm: 760.1193621819596\n",
      "Rbeta: 906.6259187860272\n",
      "\n",
      "Train set: Avg. loss: 0.000043698, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.73081 109.88574\n",
      "l2 norm: 906.4953218150948\n",
      "l1 norm: 760.0912702207177\n",
      "Rbeta: 906.5914609379117\n",
      "\n",
      "Train set: Avg. loss: 0.000043617, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.754456 109.90421\n",
      "l2 norm: 906.4615624833384\n",
      "l1 norm: 760.0634680286742\n",
      "Rbeta: 906.5573436130045\n",
      "\n",
      "Train set: Avg. loss: 0.000043537, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.77811 109.92263\n",
      "l2 norm: 906.4340699244104\n",
      "l1 norm: 760.0409456289317\n",
      "Rbeta: 906.5295565208138\n",
      "\n",
      "Train set: Avg. loss: 0.000043456, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.80176 109.941055\n",
      "l2 norm: 906.3963691782353\n",
      "l1 norm: 760.009871232174\n",
      "Rbeta: 906.4914360671235\n",
      "\n",
      "Train set: Avg. loss: 0.000043376, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.82542 109.95944\n",
      "l2 norm: 906.3600039568313\n",
      "l1 norm: 759.9799547723596\n",
      "Rbeta: 906.4547309256886\n",
      "\n",
      "Train set: Avg. loss: 0.000043297, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.849075 109.9778\n",
      "l2 norm: 906.3389321594742\n",
      "l1 norm: 759.9628128296899\n",
      "Rbeta: 906.4333373990238\n",
      "\n",
      "Train set: Avg. loss: 0.000043217, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.872734 109.9962\n",
      "l2 norm: 906.3223620635353\n",
      "l1 norm: 759.9494907602505\n",
      "Rbeta: 906.4163710429605\n",
      "\n",
      "Train set: Avg. loss: 0.000043141, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.894424 110.014595\n",
      "l2 norm: 906.2974330070991\n",
      "l1 norm: 759.9293094373807\n",
      "Rbeta: 906.3912504354023\n",
      "\n",
      "Train set: Avg. loss: 0.000043065, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.916115 110.03291\n",
      "l2 norm: 906.255288158822\n",
      "l1 norm: 759.8947827899785\n",
      "Rbeta: 906.3488873429483\n",
      "\n",
      "Train set: Avg. loss: 0.000042990, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.93781 110.0512\n",
      "l2 norm: 906.211648609879\n",
      "l1 norm: 759.8589538722443\n",
      "Rbeta: 906.3049497881226\n",
      "\n",
      "Train set: Avg. loss: 0.000042915, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.95951 110.06944\n",
      "l2 norm: 906.1630085778863\n",
      "l1 norm: 759.8189061015091\n",
      "Rbeta: 906.256112043839\n",
      "\n",
      "Train set: Avg. loss: 0.000042840, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.98121 110.08765\n",
      "l2 norm: 906.1107754297601\n",
      "l1 norm: 759.7757907491641\n",
      "Rbeta: 906.2035475654656\n",
      "\n",
      "Train set: Avg. loss: 0.000042765, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.00291 110.10584\n",
      "l2 norm: 906.0613490210765\n",
      "l1 norm: 759.7349746014332\n",
      "Rbeta: 906.1538900145906\n",
      "\n",
      "Train set: Avg. loss: 0.000042690, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.02461 110.12401\n",
      "l2 norm: 906.0131703804454\n",
      "l1 norm: 759.6951727740064\n",
      "Rbeta: 906.1054809797774\n",
      "\n",
      "Train set: Avg. loss: 0.000042615, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.04632 110.142105\n",
      "l2 norm: 905.9648073434644\n",
      "l1 norm: 759.6551959774658\n",
      "Rbeta: 906.0567766082384\n",
      "\n",
      "Train set: Avg. loss: 0.000042541, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.06803 110.16017\n",
      "l2 norm: 905.923112819798\n",
      "l1 norm: 759.620787814571\n",
      "Rbeta: 906.0149649096398\n",
      "\n",
      "Train set: Avg. loss: 0.000042467, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.08974 110.17819\n",
      "l2 norm: 905.8879612176818\n",
      "l1 norm: 759.5918219408941\n",
      "Rbeta: 905.979496880341\n",
      "\n",
      "Train set: Avg. loss: 0.000042393, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.11145 110.19612\n",
      "l2 norm: 905.8504580321469\n",
      "l1 norm: 759.5609053317531\n",
      "Rbeta: 905.9417567596362\n",
      "\n",
      "Train set: Avg. loss: 0.000042320, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.13317 110.214005\n",
      "l2 norm: 905.8080641706476\n",
      "l1 norm: 759.5259135420467\n",
      "Rbeta: 905.8990603404934\n",
      "\n",
      "Train set: Avg. loss: 0.000042246, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.154884 110.23185\n",
      "l2 norm: 905.7738493983638\n",
      "l1 norm: 759.4978018392853\n",
      "Rbeta: 905.864581083069\n",
      "\n",
      "Train set: Avg. loss: 0.000042173, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.176605 110.249695\n",
      "l2 norm: 905.7342766847926\n",
      "l1 norm: 759.4652202280566\n",
      "Rbeta: 905.8247747672805\n",
      "\n",
      "Train set: Avg. loss: 0.000042100, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.198326 110.26755\n",
      "l2 norm: 905.6946247125861\n",
      "l1 norm: 759.4326361196839\n",
      "Rbeta: 905.784881332831\n",
      "\n",
      "Train set: Avg. loss: 0.000042027, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.220055 110.285355\n",
      "l2 norm: 905.6466761525802\n",
      "l1 norm: 759.3931549128002\n",
      "Rbeta: 905.7366023612938\n",
      "\n",
      "Train set: Avg. loss: 0.000041954, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.241776 110.30321\n",
      "l2 norm: 905.6012710541422\n",
      "l1 norm: 759.3557688311735\n",
      "Rbeta: 905.6909599996327\n",
      "\n",
      "Train set: Avg. loss: 0.000041881, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.263504 110.32107\n",
      "l2 norm: 905.5600542551286\n",
      "l1 norm: 759.3219834484714\n",
      "Rbeta: 905.6494393066731\n",
      "\n",
      "Train set: Avg. loss: 0.000041808, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.28523 110.33896\n",
      "l2 norm: 905.5241715307111\n",
      "l1 norm: 759.2926685752376\n",
      "Rbeta: 905.6133505197886\n",
      "\n",
      "Train set: Avg. loss: 0.000041735, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.30697 110.35687\n",
      "l2 norm: 905.4823035860195\n",
      "l1 norm: 759.2583443599758\n",
      "Rbeta: 905.5712291779058\n",
      "\n",
      "Train set: Avg. loss: 0.000041663, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.328705 110.3748\n",
      "l2 norm: 905.4378367371891\n",
      "l1 norm: 759.2217788118423\n",
      "Rbeta: 905.5264892112211\n",
      "\n",
      "Train set: Avg. loss: 0.000041590, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.35044 110.39276\n",
      "l2 norm: 905.399171787029\n",
      "l1 norm: 759.1900332387463\n",
      "Rbeta: 905.4875078341331\n",
      "\n",
      "Train set: Avg. loss: 0.000041518, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.37218 110.410736\n",
      "l2 norm: 905.355038329993\n",
      "l1 norm: 759.1537280701589\n",
      "Rbeta: 905.4431736235807\n",
      "\n",
      "Train set: Avg. loss: 0.000041446, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.39392 110.428665\n",
      "l2 norm: 905.3027627957738\n",
      "l1 norm: 759.110625997063\n",
      "Rbeta: 905.3906504824151\n",
      "\n",
      "Train set: Avg. loss: 0.000041374, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.415665 110.446526\n",
      "l2 norm: 905.257545315938\n",
      "l1 norm: 759.0733806247786\n",
      "Rbeta: 905.3451234844949\n",
      "\n",
      "Train set: Avg. loss: 0.000041302, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.43741 110.46434\n",
      "l2 norm: 905.2152325236196\n",
      "l1 norm: 759.038461983309\n",
      "Rbeta: 905.3025733552474\n",
      "\n",
      "Train set: Avg. loss: 0.000041230, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.45916 110.4821\n",
      "l2 norm: 905.1788954935643\n",
      "l1 norm: 759.0084605889181\n",
      "Rbeta: 905.2659525860612\n",
      "\n",
      "Train set: Avg. loss: 0.000041159, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.48091 110.49981\n",
      "l2 norm: 905.1391411748754\n",
      "l1 norm: 758.9755984935339\n",
      "Rbeta: 905.2259649993487\n",
      "\n",
      "Train set: Avg. loss: 0.000041088, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.50266 110.517456\n",
      "l2 norm: 905.0906777635263\n",
      "l1 norm: 758.935513659545\n",
      "Rbeta: 905.1771738746457\n",
      "\n",
      "Train set: Avg. loss: 0.000041017, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.524414 110.53508\n",
      "l2 norm: 905.0428633225123\n",
      "l1 norm: 758.8960753445297\n",
      "Rbeta: 905.1291260799793\n",
      "\n",
      "Train set: Avg. loss: 0.000040946, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.54617 110.55263\n",
      "l2 norm: 904.9987649685473\n",
      "l1 norm: 758.8596739551324\n",
      "Rbeta: 905.0847578871704\n",
      "\n",
      "Train set: Avg. loss: 0.000040876, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.56793 110.57016\n",
      "l2 norm: 904.955143289977\n",
      "l1 norm: 758.8236777748045\n",
      "Rbeta: 905.0408503019482\n",
      "\n",
      "Train set: Avg. loss: 0.000040805, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.58969 110.58774\n",
      "l2 norm: 904.9198201802194\n",
      "l1 norm: 758.7947079178253\n",
      "Rbeta: 905.005263709959\n",
      "\n",
      "Train set: Avg. loss: 0.000040735, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.61146 110.60528\n",
      "l2 norm: 904.8854234405629\n",
      "l1 norm: 758.7665296732889\n",
      "Rbeta: 904.9705728224264\n",
      "\n",
      "Train set: Avg. loss: 0.000040665, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.633224 110.62273\n",
      "l2 norm: 904.8479782172352\n",
      "l1 norm: 758.7357430918706\n",
      "Rbeta: 904.9328897299641\n",
      "\n",
      "Train set: Avg. loss: 0.000040595, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.65499 110.64017\n",
      "l2 norm: 904.8133683782568\n",
      "l1 norm: 758.7072796986167\n",
      "Rbeta: 904.8980180691256\n",
      "\n",
      "Train set: Avg. loss: 0.000040525, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.676765 110.65762\n",
      "l2 norm: 904.7759294127447\n",
      "l1 norm: 758.676443576356\n",
      "Rbeta: 904.8602558207236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000040456, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.69853 110.675064\n",
      "l2 norm: 904.7431970983453\n",
      "l1 norm: 758.649597986588\n",
      "Rbeta: 904.8272589109389\n",
      "\n",
      "Train set: Avg. loss: 0.000040386, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.72031 110.69252\n",
      "l2 norm: 904.7050335835154\n",
      "l1 norm: 758.6182441400404\n",
      "Rbeta: 904.788810113777\n",
      "\n",
      "Train set: Avg. loss: 0.000040317, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.74209 110.7099\n",
      "l2 norm: 904.663945114113\n",
      "l1 norm: 758.5844465565713\n",
      "Rbeta: 904.747501278737\n",
      "\n",
      "Train set: Avg. loss: 0.000040248, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.76386 110.72728\n",
      "l2 norm: 904.6242889660499\n",
      "l1 norm: 758.5520172286506\n",
      "Rbeta: 904.7075433815829\n",
      "\n",
      "Train set: Avg. loss: 0.000040178, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.785645 110.744675\n",
      "l2 norm: 904.5866612760018\n",
      "l1 norm: 758.5213402308202\n",
      "Rbeta: 904.6695639310692\n",
      "\n",
      "Train set: Avg. loss: 0.000040110, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.80743 110.76207\n",
      "l2 norm: 904.555023767629\n",
      "l1 norm: 758.4956891925447\n",
      "Rbeta: 904.6376268859119\n",
      "\n",
      "Train set: Avg. loss: 0.000040041, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.829216 110.77945\n",
      "l2 norm: 904.5200012652427\n",
      "l1 norm: 758.4671729777904\n",
      "Rbeta: 904.6024199470826\n",
      "\n",
      "Train set: Avg. loss: 0.000039972, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.851006 110.796844\n",
      "l2 norm: 904.4903316760474\n",
      "l1 norm: 758.4431096474034\n",
      "Rbeta: 904.5724702447187\n",
      "\n",
      "Train set: Avg. loss: 0.000039903, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.872795 110.81424\n",
      "l2 norm: 904.4694366020459\n",
      "l1 norm: 758.4263219142306\n",
      "Rbeta: 904.5512862970132\n",
      "\n",
      "Train set: Avg. loss: 0.000039835, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.894585 110.831604\n",
      "l2 norm: 904.4440641781874\n",
      "l1 norm: 758.4057512725714\n",
      "Rbeta: 904.5256370454515\n",
      "\n",
      "Train set: Avg. loss: 0.000039767, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.91638 110.84894\n",
      "l2 norm: 904.4148630297506\n",
      "l1 norm: 758.3818921638689\n",
      "Rbeta: 904.4961386186316\n",
      "\n",
      "Train set: Avg. loss: 0.000039699, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.93818 110.86624\n",
      "l2 norm: 904.3795790676168\n",
      "l1 norm: 758.3529609722256\n",
      "Rbeta: 904.4605096540279\n",
      "\n",
      "Train set: Avg. loss: 0.000039631, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.95998 110.88355\n",
      "l2 norm: 904.3347035369121\n",
      "l1 norm: 758.3159791524647\n",
      "Rbeta: 904.4154425688596\n",
      "\n",
      "Train set: Avg. loss: 0.000039563, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.98177 110.90074\n",
      "l2 norm: 904.2867892806222\n",
      "l1 norm: 758.2764501293508\n",
      "Rbeta: 904.3671486627252\n",
      "\n",
      "Train set: Avg. loss: 0.000039496, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.00358 110.917786\n",
      "l2 norm: 904.2407944725389\n",
      "l1 norm: 758.238541652111\n",
      "Rbeta: 904.3208777466747\n",
      "\n",
      "Train set: Avg. loss: 0.000039429, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.02538 110.93471\n",
      "l2 norm: 904.1997334395036\n",
      "l1 norm: 758.2047879896363\n",
      "Rbeta: 904.2795218091791\n",
      "\n",
      "Train set: Avg. loss: 0.000039362, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.047195 110.9516\n",
      "l2 norm: 904.1659210961052\n",
      "l1 norm: 758.1770869529429\n",
      "Rbeta: 904.2454650861238\n",
      "\n",
      "Train set: Avg. loss: 0.000039297, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.06761 110.96847\n",
      "l2 norm: 904.1402607246956\n",
      "l1 norm: 758.1562738743833\n",
      "Rbeta: 904.2195186136474\n",
      "\n",
      "Train set: Avg. loss: 0.000039234, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.08744 110.985275\n",
      "l2 norm: 904.1098181971022\n",
      "l1 norm: 758.1315117365268\n",
      "Rbeta: 904.1889427812596\n",
      "\n",
      "Train set: Avg. loss: 0.000039171, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.10728 111.00203\n",
      "l2 norm: 904.0766793161246\n",
      "l1 norm: 758.1044815698109\n",
      "Rbeta: 904.1556255431464\n",
      "\n",
      "Train set: Avg. loss: 0.000039109, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.127106 111.018776\n",
      "l2 norm: 904.0477483953496\n",
      "l1 norm: 758.0809504596589\n",
      "Rbeta: 904.1264202985814\n",
      "\n",
      "Train set: Avg. loss: 0.000039046, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.14694 111.03544\n",
      "l2 norm: 904.017231021491\n",
      "l1 norm: 758.0560704429315\n",
      "Rbeta: 904.0956968008131\n",
      "\n",
      "Train set: Avg. loss: 0.000038984, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.16678 111.05206\n",
      "l2 norm: 903.9936579973688\n",
      "l1 norm: 758.0370101303531\n",
      "Rbeta: 904.0719711602101\n",
      "\n",
      "Train set: Avg. loss: 0.000038921, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.186615 111.06867\n",
      "l2 norm: 903.968633470579\n",
      "l1 norm: 758.0167173250143\n",
      "Rbeta: 904.0466846296664\n",
      "\n",
      "Train set: Avg. loss: 0.000038859, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.20645 111.08525\n",
      "l2 norm: 903.9381166739504\n",
      "l1 norm: 757.9918415766383\n",
      "Rbeta: 904.0160138249838\n",
      "\n",
      "Train set: Avg. loss: 0.000038798, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.225975 111.10179\n",
      "l2 norm: 903.9060311526223\n",
      "l1 norm: 757.9656566918173\n",
      "Rbeta: 903.9836793483531\n",
      "\n",
      "Train set: Avg. loss: 0.000038739, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.243835 111.11833\n",
      "l2 norm: 903.8705838375613\n",
      "l1 norm: 757.936559424341\n",
      "Rbeta: 903.9481748883217\n",
      "\n",
      "Train set: Avg. loss: 0.000038681, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.261696 111.13487\n",
      "l2 norm: 903.8290941744857\n",
      "l1 norm: 757.9023039046115\n",
      "Rbeta: 903.906635585796\n",
      "\n",
      "Train set: Avg. loss: 0.000038623, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.27956 111.15138\n",
      "l2 norm: 903.7899709784557\n",
      "l1 norm: 757.8699838449995\n",
      "Rbeta: 903.8673656540418\n",
      "\n",
      "Train set: Avg. loss: 0.000038564, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.297424 111.16791\n",
      "l2 norm: 903.7536920776829\n",
      "l1 norm: 757.8399957153212\n",
      "Rbeta: 903.8310104142593\n",
      "\n",
      "Train set: Avg. loss: 0.000038506, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.315285 111.18437\n",
      "l2 norm: 903.7187937126329\n",
      "l1 norm: 757.8112041452296\n",
      "Rbeta: 903.7960263187572\n",
      "\n",
      "Train set: Avg. loss: 0.000038448, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.33315 111.200806\n",
      "l2 norm: 903.6785025023555\n",
      "l1 norm: 757.7779478712916\n",
      "Rbeta: 903.7556080029867\n",
      "\n",
      "Train set: Avg. loss: 0.000038391, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.35102 111.21721\n",
      "l2 norm: 903.6436154261858\n",
      "l1 norm: 757.7492076431504\n",
      "Rbeta: 903.7206229580387\n",
      "\n",
      "Train set: Avg. loss: 0.000038333, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.36889 111.23357\n",
      "l2 norm: 903.6069863956305\n",
      "l1 norm: 757.718997390155\n",
      "Rbeta: 903.6838487921935\n",
      "\n",
      "Train set: Avg. loss: 0.000038276, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.386765 111.24987\n",
      "l2 norm: 903.5710386717249\n",
      "l1 norm: 757.6892935571443\n",
      "Rbeta: 903.6477945223226\n",
      "\n",
      "Train set: Avg. loss: 0.000038218, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.40463 111.26616\n",
      "l2 norm: 903.5342455835187\n",
      "l1 norm: 757.6589210268849\n",
      "Rbeta: 903.6109161570653\n",
      "\n",
      "Train set: Avg. loss: 0.000038161, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.42251 111.28238\n",
      "l2 norm: 903.498842473162\n",
      "l1 norm: 757.6297731281351\n",
      "Rbeta: 903.5754071074351\n",
      "\n",
      "Train set: Avg. loss: 0.000038104, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.440384 111.298615\n",
      "l2 norm: 903.4706650669604\n",
      "l1 norm: 757.606702511685\n",
      "Rbeta: 903.5471089182729\n",
      "\n",
      "Train set: Avg. loss: 0.000038048, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.45796 111.314896\n",
      "l2 norm: 903.4411319685955\n",
      "l1 norm: 757.5824873106105\n",
      "Rbeta: 903.5174854845612\n",
      "\n",
      "Train set: Avg. loss: 0.000037994, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.473854 111.33116\n",
      "l2 norm: 903.4012476471902\n",
      "l1 norm: 757.5495907050886\n",
      "Rbeta: 903.4776463650511\n",
      "\n",
      "Train set: Avg. loss: 0.000037941, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.489746 111.347404\n",
      "l2 norm: 903.3476063849001\n",
      "l1 norm: 757.5051466715871\n",
      "Rbeta: 903.4239767987008\n",
      "\n",
      "Train set: Avg. loss: 0.000037887, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.50564 111.36368\n",
      "l2 norm: 903.2994998575707\n",
      "l1 norm: 757.4653591323498\n",
      "Rbeta: 903.375819579291\n",
      "\n",
      "Train set: Avg. loss: 0.000037833, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.52154 111.37996\n",
      "l2 norm: 903.2450962879775\n",
      "l1 norm: 757.4202941237413\n",
      "Rbeta: 903.3213780411947\n",
      "\n",
      "Train set: Avg. loss: 0.000037780, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.53743 111.396255\n",
      "l2 norm: 903.196242505567\n",
      "l1 norm: 757.3798341877279\n",
      "Rbeta: 903.27255698261\n",
      "\n",
      "Train set: Avg. loss: 0.000037727, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.55333 111.41252\n",
      "l2 norm: 903.1520742486066\n",
      "l1 norm: 757.3432818144498\n",
      "Rbeta: 903.228383051612\n",
      "\n",
      "Train set: Avg. loss: 0.000037673, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.56923 111.42874\n",
      "l2 norm: 903.1129319972317\n",
      "l1 norm: 757.3109794956226\n",
      "Rbeta: 903.1892322140167\n",
      "\n",
      "Train set: Avg. loss: 0.000037620, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.58513 111.44496\n",
      "l2 norm: 903.0727807250321\n",
      "l1 norm: 757.2778701382003\n",
      "Rbeta: 903.14910187061\n",
      "\n",
      "Train set: Avg. loss: 0.000037567, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.60103 111.46112\n",
      "l2 norm: 903.034707061051\n",
      "l1 norm: 757.2464953150704\n",
      "Rbeta: 903.1110006556796\n",
      "\n",
      "Train set: Avg. loss: 0.000037515, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.61693 111.47727\n",
      "l2 norm: 903.0024703492428\n",
      "l1 norm: 757.2200456072366\n",
      "Rbeta: 903.078766582226\n",
      "\n",
      "Train set: Avg. loss: 0.000037462, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.632835 111.49342\n",
      "l2 norm: 902.9717433907593\n",
      "l1 norm: 757.1948748161624\n",
      "Rbeta: 903.0479721033893\n",
      "\n",
      "Train set: Avg. loss: 0.000037409, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.648735 111.50952\n",
      "l2 norm: 902.93625931217\n",
      "l1 norm: 757.1657455286235\n",
      "Rbeta: 903.0125377839972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000037357, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.66464 111.52562\n",
      "l2 norm: 902.9028694514451\n",
      "l1 norm: 757.1383449640659\n",
      "Rbeta: 902.9790956079804\n",
      "\n",
      "Train set: Avg. loss: 0.000037304, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.68055 111.54169\n",
      "l2 norm: 902.8604192286051\n",
      "l1 norm: 757.1033469738692\n",
      "Rbeta: 902.9366034778318\n",
      "\n",
      "Train set: Avg. loss: 0.000037252, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.69646 111.55772\n",
      "l2 norm: 902.8128193877843\n",
      "l1 norm: 757.0640738761047\n",
      "Rbeta: 902.8890462287375\n",
      "\n",
      "Train set: Avg. loss: 0.000037200, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.712364 111.573685\n",
      "l2 norm: 902.7614803095424\n",
      "l1 norm: 757.0217095912265\n",
      "Rbeta: 902.8376036973531\n",
      "\n",
      "Train set: Avg. loss: 0.000037150, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.72712 111.58966\n",
      "l2 norm: 902.7094625513384\n",
      "l1 norm: 756.978772767083\n",
      "Rbeta: 902.7856342054521\n",
      "\n",
      "Train set: Avg. loss: 0.000037101, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.74104 111.605606\n",
      "l2 norm: 902.6659326308068\n",
      "l1 norm: 756.9429449896761\n",
      "Rbeta: 902.7421985689175\n",
      "\n",
      "Train set: Avg. loss: 0.000037052, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.75497 111.621506\n",
      "l2 norm: 902.6257067566829\n",
      "l1 norm: 756.9099977367937\n",
      "Rbeta: 902.7020551998371\n",
      "\n",
      "Train set: Avg. loss: 0.000037004, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.76889 111.6374\n",
      "l2 norm: 902.5752675719475\n",
      "l1 norm: 756.8684980221432\n",
      "Rbeta: 902.6516802588292\n",
      "\n",
      "Train set: Avg. loss: 0.000036956, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.782814 111.65323\n",
      "l2 norm: 902.5251975975623\n",
      "l1 norm: 756.8272674303491\n",
      "Rbeta: 902.6016804392855\n",
      "\n",
      "Train set: Avg. loss: 0.000036907, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.796745 111.66904\n",
      "l2 norm: 902.4752814949537\n",
      "l1 norm: 756.7861111052525\n",
      "Rbeta: 902.5519198859976\n",
      "\n",
      "Train set: Avg. loss: 0.000036859, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.81067 111.684845\n",
      "l2 norm: 902.4311041006399\n",
      "l1 norm: 756.7497723928321\n",
      "Rbeta: 902.5077780377383\n",
      "\n",
      "Train set: Avg. loss: 0.000036811, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.8246 111.70065\n",
      "l2 norm: 902.3962987454021\n",
      "l1 norm: 756.7212740345592\n",
      "Rbeta: 902.47307051489\n",
      "\n",
      "Train set: Avg. loss: 0.000036763, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.838524 111.7164\n",
      "l2 norm: 902.3691136227569\n",
      "l1 norm: 756.6991986534265\n",
      "Rbeta: 902.4459935322998\n",
      "\n",
      "Train set: Avg. loss: 0.000036715, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.852455 111.732086\n",
      "l2 norm: 902.351773039675\n",
      "l1 norm: 756.6853338736526\n",
      "Rbeta: 902.4286257194257\n",
      "\n",
      "Train set: Avg. loss: 0.000036667, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.86639 111.74776\n",
      "l2 norm: 902.3354696221276\n",
      "l1 norm: 756.6723862835368\n",
      "Rbeta: 902.4124183251143\n",
      "\n",
      "Train set: Avg. loss: 0.000036620, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.88032 111.76341\n",
      "l2 norm: 902.3154138867429\n",
      "l1 norm: 756.6562130175784\n",
      "Rbeta: 902.39247621851\n",
      "\n",
      "Train set: Avg. loss: 0.000036572, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.89425 111.77907\n",
      "l2 norm: 902.2971299722395\n",
      "l1 norm: 756.6414928439165\n",
      "Rbeta: 902.3742300503781\n",
      "\n",
      "Train set: Avg. loss: 0.000036525, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.90818 111.79471\n",
      "l2 norm: 902.2790428072278\n",
      "l1 norm: 756.6269690802164\n",
      "Rbeta: 902.3562094107481\n",
      "\n",
      "Train set: Avg. loss: 0.000036477, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.92212 111.81034\n",
      "l2 norm: 902.2532750268449\n",
      "l1 norm: 756.6059871075872\n",
      "Rbeta: 902.3304991717172\n",
      "\n",
      "Train set: Avg. loss: 0.000036430, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.93605 111.82597\n",
      "l2 norm: 902.2255543446581\n",
      "l1 norm: 756.5833867578945\n",
      "Rbeta: 902.3028738859763\n",
      "\n",
      "Train set: Avg. loss: 0.000036382, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.94999 111.84157\n",
      "l2 norm: 902.2092013373916\n",
      "l1 norm: 756.570252502513\n",
      "Rbeta: 902.2866162983225\n",
      "\n",
      "Train set: Avg. loss: 0.000036335, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.96392 111.85715\n",
      "l2 norm: 902.1978320201116\n",
      "l1 norm: 756.5611964565089\n",
      "Rbeta: 902.275293043615\n",
      "\n",
      "Train set: Avg. loss: 0.000036288, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.97786 111.872696\n",
      "l2 norm: 902.1879297175027\n",
      "l1 norm: 756.5533558374143\n",
      "Rbeta: 902.265441318682\n",
      "\n",
      "Train set: Avg. loss: 0.000036241, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.9918 111.88826\n",
      "l2 norm: 902.1880615258151\n",
      "l1 norm: 756.5539216156067\n",
      "Rbeta: 902.2656575517385\n",
      "\n",
      "Train set: Avg. loss: 0.000036194, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.00574 111.90381\n",
      "l2 norm: 902.1896349787828\n",
      "l1 norm: 756.5556862103704\n",
      "Rbeta: 902.2672866480102\n",
      "\n",
      "Train set: Avg. loss: 0.000036147, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.01968 111.91936\n",
      "l2 norm: 902.1787349983192\n",
      "l1 norm: 756.5469952321005\n",
      "Rbeta: 902.2563969250557\n",
      "\n",
      "Train set: Avg. loss: 0.000036100, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.03362 111.934944\n",
      "l2 norm: 902.1636731106839\n",
      "l1 norm: 756.53480240197\n",
      "Rbeta: 902.2414777316444\n",
      "\n",
      "Train set: Avg. loss: 0.000036054, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.04756 111.95057\n",
      "l2 norm: 902.1489418098217\n",
      "l1 norm: 756.5228412433211\n",
      "Rbeta: 902.2267973805315\n",
      "\n",
      "Train set: Avg. loss: 0.000036007, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.0615 111.966225\n",
      "l2 norm: 902.1364416151175\n",
      "l1 norm: 756.5127894232219\n",
      "Rbeta: 902.2143239298487\n",
      "\n",
      "Train set: Avg. loss: 0.000035960, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.07545 111.98187\n",
      "l2 norm: 902.1259643951845\n",
      "l1 norm: 756.5044798029336\n",
      "Rbeta: 902.2039391686243\n",
      "\n",
      "Train set: Avg. loss: 0.000035913, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.08939 111.99749\n",
      "l2 norm: 902.1099003413185\n",
      "l1 norm: 756.491559714848\n",
      "Rbeta: 902.1879543239095\n",
      "\n",
      "Train set: Avg. loss: 0.000035867, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.10334 112.01309\n",
      "l2 norm: 902.095130833761\n",
      "l1 norm: 756.4797946652767\n",
      "Rbeta: 902.1732210621904\n",
      "\n",
      "Train set: Avg. loss: 0.000035820, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.11729 112.028725\n",
      "l2 norm: 902.0755422639543\n",
      "l1 norm: 756.4640139065559\n",
      "Rbeta: 902.1537432809691\n",
      "\n",
      "Train set: Avg. loss: 0.000035773, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.13123 112.04435\n",
      "l2 norm: 902.0473937612637\n",
      "l1 norm: 756.4410431199433\n",
      "Rbeta: 902.1256557772059\n",
      "\n",
      "Train set: Avg. loss: 0.000035727, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.14518 112.060005\n",
      "l2 norm: 902.0209063563277\n",
      "l1 norm: 756.419452584181\n",
      "Rbeta: 902.0992369227979\n",
      "\n",
      "Train set: Avg. loss: 0.000035681, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.15913 112.07564\n",
      "l2 norm: 901.9976843557195\n",
      "l1 norm: 756.4005482401174\n",
      "Rbeta: 902.0761078139956\n",
      "\n",
      "Train set: Avg. loss: 0.000035634, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.17308 112.09128\n",
      "l2 norm: 901.9613374018663\n",
      "l1 norm: 756.3706171614216\n",
      "Rbeta: 902.0397797071178\n",
      "\n",
      "Train set: Avg. loss: 0.000035588, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.18703 112.10693\n",
      "l2 norm: 901.9277380042511\n",
      "l1 norm: 756.3430556038657\n",
      "Rbeta: 902.0062657265728\n",
      "\n",
      "Train set: Avg. loss: 0.000035542, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.20098 112.12256\n",
      "l2 norm: 901.8888354096478\n",
      "l1 norm: 756.3110205259178\n",
      "Rbeta: 901.9674426835307\n",
      "\n",
      "Train set: Avg. loss: 0.000035496, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.214935 112.13817\n",
      "l2 norm: 901.855189333143\n",
      "l1 norm: 756.28335025659\n",
      "Rbeta: 901.9338757128613\n",
      "\n",
      "Train set: Avg. loss: 0.000035450, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.22889 112.15372\n",
      "l2 norm: 901.8237893479874\n",
      "l1 norm: 756.2575236634759\n",
      "Rbeta: 901.9024806069596\n",
      "\n",
      "Train set: Avg. loss: 0.000035404, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.24284 112.16924\n",
      "l2 norm: 901.7952385605506\n",
      "l1 norm: 756.234123804821\n",
      "Rbeta: 901.8739898864827\n",
      "\n",
      "Train set: Avg. loss: 0.000035358, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.2568 112.18474\n",
      "l2 norm: 901.7659363108585\n",
      "l1 norm: 756.210094206144\n",
      "Rbeta: 901.8447275056315\n",
      "\n",
      "Train set: Avg. loss: 0.000035312, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.27075 112.200195\n",
      "l2 norm: 901.7385489013092\n",
      "l1 norm: 756.1876458744754\n",
      "Rbeta: 901.8174298683498\n",
      "\n",
      "Train set: Avg. loss: 0.000035267, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.28471 112.21559\n",
      "l2 norm: 901.7085824030005\n",
      "l1 norm: 756.1629876340476\n",
      "Rbeta: 901.787540174214\n",
      "\n",
      "Train set: Avg. loss: 0.000035221, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.29867 112.23099\n",
      "l2 norm: 901.677727555228\n",
      "l1 norm: 756.1375998011199\n",
      "Rbeta: 901.7567852227078\n",
      "\n",
      "Train set: Avg. loss: 0.000035176, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.31263 112.24635\n",
      "l2 norm: 901.6502552242961\n",
      "l1 norm: 756.1150216116193\n",
      "Rbeta: 901.7293615749503\n",
      "\n",
      "Train set: Avg. loss: 0.000035131, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.326584 112.26167\n",
      "l2 norm: 901.6234844664061\n",
      "l1 norm: 756.0930617785352\n",
      "Rbeta: 901.702573306928\n",
      "\n",
      "Train set: Avg. loss: 0.000035086, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.340546 112.27699\n",
      "l2 norm: 901.5881784894456\n",
      "l1 norm: 756.0639404562021\n",
      "Rbeta: 901.667327980258\n",
      "\n",
      "Train set: Avg. loss: 0.000035040, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.35451 112.29233\n",
      "l2 norm: 901.5483730726385\n",
      "l1 norm: 756.0310681524395\n",
      "Rbeta: 901.6275659269439\n",
      "\n",
      "Train set: Avg. loss: 0.000034995, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.36847 112.30762\n",
      "l2 norm: 901.5117203548976\n",
      "l1 norm: 756.0008355119202\n",
      "Rbeta: 901.5909648120002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000034950, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.38243 112.32288\n",
      "l2 norm: 901.4776405127276\n",
      "l1 norm: 755.9727119401505\n",
      "Rbeta: 901.5569365032948\n",
      "\n",
      "Train set: Avg. loss: 0.000034906, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.3964 112.33821\n",
      "l2 norm: 901.4490220550476\n",
      "l1 norm: 755.9491410727358\n",
      "Rbeta: 901.5284113046758\n",
      "\n",
      "Train set: Avg. loss: 0.000034861, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.41036 112.353546\n",
      "l2 norm: 901.424941852946\n",
      "l1 norm: 755.929340759413\n",
      "Rbeta: 901.5043733699888\n",
      "\n",
      "Train set: Avg. loss: 0.000034816, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.42411 112.36884\n",
      "l2 norm: 901.3999746197067\n",
      "l1 norm: 755.9088061783913\n",
      "Rbeta: 901.4794525326844\n",
      "\n",
      "Train set: Avg. loss: 0.000034775, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.43608 112.38411\n",
      "l2 norm: 901.3723689267853\n",
      "l1 norm: 755.886112951019\n",
      "Rbeta: 901.4519716660416\n",
      "\n",
      "Train set: Avg. loss: 0.000034733, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.44805 112.399376\n",
      "l2 norm: 901.3461690423419\n",
      "l1 norm: 755.8646141550153\n",
      "Rbeta: 901.4259182745241\n",
      "\n",
      "Train set: Avg. loss: 0.000034691, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.46003 112.41463\n",
      "l2 norm: 901.3215942884002\n",
      "l1 norm: 755.8445032173142\n",
      "Rbeta: 901.4015644324982\n",
      "\n",
      "Train set: Avg. loss: 0.000034650, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.472 112.42991\n",
      "l2 norm: 901.3003838316783\n",
      "l1 norm: 755.8272311779326\n",
      "Rbeta: 901.3804989024485\n",
      "\n",
      "Train set: Avg. loss: 0.000034608, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.48398 112.445206\n",
      "l2 norm: 901.2801947639622\n",
      "l1 norm: 755.8108348931571\n",
      "Rbeta: 901.3604834202835\n",
      "\n",
      "Train set: Avg. loss: 0.000034567, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.49595 112.46051\n",
      "l2 norm: 901.2633499256399\n",
      "l1 norm: 755.7972203133211\n",
      "Rbeta: 901.3437462690459\n",
      "\n",
      "Train set: Avg. loss: 0.000034526, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.50793 112.475784\n",
      "l2 norm: 901.243580692667\n",
      "l1 norm: 755.7811254669846\n",
      "Rbeta: 901.3241161598132\n",
      "\n",
      "Train set: Avg. loss: 0.000034484, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.5199 112.491035\n",
      "l2 norm: 901.2256033016837\n",
      "l1 norm: 755.7665196482105\n",
      "Rbeta: 901.3062761798878\n",
      "\n",
      "Train set: Avg. loss: 0.000034443, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.531876 112.50627\n",
      "l2 norm: 901.1906094822989\n",
      "l1 norm: 755.7376550383051\n",
      "Rbeta: 901.2714872739122\n",
      "\n",
      "Train set: Avg. loss: 0.000034402, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.54385 112.521454\n",
      "l2 norm: 901.1529509243084\n",
      "l1 norm: 755.7065432530152\n",
      "Rbeta: 901.2339547638633\n",
      "\n",
      "Train set: Avg. loss: 0.000034361, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.55583 112.53662\n",
      "l2 norm: 901.1238919875077\n",
      "l1 norm: 755.6826998430565\n",
      "Rbeta: 901.2050368025192\n",
      "\n",
      "Train set: Avg. loss: 0.000034320, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.56781 112.55177\n",
      "l2 norm: 901.0901437775836\n",
      "l1 norm: 755.6549841520081\n",
      "Rbeta: 901.1714349158725\n",
      "\n",
      "Train set: Avg. loss: 0.000034279, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.57979 112.566895\n",
      "l2 norm: 901.0503947781928\n",
      "l1 norm: 755.622178841943\n",
      "Rbeta: 901.1318371542148\n",
      "\n",
      "Train set: Avg. loss: 0.000034239, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.591774 112.58198\n",
      "l2 norm: 901.0119193485878\n",
      "l1 norm: 755.5904042435951\n",
      "Rbeta: 901.0935446811872\n",
      "\n",
      "Train set: Avg. loss: 0.000034198, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.60375 112.597046\n",
      "l2 norm: 900.9777532946168\n",
      "l1 norm: 755.5623142465336\n",
      "Rbeta: 901.0595587288907\n",
      "\n",
      "Train set: Avg. loss: 0.000034158, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.61573 112.61208\n",
      "l2 norm: 900.9430737743141\n",
      "l1 norm: 755.5337401197598\n",
      "Rbeta: 901.0249625158375\n",
      "\n",
      "Train set: Avg. loss: 0.000034117, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.627716 112.62709\n",
      "l2 norm: 900.9114174128622\n",
      "l1 norm: 755.5077101106697\n",
      "Rbeta: 900.9934722487511\n",
      "\n",
      "Train set: Avg. loss: 0.000034078, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.63888 112.64207\n",
      "l2 norm: 900.8881167974747\n",
      "l1 norm: 755.4886954616561\n",
      "Rbeta: 900.9703368163715\n",
      "\n",
      "Train set: Avg. loss: 0.000034041, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.648865 112.656975\n",
      "l2 norm: 900.8608960493051\n",
      "l1 norm: 755.4664496635987\n",
      "Rbeta: 900.9433959894119\n",
      "\n",
      "Train set: Avg. loss: 0.000034003, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.65885 112.671875\n",
      "l2 norm: 900.8373129982357\n",
      "l1 norm: 755.4472439303797\n",
      "Rbeta: 900.9200382821346\n",
      "\n",
      "Train set: Avg. loss: 0.000033966, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.66884 112.68674\n",
      "l2 norm: 900.8129931626928\n",
      "l1 norm: 755.4273446447984\n",
      "Rbeta: 900.895972504435\n",
      "\n",
      "Train set: Avg. loss: 0.000033929, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.678825 112.70155\n",
      "l2 norm: 900.7846880778798\n",
      "l1 norm: 755.4041187000508\n",
      "Rbeta: 900.8678594617331\n",
      "\n",
      "Train set: Avg. loss: 0.000033892, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.68881 112.71635\n",
      "l2 norm: 900.7591377229311\n",
      "l1 norm: 755.3832073781193\n",
      "Rbeta: 900.8425743285986\n",
      "\n",
      "Train set: Avg. loss: 0.000033855, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.6988 112.73109\n",
      "l2 norm: 900.7432649664897\n",
      "l1 norm: 755.3704021809372\n",
      "Rbeta: 900.8269505971177\n",
      "\n",
      "Train set: Avg. loss: 0.000033819, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.70879 112.74579\n",
      "l2 norm: 900.7243264037133\n",
      "l1 norm: 755.3550349298018\n",
      "Rbeta: 900.8083264524697\n",
      "\n",
      "Train set: Avg. loss: 0.000033782, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.71878 112.76047\n",
      "l2 norm: 900.7071253839824\n",
      "l1 norm: 755.3411286571993\n",
      "Rbeta: 900.791315332626\n",
      "\n",
      "Train set: Avg. loss: 0.000033746, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.72877 112.77513\n",
      "l2 norm: 900.6940556999268\n",
      "l1 norm: 755.3306784420745\n",
      "Rbeta: 900.7784927749732\n",
      "\n",
      "Train set: Avg. loss: 0.000033709, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.73876 112.789795\n",
      "l2 norm: 900.676063126941\n",
      "l1 norm: 755.3161349180589\n",
      "Rbeta: 900.7607835444991\n",
      "\n",
      "Train set: Avg. loss: 0.000033672, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.74875 112.80446\n",
      "l2 norm: 900.6629275348404\n",
      "l1 norm: 755.3056877301872\n",
      "Rbeta: 900.7478519982399\n",
      "\n",
      "Train set: Avg. loss: 0.000033636, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.75874 112.81912\n",
      "l2 norm: 900.6363839312694\n",
      "l1 norm: 755.2839621619378\n",
      "Rbeta: 900.7215151326207\n",
      "\n",
      "Train set: Avg. loss: 0.000033600, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.76873 112.83377\n",
      "l2 norm: 900.6029532169407\n",
      "l1 norm: 755.2564020698906\n",
      "Rbeta: 900.6883357356952\n",
      "\n",
      "Train set: Avg. loss: 0.000033563, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.778725 112.84837\n",
      "l2 norm: 900.5657867449215\n",
      "l1 norm: 755.2257535934049\n",
      "Rbeta: 900.6514517816291\n",
      "\n",
      "Train set: Avg. loss: 0.000033527, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.78872 112.8629\n",
      "l2 norm: 900.5335886879941\n",
      "l1 norm: 755.1992248850461\n",
      "Rbeta: 900.6194199650178\n",
      "\n",
      "Train set: Avg. loss: 0.000033491, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.79871 112.87741\n",
      "l2 norm: 900.5054953594251\n",
      "l1 norm: 755.1761262530424\n",
      "Rbeta: 900.5915766549662\n",
      "\n",
      "Train set: Avg. loss: 0.000033455, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.8087 112.89189\n",
      "l2 norm: 900.4744348237\n",
      "l1 norm: 755.1505663467703\n",
      "Rbeta: 900.5607416986132\n",
      "\n",
      "Train set: Avg. loss: 0.000033419, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.818695 112.906364\n",
      "l2 norm: 900.4525264627152\n",
      "l1 norm: 755.1327279478417\n",
      "Rbeta: 900.5390849773067\n",
      "\n",
      "Train set: Avg. loss: 0.000033383, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.82869 112.9208\n",
      "l2 norm: 900.4329331345182\n",
      "l1 norm: 755.1167839790385\n",
      "Rbeta: 900.5196827435478\n",
      "\n",
      "Train set: Avg. loss: 0.000033347, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.838684 112.935234\n",
      "l2 norm: 900.4137693924494\n",
      "l1 norm: 755.1012398412581\n",
      "Rbeta: 900.5007886403979\n",
      "\n",
      "Train set: Avg. loss: 0.000033311, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.84868 112.94968\n",
      "l2 norm: 900.3881819428086\n",
      "l1 norm: 755.0803184578905\n",
      "Rbeta: 900.4753842066855\n",
      "\n",
      "Train set: Avg. loss: 0.000033276, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.85867 112.96414\n",
      "l2 norm: 900.3648248129876\n",
      "l1 norm: 755.0612601983576\n",
      "Rbeta: 900.4522790073879\n",
      "\n",
      "Train set: Avg. loss: 0.000033240, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.868675 112.978615\n",
      "l2 norm: 900.345239219859\n",
      "l1 norm: 755.0453227378532\n",
      "Rbeta: 900.4329282908142\n",
      "\n",
      "Train set: Avg. loss: 0.000033204, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.87867 112.99308\n",
      "l2 norm: 900.3262068503686\n",
      "l1 norm: 755.029796835951\n",
      "Rbeta: 900.4141172782006\n",
      "\n",
      "Train set: Avg. loss: 0.000033169, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.888664 113.00749\n",
      "l2 norm: 900.3024695281481\n",
      "l1 norm: 755.0103558627286\n",
      "Rbeta: 900.3905662099115\n",
      "\n",
      "Train set: Avg. loss: 0.000033133, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.89867 113.02187\n",
      "l2 norm: 900.2790352793422\n",
      "l1 norm: 754.9912042068222\n",
      "Rbeta: 900.3673934091294\n",
      "\n",
      "Train set: Avg. loss: 0.000033098, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.90866 113.03624\n",
      "l2 norm: 900.2511812859456\n",
      "l1 norm: 754.9682789953308\n",
      "Rbeta: 900.339776682734\n",
      "\n",
      "Train set: Avg. loss: 0.000033062, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.91866 113.05058\n",
      "l2 norm: 900.2152886218819\n",
      "l1 norm: 754.938538106212\n",
      "Rbeta: 900.3040814970913\n",
      "\n",
      "Train set: Avg. loss: 0.000033027, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.92866 113.06494\n",
      "l2 norm: 900.1816219117613\n",
      "l1 norm: 754.9106487079732\n",
      "Rbeta: 900.2706249061143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000032992, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.93866 113.07922\n",
      "l2 norm: 900.1449370288254\n",
      "l1 norm: 754.880310364471\n",
      "Rbeta: 900.2342242677514\n",
      "\n",
      "Train set: Avg. loss: 0.000032956, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.948654 113.093475\n",
      "l2 norm: 900.1119617763212\n",
      "l1 norm: 754.8531054199004\n",
      "Rbeta: 900.2014394823517\n",
      "\n",
      "Train set: Avg. loss: 0.000032921, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.95866 113.10774\n",
      "l2 norm: 900.0801248180355\n",
      "l1 norm: 754.8268017451808\n",
      "Rbeta: 900.1698699535018\n",
      "\n",
      "Train set: Avg. loss: 0.000032886, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.96866 113.12201\n",
      "l2 norm: 900.0547305000218\n",
      "l1 norm: 754.8059003811122\n",
      "Rbeta: 900.1446882583946\n",
      "\n",
      "Train set: Avg. loss: 0.000032851, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.97866 113.13626\n",
      "l2 norm: 900.0308917607765\n",
      "l1 norm: 754.7863304863331\n",
      "Rbeta: 900.1210826925885\n",
      "\n",
      "Train set: Avg. loss: 0.000032816, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.98866 113.150536\n",
      "l2 norm: 900.0052819587938\n",
      "l1 norm: 754.765294753426\n",
      "Rbeta: 900.0956439908501\n",
      "\n",
      "Train set: Avg. loss: 0.000032781, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.998665 113.164825\n",
      "l2 norm: 899.9803026629013\n",
      "l1 norm: 754.7447862795037\n",
      "Rbeta: 900.0708762834707\n",
      "\n",
      "Train set: Avg. loss: 0.000032746, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.00867 113.17911\n",
      "l2 norm: 899.9515244643286\n",
      "l1 norm: 754.7210819979285\n",
      "Rbeta: 900.04232937515\n",
      "\n",
      "Train set: Avg. loss: 0.000032711, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.01867 113.1934\n",
      "l2 norm: 899.9258422340042\n",
      "l1 norm: 754.6999321478424\n",
      "Rbeta: 900.0168728061368\n",
      "\n",
      "Train set: Avg. loss: 0.000032676, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.02867 113.20769\n",
      "l2 norm: 899.8997258224875\n",
      "l1 norm: 754.6784432046454\n",
      "Rbeta: 899.9909906814156\n",
      "\n",
      "Train set: Avg. loss: 0.000032641, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.03867 113.22197\n",
      "l2 norm: 899.8689532676264\n",
      "l1 norm: 754.6530559414041\n",
      "Rbeta: 899.9604667204701\n",
      "\n",
      "Train set: Avg. loss: 0.000032606, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.048676 113.2362\n",
      "l2 norm: 899.8425892515475\n",
      "l1 norm: 754.6313826933366\n",
      "Rbeta: 899.9342995600804\n",
      "\n",
      "Train set: Avg. loss: 0.000032572, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.058685 113.25043\n",
      "l2 norm: 899.8178770956251\n",
      "l1 norm: 754.6110981608126\n",
      "Rbeta: 899.9097607826286\n",
      "\n",
      "Train set: Avg. loss: 0.000032537, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.06869 113.26466\n",
      "l2 norm: 899.7937194397567\n",
      "l1 norm: 754.5913141779504\n",
      "Rbeta: 899.8858230060376\n",
      "\n",
      "Train set: Avg. loss: 0.000032502, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.0787 113.27884\n",
      "l2 norm: 899.7688175869339\n",
      "l1 norm: 754.5709394989024\n",
      "Rbeta: 899.8612285270002\n",
      "\n",
      "Train set: Avg. loss: 0.000032468, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.0887 113.293045\n",
      "l2 norm: 899.7511014885516\n",
      "l1 norm: 754.5566007563943\n",
      "Rbeta: 899.8436624895764\n",
      "\n",
      "Train set: Avg. loss: 0.000032439, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.094864 113.30724\n",
      "l2 norm: 899.7341353363573\n",
      "l1 norm: 754.5429296237859\n",
      "Rbeta: 899.8271392069319\n",
      "\n",
      "Train set: Avg. loss: 0.000032410, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.10087 113.32144\n",
      "l2 norm: 899.7067080868229\n",
      "l1 norm: 754.5204633070878\n",
      "Rbeta: 899.8001770840473\n",
      "\n",
      "Train set: Avg. loss: 0.000032381, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.10687 113.33563\n",
      "l2 norm: 899.6767975377629\n",
      "l1 norm: 754.495922543694\n",
      "Rbeta: 899.7707378781549\n",
      "\n",
      "Train set: Avg. loss: 0.000032352, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.11288 113.34981\n",
      "l2 norm: 899.6464971023522\n",
      "l1 norm: 754.4710513527378\n",
      "Rbeta: 899.7408591534221\n",
      "\n",
      "Train set: Avg. loss: 0.000032324, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.11888 113.36396\n",
      "l2 norm: 899.6150649841038\n",
      "l1 norm: 754.4451983138421\n",
      "Rbeta: 899.7099623077195\n",
      "\n",
      "Train set: Avg. loss: 0.000032295, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.124886 113.378105\n",
      "l2 norm: 899.5797590894261\n",
      "l1 norm: 754.4161305344064\n",
      "Rbeta: 899.6750648557638\n",
      "\n",
      "Train set: Avg. loss: 0.000032266, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.13089 113.392235\n",
      "l2 norm: 899.5433722484571\n",
      "l1 norm: 754.3861144467066\n",
      "Rbeta: 899.639151246946\n",
      "\n",
      "Train set: Avg. loss: 0.000032238, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.136894 113.40636\n",
      "l2 norm: 899.5069460913761\n",
      "l1 norm: 754.356074235247\n",
      "Rbeta: 899.6031241645901\n",
      "\n",
      "Train set: Avg. loss: 0.000032209, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.1429 113.42049\n",
      "l2 norm: 899.472569806827\n",
      "l1 norm: 754.327725746121\n",
      "Rbeta: 899.5692249172372\n",
      "\n",
      "Train set: Avg. loss: 0.000032180, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.1489 113.43467\n",
      "l2 norm: 899.4399937391331\n",
      "l1 norm: 754.3008560296867\n",
      "Rbeta: 899.5371363543003\n",
      "\n",
      "Train set: Avg. loss: 0.000032152, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.154915 113.448814\n",
      "l2 norm: 899.4140756405909\n",
      "l1 norm: 754.2795818445643\n",
      "Rbeta: 899.511673132858\n",
      "\n",
      "Train set: Avg. loss: 0.000032123, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.16092 113.46297\n",
      "l2 norm: 899.3904451570837\n",
      "l1 norm: 754.2601839402214\n",
      "Rbeta: 899.4885341942894\n",
      "\n",
      "Train set: Avg. loss: 0.000032095, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.16692 113.47714\n",
      "l2 norm: 899.3678936343103\n",
      "l1 norm: 754.2416493798119\n",
      "Rbeta: 899.4664936513851\n",
      "\n",
      "Train set: Avg. loss: 0.000032066, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.17293 113.491295\n",
      "l2 norm: 899.3456362889626\n",
      "l1 norm: 754.2232943353697\n",
      "Rbeta: 899.4445974693897\n",
      "\n",
      "Train set: Avg. loss: 0.000032038, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.17893 113.50546\n",
      "l2 norm: 899.3246745962795\n",
      "l1 norm: 754.2060198920933\n",
      "Rbeta: 899.4241223799261\n",
      "\n",
      "Train set: Avg. loss: 0.000032009, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.184944 113.51964\n",
      "l2 norm: 899.303506459536\n",
      "l1 norm: 754.1886067459285\n",
      "Rbeta: 899.4035051941428\n",
      "\n",
      "Train set: Avg. loss: 0.000031981, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.19095 113.53381\n",
      "l2 norm: 899.2834372109277\n",
      "l1 norm: 754.1720991550665\n",
      "Rbeta: 899.383847515772\n",
      "\n",
      "Train set: Avg. loss: 0.000031952, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.19695 113.54798\n",
      "l2 norm: 899.2609667202297\n",
      "l1 norm: 754.1535754419799\n",
      "Rbeta: 899.361824015736\n",
      "\n",
      "Train set: Avg. loss: 0.000031924, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.202965 113.56215\n",
      "l2 norm: 899.2367250957427\n",
      "l1 norm: 754.1335627764164\n",
      "Rbeta: 899.3380281513003\n",
      "\n",
      "Train set: Avg. loss: 0.000031896, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.20897 113.57634\n",
      "l2 norm: 899.2104905641513\n",
      "l1 norm: 754.111881584168\n",
      "Rbeta: 899.3123140577129\n",
      "\n",
      "Train set: Avg. loss: 0.000031867, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.21497 113.59054\n",
      "l2 norm: 899.1824881457023\n",
      "l1 norm: 754.0886767171567\n",
      "Rbeta: 899.284775349355\n",
      "\n",
      "Train set: Avg. loss: 0.000031839, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.220985 113.60472\n",
      "l2 norm: 899.155907379441\n",
      "l1 norm: 754.0666267058821\n",
      "Rbeta: 899.2586968902702\n",
      "\n",
      "Train set: Avg. loss: 0.000031811, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.22699 113.61889\n",
      "l2 norm: 899.131225686012\n",
      "l1 norm: 754.0461406689653\n",
      "Rbeta: 899.2344937268068\n",
      "\n",
      "Train set: Avg. loss: 0.000031782, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.232994 113.633026\n",
      "l2 norm: 899.1071153214103\n",
      "l1 norm: 754.0261029082463\n",
      "Rbeta: 899.210819502911\n",
      "\n",
      "Train set: Avg. loss: 0.000031754, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.239006 113.647125\n",
      "l2 norm: 899.0909083219257\n",
      "l1 norm: 754.0127072954031\n",
      "Rbeta: 899.1951003476906\n",
      "\n",
      "Train set: Avg. loss: 0.000031726, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.24501 113.66122\n",
      "l2 norm: 899.0748672199749\n",
      "l1 norm: 753.9994414226048\n",
      "Rbeta: 899.1794772427781\n",
      "\n",
      "Train set: Avg. loss: 0.000031698, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.25102 113.6753\n",
      "l2 norm: 899.0590120191565\n",
      "l1 norm: 753.9862867680382\n",
      "Rbeta: 899.1641952295138\n",
      "\n",
      "Train set: Avg. loss: 0.000031670, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.25703 113.68938\n",
      "l2 norm: 899.0439676644972\n",
      "l1 norm: 753.9738361560869\n",
      "Rbeta: 899.1496008722513\n",
      "\n",
      "Train set: Avg. loss: 0.000031642, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.26304 113.70346\n",
      "l2 norm: 899.0293620121621\n",
      "l1 norm: 753.9617787294387\n",
      "Rbeta: 899.1354399113196\n",
      "\n",
      "Train set: Avg. loss: 0.000031614, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.26904 113.717545\n",
      "l2 norm: 899.0155419349354\n",
      "l1 norm: 753.9503880819115\n",
      "Rbeta: 899.1220961315969\n",
      "\n",
      "Train set: Avg. loss: 0.000031586, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.275055 113.73166\n",
      "l2 norm: 899.0033232353746\n",
      "l1 norm: 753.9403356558848\n",
      "Rbeta: 899.1103833458993\n",
      "\n",
      "Train set: Avg. loss: 0.000031558, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.28107 113.74576\n",
      "l2 norm: 898.9900147133669\n",
      "l1 norm: 753.9293836066993\n",
      "Rbeta: 899.0975363453027\n",
      "\n",
      "Train set: Avg. loss: 0.000031530, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.28707 113.75987\n",
      "l2 norm: 898.9755684113585\n",
      "l1 norm: 753.9175043886097\n",
      "Rbeta: 899.0835629995947\n",
      "\n",
      "Train set: Avg. loss: 0.000031503, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.29308 113.773926\n",
      "l2 norm: 898.9680511020973\n",
      "l1 norm: 753.9115381027459\n",
      "Rbeta: 899.0765660209672\n",
      "\n",
      "Train set: Avg. loss: 0.000031475, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.299095 113.78798\n",
      "l2 norm: 898.9629046289562\n",
      "l1 norm: 753.9075930927352\n",
      "Rbeta: 899.0718667839424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000031447, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.3051 113.802\n",
      "l2 norm: 898.9538882476563\n",
      "l1 norm: 753.9004079597772\n",
      "Rbeta: 899.0633449955841\n",
      "\n",
      "Train set: Avg. loss: 0.000031419, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.31111 113.815994\n",
      "l2 norm: 898.9467570752009\n",
      "l1 norm: 753.8948493340981\n",
      "Rbeta: 899.0566908610174\n",
      "\n",
      "Train set: Avg. loss: 0.000031392, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.31712 113.829956\n",
      "l2 norm: 898.9313940954771\n",
      "l1 norm: 753.8824530633714\n",
      "Rbeta: 899.041801744274\n",
      "\n",
      "Train set: Avg. loss: 0.000031364, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.32313 113.8439\n",
      "l2 norm: 898.9101811951543\n",
      "l1 norm: 753.8652155941948\n",
      "Rbeta: 899.0210446160489\n",
      "\n",
      "Train set: Avg. loss: 0.000031337, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.32914 113.857834\n",
      "l2 norm: 898.8974028438494\n",
      "l1 norm: 753.8550804598816\n",
      "Rbeta: 899.008787319337\n",
      "\n",
      "Train set: Avg. loss: 0.000031309, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.33515 113.87178\n",
      "l2 norm: 898.8856133079465\n",
      "l1 norm: 753.8457897433516\n",
      "Rbeta: 898.9974993911592\n",
      "\n",
      "Train set: Avg. loss: 0.000031282, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.34116 113.88575\n",
      "l2 norm: 898.8759214730934\n",
      "l1 norm: 753.8382195421862\n",
      "Rbeta: 898.9882621306342\n",
      "\n",
      "Train set: Avg. loss: 0.000031254, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.347176 113.899704\n",
      "l2 norm: 898.8652137602578\n",
      "l1 norm: 753.8298035793566\n",
      "Rbeta: 898.978060065228\n",
      "\n",
      "Train set: Avg. loss: 0.000031227, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.35319 113.913666\n",
      "l2 norm: 898.8565627481635\n",
      "l1 norm: 753.8231225189242\n",
      "Rbeta: 898.969839343183\n",
      "\n",
      "Train set: Avg. loss: 0.000031199, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.35919 113.92761\n",
      "l2 norm: 898.8491391054675\n",
      "l1 norm: 753.8175093723362\n",
      "Rbeta: 898.9629088120463\n",
      "\n",
      "Train set: Avg. loss: 0.000031172, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.365204 113.94153\n",
      "l2 norm: 898.838968015884\n",
      "l1 norm: 753.8096470937985\n",
      "Rbeta: 898.9532267486685\n",
      "\n",
      "Train set: Avg. loss: 0.000031145, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.371216 113.955414\n",
      "l2 norm: 898.8255632300271\n",
      "l1 norm: 753.7990946471746\n",
      "Rbeta: 898.940310952757\n",
      "\n",
      "Train set: Avg. loss: 0.000031117, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.37723 113.96926\n",
      "l2 norm: 898.8058649791457\n",
      "l1 norm: 753.783246789467\n",
      "Rbeta: 898.9211032706384\n",
      "\n",
      "Train set: Avg. loss: 0.000031090, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.38324 113.983116\n",
      "l2 norm: 898.7837026810697\n",
      "l1 norm: 753.765336951951\n",
      "Rbeta: 898.8994312511686\n",
      "\n",
      "Train set: Avg. loss: 0.000031063, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.38925 113.99699\n",
      "l2 norm: 898.7532365857465\n",
      "l1 norm: 753.7404785964579\n",
      "Rbeta: 898.869393723144\n",
      "\n",
      "Train set: Avg. loss: 0.000031036, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.39526 114.01083\n",
      "l2 norm: 898.7226755475018\n",
      "l1 norm: 753.7155496181788\n",
      "Rbeta: 898.8393635204982\n",
      "\n",
      "Train set: Avg. loss: 0.000031009, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.401276 114.02466\n",
      "l2 norm: 898.6906372855561\n",
      "l1 norm: 753.6893094335715\n",
      "Rbeta: 898.8077701644629\n",
      "\n",
      "Train set: Avg. loss: 0.000030982, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.40729 114.03849\n",
      "l2 norm: 898.6605016972348\n",
      "l1 norm: 753.6646463734157\n",
      "Rbeta: 898.7781153150714\n",
      "\n",
      "Train set: Avg. loss: 0.000030955, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.4133 114.052284\n",
      "l2 norm: 898.6282072397217\n",
      "l1 norm: 753.6381904193938\n",
      "Rbeta: 898.7463078243564\n",
      "\n",
      "Train set: Avg. loss: 0.000030928, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.41931 114.06604\n",
      "l2 norm: 898.5982340615077\n",
      "l1 norm: 753.6136731349866\n",
      "Rbeta: 898.716798642895\n",
      "\n",
      "Train set: Avg. loss: 0.000030901, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.42533 114.07979\n",
      "l2 norm: 898.5706341596192\n",
      "l1 norm: 753.5911443047053\n",
      "Rbeta: 898.6897624666846\n",
      "\n",
      "Train set: Avg. loss: 0.000030874, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.43134 114.09349\n",
      "l2 norm: 898.5397787018826\n",
      "l1 norm: 753.5658046279841\n",
      "Rbeta: 898.659379709653\n",
      "\n",
      "Train set: Avg. loss: 0.000030847, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.437355 114.10716\n",
      "l2 norm: 898.5101975826068\n",
      "l1 norm: 753.5415027801455\n",
      "Rbeta: 898.6302755347529\n",
      "\n",
      "Train set: Avg. loss: 0.000030821, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.44337 114.12087\n",
      "l2 norm: 898.4868904323986\n",
      "l1 norm: 753.5224460028427\n",
      "Rbeta: 898.6074087784074\n",
      "\n",
      "Train set: Avg. loss: 0.000030794, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.4489 114.1346\n",
      "l2 norm: 898.4627342755505\n",
      "l1 norm: 753.5027068731941\n",
      "Rbeta: 898.5837335556489\n",
      "\n",
      "Train set: Avg. loss: 0.000030770, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.45291 114.14834\n",
      "l2 norm: 898.4342840522993\n",
      "l1 norm: 753.4793798640048\n",
      "Rbeta: 898.555956046828\n",
      "\n",
      "Train set: Avg. loss: 0.000030746, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.45692 114.16203\n",
      "l2 norm: 898.4014565976842\n",
      "l1 norm: 753.4523665389158\n",
      "Rbeta: 898.5237093088699\n",
      "\n",
      "Train set: Avg. loss: 0.000030723, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.46093 114.17572\n",
      "l2 norm: 898.3705967766556\n",
      "l1 norm: 753.4269786738928\n",
      "Rbeta: 898.4935049082715\n",
      "\n",
      "Train set: Avg. loss: 0.000030699, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.464935 114.18939\n",
      "l2 norm: 898.3379097880121\n",
      "l1 norm: 753.4000350128824\n",
      "Rbeta: 898.4613647762401\n",
      "\n",
      "Train set: Avg. loss: 0.000030675, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.46895 114.20305\n",
      "l2 norm: 898.312194294151\n",
      "l1 norm: 753.378938720079\n",
      "Rbeta: 898.4362986919967\n",
      "\n",
      "Train set: Avg. loss: 0.000030651, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.47295 114.21666\n",
      "l2 norm: 898.2803184353639\n",
      "l1 norm: 753.3526766093933\n",
      "Rbeta: 898.4049987201957\n",
      "\n",
      "Train set: Avg. loss: 0.000030627, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.47697 114.23021\n",
      "l2 norm: 898.245947106135\n",
      "l1 norm: 753.3242899347956\n",
      "Rbeta: 898.3713359861378\n",
      "\n",
      "Train set: Avg. loss: 0.000030604, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.48097 114.24373\n",
      "l2 norm: 898.2107627339894\n",
      "l1 norm: 753.2952054268513\n",
      "Rbeta: 898.3366878897269\n",
      "\n",
      "Train set: Avg. loss: 0.000030580, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.484985 114.25725\n",
      "l2 norm: 898.1804575672382\n",
      "l1 norm: 753.2702762842208\n",
      "Rbeta: 898.3070311832734\n",
      "\n",
      "Train set: Avg. loss: 0.000030556, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.48899 114.270744\n",
      "l2 norm: 898.1557867788702\n",
      "l1 norm: 753.2500935726731\n",
      "Rbeta: 898.2829297204224\n",
      "\n",
      "Train set: Avg. loss: 0.000030533, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.493004 114.28421\n",
      "l2 norm: 898.1348053550003\n",
      "l1 norm: 753.2329563927835\n",
      "Rbeta: 898.2626236064003\n",
      "\n",
      "Train set: Avg. loss: 0.000030509, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.49701 114.297646\n",
      "l2 norm: 898.1147909868026\n",
      "l1 norm: 753.2166419780605\n",
      "Rbeta: 898.2431473570691\n",
      "\n",
      "Train set: Avg. loss: 0.000030486, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.50102 114.31105\n",
      "l2 norm: 898.0973450125667\n",
      "l1 norm: 753.2025045722459\n",
      "Rbeta: 898.2263488443648\n",
      "\n",
      "Train set: Avg. loss: 0.000030463, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.505035 114.32445\n",
      "l2 norm: 898.0805194528543\n",
      "l1 norm: 753.1888814683309\n",
      "Rbeta: 898.2101342619569\n",
      "\n",
      "Train set: Avg. loss: 0.000030439, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.50904 114.33786\n",
      "l2 norm: 898.0628681083679\n",
      "l1 norm: 753.1745651356528\n",
      "Rbeta: 898.1930673290345\n",
      "\n",
      "Train set: Avg. loss: 0.000030416, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.513054 114.351295\n",
      "l2 norm: 898.0441557546837\n",
      "l1 norm: 753.1594282446819\n",
      "Rbeta: 898.1750104308212\n",
      "\n",
      "Train set: Avg. loss: 0.000030393, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.51707 114.364746\n",
      "l2 norm: 898.0226405163645\n",
      "l1 norm: 753.1419613415262\n",
      "Rbeta: 898.1540706210476\n",
      "\n",
      "Train set: Avg. loss: 0.000030369, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.52107 114.37822\n",
      "l2 norm: 897.9982842663164\n",
      "l1 norm: 753.1221051510245\n",
      "Rbeta: 898.1303708457248\n",
      "\n",
      "Train set: Avg. loss: 0.000030346, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.525085 114.39165\n",
      "l2 norm: 897.9736390930437\n",
      "l1 norm: 753.1020258526927\n",
      "Rbeta: 898.1063863411773\n",
      "\n",
      "Train set: Avg. loss: 0.000030323, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.52909 114.405014\n",
      "l2 norm: 897.9582029829918\n",
      "l1 norm: 753.0896361381507\n",
      "Rbeta: 898.0915671779964\n",
      "\n",
      "Train set: Avg. loss: 0.000030300, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.533104 114.41835\n",
      "l2 norm: 897.9460734988139\n",
      "l1 norm: 753.0800090063707\n",
      "Rbeta: 898.0799873594957\n",
      "\n",
      "Train set: Avg. loss: 0.000030276, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.53712 114.43168\n",
      "l2 norm: 897.9347558071138\n",
      "l1 norm: 753.0710444813656\n",
      "Rbeta: 898.0693252827085\n",
      "\n",
      "Train set: Avg. loss: 0.000030253, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.54113 114.445\n",
      "l2 norm: 897.9238261461542\n",
      "l1 norm: 753.0623404311727\n",
      "Rbeta: 898.0590315266553\n",
      "\n",
      "Train set: Avg. loss: 0.000030230, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.545135 114.45834\n",
      "l2 norm: 897.913408632161\n",
      "l1 norm: 753.0540618114003\n",
      "Rbeta: 898.0492277826943\n",
      "\n",
      "Train set: Avg. loss: 0.000030207, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.54915 114.47168\n",
      "l2 norm: 897.8958087441898\n",
      "l1 norm: 753.0397633293618\n",
      "Rbeta: 898.0322694479997\n",
      "\n",
      "Train set: Avg. loss: 0.000030184, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.55316 114.485\n",
      "l2 norm: 897.8743451711207\n",
      "l1 norm: 753.0222224996596\n",
      "Rbeta: 898.0114087821887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000030161, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.55717 114.49833\n",
      "l2 norm: 897.851256008883\n",
      "l1 norm: 753.003335361942\n",
      "Rbeta: 897.9889505495472\n",
      "\n",
      "Train set: Avg. loss: 0.000030138, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.56118 114.51162\n",
      "l2 norm: 897.8290078881861\n",
      "l1 norm: 752.9852146009082\n",
      "Rbeta: 897.9673360706855\n",
      "\n",
      "Train set: Avg. loss: 0.000030115, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.56519 114.52488\n",
      "l2 norm: 897.8070575786452\n",
      "l1 norm: 752.9673559092263\n",
      "Rbeta: 897.9460312380145\n",
      "\n",
      "Train set: Avg. loss: 0.000030092, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.56921 114.5381\n",
      "l2 norm: 897.7861826320341\n",
      "l1 norm: 752.9503713638421\n",
      "Rbeta: 897.9257222661541\n",
      "\n",
      "Train set: Avg. loss: 0.000030070, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.57321 114.551346\n",
      "l2 norm: 897.7630251948067\n",
      "l1 norm: 752.9314647930862\n",
      "Rbeta: 897.9031909749511\n",
      "\n",
      "Train set: Avg. loss: 0.000030047, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.577225 114.56462\n",
      "l2 norm: 897.7314232785882\n",
      "l1 norm: 752.9054939119909\n",
      "Rbeta: 897.8722487421836\n",
      "\n",
      "Train set: Avg. loss: 0.000030024, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.58124 114.5779\n",
      "l2 norm: 897.7005100623469\n",
      "l1 norm: 752.8801218683388\n",
      "Rbeta: 897.8420144323316\n",
      "\n",
      "Train set: Avg. loss: 0.000030001, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.58525 114.59117\n",
      "l2 norm: 897.6726656163643\n",
      "l1 norm: 752.8572826230616\n",
      "Rbeta: 897.8147589618779\n",
      "\n",
      "Train set: Avg. loss: 0.000029978, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.589264 114.60443\n",
      "l2 norm: 897.6449918734451\n",
      "l1 norm: 752.8345975841305\n",
      "Rbeta: 897.7877226663093\n",
      "\n",
      "Train set: Avg. loss: 0.000029955, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.59327 114.61769\n",
      "l2 norm: 897.6153098120869\n",
      "l1 norm: 752.8102340858215\n",
      "Rbeta: 897.7586657029527\n",
      "\n",
      "Train set: Avg. loss: 0.000029933, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.59728 114.63097\n",
      "l2 norm: 897.5884376657729\n",
      "l1 norm: 752.7882062806126\n",
      "Rbeta: 897.7323877140449\n",
      "\n",
      "Train set: Avg. loss: 0.000029910, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.601295 114.644226\n",
      "l2 norm: 897.5707285037532\n",
      "l1 norm: 752.773861604778\n",
      "Rbeta: 897.7152852062932\n",
      "\n",
      "Train set: Avg. loss: 0.000029887, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.60531 114.6575\n",
      "l2 norm: 897.5552364447478\n",
      "l1 norm: 752.7613493843056\n",
      "Rbeta: 897.7005465254475\n",
      "\n",
      "Train set: Avg. loss: 0.000029865, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.60932 114.67076\n",
      "l2 norm: 897.538776892503\n",
      "l1 norm: 752.7480126792758\n",
      "Rbeta: 897.6847295472486\n",
      "\n",
      "Train set: Avg. loss: 0.000029842, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.613335 114.684\n",
      "l2 norm: 897.5183138174398\n",
      "l1 norm: 752.7313154307262\n",
      "Rbeta: 897.6648500559992\n",
      "\n",
      "Train set: Avg. loss: 0.000029819, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.61734 114.69722\n",
      "l2 norm: 897.501498375231\n",
      "l1 norm: 752.7177378082274\n",
      "Rbeta: 897.648643963554\n",
      "\n",
      "Train set: Avg. loss: 0.000029797, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.62135 114.71044\n",
      "l2 norm: 897.4838128302222\n",
      "l1 norm: 752.7033967581161\n",
      "Rbeta: 897.631588395022\n",
      "\n",
      "Train set: Avg. loss: 0.000029774, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.62537 114.72365\n",
      "l2 norm: 897.4660853069988\n",
      "l1 norm: 752.689016516517\n",
      "Rbeta: 897.6145360821315\n",
      "\n",
      "Train set: Avg. loss: 0.000029752, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.62938 114.736824\n",
      "l2 norm: 897.4461143889143\n",
      "l1 norm: 752.6727558569314\n",
      "Rbeta: 897.5952346247471\n",
      "\n",
      "Train set: Avg. loss: 0.000029729, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.63339 114.749985\n",
      "l2 norm: 897.4175841488485\n",
      "l1 norm: 752.6493305983072\n",
      "Rbeta: 897.5672931168965\n",
      "\n",
      "Train set: Avg. loss: 0.000029707, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.637405 114.76314\n",
      "l2 norm: 897.3939092104556\n",
      "l1 norm: 752.6299912259738\n",
      "Rbeta: 897.5442572236699\n",
      "\n",
      "Train set: Avg. loss: 0.000029684, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.64142 114.77626\n",
      "l2 norm: 897.3641224034342\n",
      "l1 norm: 752.6055185916348\n",
      "Rbeta: 897.5151405265271\n",
      "\n",
      "Train set: Avg. loss: 0.000029663, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.64495 114.78935\n",
      "l2 norm: 897.3362336668413\n",
      "l1 norm: 752.5826655881017\n",
      "Rbeta: 897.4879046548394\n",
      "\n",
      "Train set: Avg. loss: 0.000029643, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.64696 114.80237\n",
      "l2 norm: 897.3127307802862\n",
      "l1 norm: 752.5634737322373\n",
      "Rbeta: 897.4652174016205\n",
      "After training:\n",
      "\n",
      "Train set: Avg. loss: 0.000029643, Accuracy: 512/512 (100%)\n",
      "\n",
      "tensor(49.) tensor(1.0718e+22)\n",
      "110.64696 114.80237\n",
      "l2 norm: 897.3127307802862\n",
      "l1 norm: 752.5634737322373\n",
      "Rbeta: 897.4652174016205\n",
      "Time domain:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency domain:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOSUlEQVR4nO3db6hcdX7H8ffX9EZv1sTV5jaEJNZdKxQp3ahDsKwsdpddrBRUKGIeLD4wm6WsUGH7QCx0LbTglvrvQbHEGDZbrK6tyoYiZa0syD5xvbEas6atrkQ2ISY3uGpKosbk2wdzQm/Se+bOnZlz5l5/7xdc5sz5zcz5cuZ+5syc38zvF5mJpM++88ZdgKR2GHapEIZdKoRhlwph2KVCGHapEL8xzJ0j4gbgYWAZsD0z7+t1+8nJyVy1atWcbRs2bKi93+nTp+dcf9557b5WHT9+fM71K1asaLUONavueYZ2n+u6/3uo/9/fv38/R48ejbnaBg57RCwD/h74OnAAeDkidmXmG3X3WbVqFZs3b56z7aGHHqrd1mIJ2e7du+dcf80117Rah5pV9zxDu8/1IC86nU6n9j7DHBo3AW9l5tuZ+QnwJHDTEI8nqUHDhH0d8KtZ1w9U6yQtQo1/6I2IrRExHRHTJ06caHpzkmoME/aDwOyzauurdWfJzG2Z2cnMzuTk5BCbkzSMYcL+MnBFRHwhIpYDtwG7RlOWpFGLYX71FhE3Ag/R7XrbkZl/0+v2nU4np6enB96epN46nQ7T09Oj7XoDyMzngOeGeQxJ7fAbdFIhDLtUCMMuFcKwS4Uw7FIhDLtUCMMuFcKwS4Uw7FIhDLtUCMMuFcKwS4Uw7FIhDLtUCMMuFcKwS4Uw7FIhDLtUiKGGpVLZBpmeSOPjMyIVwrBLhTDsUiEMu1QIwy4VwrBLhRiq6y0i9gPHgFPAp5lZPxP8POzGWXp8XpaWUfSz/2FmHh3B40hqkC/NUiGGDXsCP4mI3RGxdRQFSWrGsG/jr8vMgxHxW8DzEfGfmfni7BtULwJbAS699NIhNydpUEMd2TPzYHV5BHgW2DTHbbZlZiczO1NTU8NsTtIQBg57RHwuIlaeWQa+AewdVWGSRmuYt/FrgGcj4szj/FNm/tugD2Y3jtSsgcOemW8DXxphLZIa5OFUKoRhlwph2KVCGHapEIZdKoQDThZi+/bttW1btmxpsRKNi0d2qRCGXSqEYZcKYdilQhh2qRCejS+EZ9zlkV0qhGGXCmHYpUIYdqkQhl0qhGGXCmHYpUIYdqkQhl0qhGGXCmHYpUIYdqkQhl0qxLxhj4gdEXEkIvbOWndJRDwfEW9Wlxc3W6ZmO3nyZO2fVKefI/sPgBvOWXc38EJmXgG8UF2XtIjNG/ZqvvX3zll9E7CzWt4J3DzasiSN2qCf2ddk5qFq+V26M7pKWsSGPkGXmQlkXXtEbI2I6YiYnpmZGXZzkgY0aNgPR8RagOrySN0NM3NbZnYyszM1NTXg5iQNa9Cw7wJur5ZvB348mnIkNWXeAScj4gngemB1RBwAvgfcBzwVEXcA7wC3NlmkzjYxMTHSxztx4kRt2+TkZG3b0aNHa9tWrlw55/rzzz+//8I0UvOGPTM31zR9bcS1SGqQ36CTCmHYpUIYdqkQhl0qhGGXCuFcb0vQxx9/XNs2SNdWr+61XlavXj3Q/ep0v4w5t4gY6bZK5JFdKoRhlwph2KVCGHapEIZdKoRhlwph19s5jh8/Xtu2YsWKFiupN+pfjvX61duyZctq25YvXz7SOj766KPatkG7B/V/PLJLhTDsUiEMu1QIwy4VwrBLhfBs/DkWyxn3Ufvwww9r21atWjXQYx4+fLi2bc2ahU8l0OuM+7Fjx2rb6sa7a0KvHoMLLrhgpNv65JNPatsG6QnxyC4VwrBLhTDsUiEMu1QIwy4VwrBLhehn+qcdwB8DRzLz96p19wLfAs5My3pPZj7XVJE62yBj0DXRvXbRRRct+PE++OCDgR6vze61XkbdvdbLqH9o1M+R/QfADXOsfzAzN1Z/Bl1a5OYNe2a+CLzXQi2SGjTMZ/Y7I2JPROyIiItHVpGkRgwa9keAy4GNwCHg/robRsTWiJiOiOmZmZm6m0lq2EBhz8zDmXkqM08DjwKbetx2W2Z2MrMzNTU1aJ2ShjRQ2CNi7ayrtwB7R1OOpKb00/X2BHA9sDoiDgDfA66PiI1AAvuBbzdXos7Vawy6uimUek2fdODAgdq29evXj/R+g3YBanjzhj0zN8+x+rEGapHUIL9BJxXCsEuFMOxSIQy7VAjDLhXCASc/Y+oGluz1i7LVq1fXtvX61Vuvbrk6vboAB/k1n/rnkV0qhGGXCmHYpUIYdqkQhl0qhGGXCrEkut7q5rwa9YB80O5cXr0MOrfZqAeBHGTONqjvAuz1qze715rlkV0qhGGXCmHYpUIYdqkQhl0qxJI4G9/EWfc6bZ5x76XXGfcTJ07Utk1OTi54W73OuNf1hACcOnWqtu2zOtbc8ePHa9tWrFjRYiUL55FdKoRhlwph2KVCGHapEIZdKoRhlwrRz/RPG4AfAmvoTve0LTMfjohLgB8Bl9GdAurWzPx1c6XqjDa7B9vs9lwKY9At9u61Xvo5sn8KfDczrwSuBb4TEVcCdwMvZOYVwAvVdUmL1Lxhz8xDmflKtXwM2AesA24CdlY32wnc3FCNkkZgQZ/ZI+Iy4CrgJWBNZh6qmt6l+zZf0iLVd9gj4kLgaeCuzDxrZILszhM851zBEbE1IqYjYnpmZmaoYiUNrq+wR8QE3aA/npnPVKsPR8Taqn0tcGSu+2bmtszsZGZnampqFDVLGsC8YY/uFB6PAfsy84FZTbuA26vl24Efj748SaPSz6/evgx8E3g9Il6t1t0D3Ac8FRF3AO8AtzZSof6fXlMo1enVrdVrvLteU0ON+td3i6V77bNq3rBn5s+Auv+ur422HElN8Rt0UiEMu1QIwy4VwrBLhTDsUiGWxICTGl6vbq1Bu7wG6V7r5eTJk7VtExMTI91WiTyyS4Uw7FIhDLtUCMMuFcKwS4Uw7FIh7HrTomH3WrM8skuFMOxSIQy7VAjDLhXCsEuFMOxSIQy7VAjDLhXCsEuFMOxSIQy7VAjDLhVi3h/CRMQG4Id0p2ROYFtmPhwR9wLfAs5MzXpPZj7XVKFaWrZv3z7n+i1btrRcic7o51dvnwLfzcxXImIlsDsinq/aHszMv2uuPEmj0s9cb4eAQ9XysYjYB6xrujBJo7Wgz+wRcRlwFfBSterOiNgTETsi4uJRFydpdPoOe0RcCDwN3JWZHwKPAJcDG+ke+e+vud/WiJiOiOmZmZm5biKpBX2FPSIm6Ab98cx8BiAzD2fmqcw8DTwKbJrrvpm5LTM7mdmZmpoaVd2SFmjesEdEAI8B+zLzgVnr18662S3A3tGXJ2lU+jkb/2Xgm8DrEfFqte4eYHNEbKTbHbcf+HYD9WmJsott8ennbPzPgJijyT51aQnxG3RSIQy7VAjDLhXCsEuFMOxSIZz+SVqCTp8+veD7eGSXCmHYpUIYdqkQhl0qhGGXCmHYpULY9aae3TjnnefxYDEa5HnxmZQKYdilQhh2qRCGXSqEYZcKYdilQtj1JrvXCuGzLBXCsEuFMOxSIQy7VAjDLhWin7neLoiIn0fEaxHxi4j4q2r9FyLipYh4KyJ+FBHLmy9X0qD6ObJ/DHw1M79Ed3rmGyLiWuD7wIOZ+TvAr4E7GqtS0tDmDXt2/U91daL6S+CrwL9U63cCNzdRoKTR6Hd+9mXVDK5HgOeBXwLvZ+an1U0OAOsaqVDSSPQV9sw8lZkbgfXAJuB3+91ARGyNiOmImJ6ZmRmsSklDW9DZ+Mx8H/gp8AfA5yPizNdt1wMHa+6zLTM7mdmZmpoaplZJQ+jnbPxURHy+Wp4Evg7soxv6P6ludjvw44ZqlDQC/fwQZi2wMyKW0X1xeCoz/zUi3gCejIi/Bv4DeKzBOiUNad6wZ+Ye4Ko51r9N9/O7pCXAb9BJhTDsUiEMu1QIwy4VwrBLhYjMbG9jETPAO9XV1cDR1jZezzrOZh1nW2p1/HZmzvnttVbDftaGI6YzszOWjVuHdRRYh2/jpUIYdqkQ4wz7tjFuezbrOJt1nO0zU8fYPrNLapdv46VCjCXsEXFDRPxXNVjl3eOooapjf0S8HhGvRsR0i9vdERFHImLvrHWXRMTzEfFmdXnxmOq4NyIOVvvk1Yi4sYU6NkTETyPijWpQ0z+r1re6T3rU0eo+aWyQ18xs9Q9YRndYqy8Cy4HXgCvbrqOqZT+wegzb/QpwNbB31rq/Be6ulu8Gvj+mOu4F/rzl/bEWuLpaXgn8N3Bl2/ukRx2t7hMggAur5QngJeBa4Cngtmr9PwB/upDHHceRfRPwVma+nZmfAE8CN42hjrHJzBeB985ZfRPdgTuhpQE8a+poXWYeysxXquVjdAdHWUfL+6RHHa3KrpEP8jqOsK8DfjXr+jgHq0zgJxGxOyK2jqmGM9Zk5qFq+V1gzRhruTMi9lRv8xv/ODFbRFxGd/yElxjjPjmnDmh5nzQxyGvpJ+iuy8yrgT8CvhMRXxl3QdB9Zaf7QjQOjwCX050j4BBwf1sbjogLgaeBuzLzw9ltbe6TOepofZ/kEIO81hlH2A8CG2Zdrx2ssmmZebC6PAI8y3hH3jkcEWsBqssj4ygiMw9X/2ingUdpaZ9ExATdgD2emc9Uq1vfJ3PVMa59Um37fRY4yGudcYT9ZeCK6szicuA2YFfbRUTE5yJi5Zll4BvA3t73atQuugN3whgH8DwTrsottLBPIiLojmG4LzMfmNXU6j6pq6PtfdLYIK9tnWE852zjjXTPdP4S+Isx1fBFuj0BrwG/aLMO4Am6bwdP0v3sdQfwm8ALwJvAvwOXjKmOfwReB/bQDdvaFuq4ju5b9D3Aq9XfjW3vkx51tLpPgN+nO4jrHrovLH8563/258BbwD8D5y/kcf0GnVSI0k/QScUw7FIhDLtUCMMuFcKwS4Uw7FIhDLtUCMMuFeJ/ARPJu4R7ONTyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "Before training:\n",
      "\n",
      "Train set: Avg. loss: 1.000010252, Accuracy: 248/512 (48%)\n",
      "\n",
      "0.0001840654 0.006584206\n",
      "l2 norm: 69.70948635068885\n",
      "l1 norm: 61.907495673039875\n",
      "Rbeta: -302.86195101590584\n",
      "Start training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f81942c2814a7bb4b2520be59e3ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.979654312, Accuracy: 296/512 (58%)\n",
      "\n",
      "0.01751268 0.023953367\n",
      "l2 norm: 4.052152596836723\n",
      "l1 norm: 1.9830687753004534\n",
      "Rbeta: -4.416206998749047\n",
      "\n",
      "Train set: Avg. loss: 0.949696481, Accuracy: 313/512 (61%)\n",
      "\n",
      "0.07686354 0.08396556\n",
      "l2 norm: 5.485869642920143\n",
      "l1 norm: 2.843089469162946\n",
      "Rbeta: -5.601995489309721\n",
      "\n",
      "Train set: Avg. loss: 0.921509624, Accuracy: 343/512 (67%)\n",
      "\n",
      "0.17338361 0.18217944\n",
      "l2 norm: 13.117978407266747\n",
      "l1 norm: 8.188865846834974\n",
      "Rbeta: -13.242020302472657\n",
      "\n",
      "Train set: Avg. loss: 0.879218638, Accuracy: 368/512 (72%)\n",
      "\n",
      "0.37385127 0.38632295\n",
      "l2 norm: 24.916054969931167\n",
      "l1 norm: 16.859316118372817\n",
      "Rbeta: -25.02611816100243\n",
      "\n",
      "Train set: Avg. loss: 0.826313257, Accuracy: 374/512 (73%)\n",
      "\n",
      "0.6928523 0.7120084\n",
      "l2 norm: 48.955896788411756\n",
      "l1 norm: 34.95340699279653\n",
      "Rbeta: -49.07386954840234\n",
      "Learning rate change\n",
      "\n",
      "Train set: Avg. loss: 0.318255424, Accuracy: 499/512 (97%)\n",
      "\n",
      "8.100851 10.339722\n",
      "l2 norm: 792.1647155740204\n",
      "l1 norm: 648.0023761379477\n",
      "Rbeta: -798.1952380184532\n",
      "\n",
      "Train set: Avg. loss: 0.118690602, Accuracy: 511/512 (100%)\n",
      "\n",
      "18.130604 21.716507\n",
      "l2 norm: 43665.92500322659\n",
      "l1 norm: 36123.12138711205\n",
      "Rbeta: -43847.13623860921\n",
      "\n",
      "Train set: Avg. loss: 0.058778226, Accuracy: 512/512 (100%)\n",
      "\n",
      "25.399109 29.37581\n",
      "l2 norm: 1967.2344405469496\n",
      "l1 norm: 1632.0360253242673\n",
      "Rbeta: 1972.549770306784\n",
      "\n",
      "Train set: Avg. loss: 0.035563584, Accuracy: 512/512 (100%)\n",
      "\n",
      "30.598892 34.725304\n",
      "l2 norm: 1433.2940816899593\n",
      "l1 norm: 1191.1538756012262\n",
      "Rbeta: 1436.2301502562477\n",
      "\n",
      "Train set: Avg. loss: 0.024627388, Accuracy: 512/512 (100%)\n",
      "\n",
      "34.5496 38.748096\n",
      "l2 norm: 1364.584450602052\n",
      "l1 norm: 1135.357357774712\n",
      "Rbeta: 1366.8866099071745\n",
      "\n",
      "Train set: Avg. loss: 0.018466510, Accuracy: 512/512 (100%)\n",
      "\n",
      "37.706688 41.94623\n",
      "l2 norm: 1315.1781215582323\n",
      "l1 norm: 1095.1144384909796\n",
      "Rbeta: 1317.096622686999\n",
      "\n",
      "Train set: Avg. loss: 0.014577951, Accuracy: 512/512 (100%)\n",
      "\n",
      "40.3244 44.589878\n",
      "l2 norm: 1277.229770337132\n",
      "l1 norm: 1064.1326153123644\n",
      "Rbeta: 1278.89132869804\n",
      "\n",
      "Train set: Avg. loss: 0.011931631, Accuracy: 512/512 (100%)\n",
      "\n",
      "42.554256 46.83747\n",
      "l2 norm: 1247.202441049869\n",
      "l1 norm: 1039.5819668065278\n",
      "Rbeta: 1248.6803670955333\n",
      "\n",
      "Train set: Avg. loss: 0.010030680, Accuracy: 512/512 (100%)\n",
      "\n",
      "44.49296 48.789066\n",
      "l2 norm: 1222.856518938739\n",
      "l1 norm: 1019.6576531549604\n",
      "Rbeta: 1224.1966849571115\n",
      "\n",
      "Train set: Avg. loss: 0.008608173, Accuracy: 512/512 (100%)\n",
      "\n",
      "46.205795 50.511578\n",
      "l2 norm: 1202.6942979253092\n",
      "l1 norm: 1003.1466534193869\n",
      "Rbeta: 1203.927129557521\n",
      "\n",
      "Train set: Avg. loss: 0.007509108, Accuracy: 512/512 (100%)\n",
      "\n",
      "47.73852 52.051807\n",
      "l2 norm: 1185.6852140243452\n",
      "l1 norm: 989.2113522587589\n",
      "Rbeta: 1186.8317447332338\n",
      "\n",
      "Train set: Avg. loss: 0.006637798, Accuracy: 512/512 (100%)\n",
      "\n",
      "49.124546 53.44375\n",
      "l2 norm: 1171.1134632256278\n",
      "l1 norm: 977.2689068387383\n",
      "Rbeta: 1172.1892813930222\n",
      "\n",
      "Train set: Avg. loss: 0.005932317, Accuracy: 512/512 (100%)\n",
      "\n",
      "50.388756 54.712776\n",
      "l2 norm: 1158.4609779973396\n",
      "l1 norm: 966.89674023683\n",
      "Rbeta: 1159.4774322763626\n",
      "\n",
      "Train set: Avg. loss: 0.005350916, Accuracy: 512/512 (100%)\n",
      "\n",
      "51.550354 55.87838\n",
      "l2 norm: 1147.3452710652625\n",
      "l1 norm: 957.7825186907853\n",
      "Rbeta: 1148.3112753563655\n",
      "\n",
      "Train set: Avg. loss: 0.004864553, Accuracy: 512/512 (100%)\n",
      "\n",
      "52.624363 56.955757\n",
      "l2 norm: 1137.4877208256703\n",
      "l1 norm: 949.6986109277182\n",
      "Rbeta: 1138.4102241153935\n",
      "\n",
      "Train set: Avg. loss: 0.004452413, Accuracy: 512/512 (100%)\n",
      "\n",
      "53.62284 57.957047\n",
      "l2 norm: 1128.6639907040408\n",
      "l1 norm: 942.4614963553915\n",
      "Rbeta: 1129.5484940332894\n",
      "\n",
      "Train set: Avg. loss: 0.004099261, Accuracy: 512/512 (100%)\n",
      "\n",
      "54.555424 58.89206\n",
      "l2 norm: 1120.7084981795404\n",
      "l1 norm: 935.9358142358475\n",
      "Rbeta: 1121.5594103031658\n",
      "\n",
      "Train set: Avg. loss: 0.003793682, Accuracy: 512/512 (100%)\n",
      "\n",
      "55.43009 59.768867\n",
      "l2 norm: 1113.4897487125365\n",
      "l1 norm: 930.0139038312971\n",
      "Rbeta: 1114.310997543315\n",
      "\n",
      "Train set: Avg. loss: 0.003526975, Accuracy: 512/512 (100%)\n",
      "\n",
      "56.253487 60.594128\n",
      "l2 norm: 1106.899873346768\n",
      "l1 norm: 924.6075170796557\n",
      "Rbeta: 1107.694393998809\n",
      "\n",
      "Train set: Avg. loss: 0.003292401, Accuracy: 512/512 (100%)\n",
      "\n",
      "57.031204 61.373466\n",
      "l2 norm: 1100.8515801430378\n",
      "l1 norm: 919.6450697000753\n",
      "Rbeta: 1101.6220975938938\n",
      "\n",
      "Train set: Avg. loss: 0.003084684, Accuracy: 512/512 (100%)\n",
      "\n",
      "57.767815 62.111607\n",
      "l2 norm: 1095.2764970464227\n",
      "l1 norm: 915.0706630842138\n",
      "Rbeta: 1096.025226787413\n",
      "\n",
      "Train set: Avg. loss: 0.002899578, Accuracy: 512/512 (100%)\n",
      "\n",
      "58.46758 62.812626\n",
      "l2 norm: 1090.1107197042938\n",
      "l1 norm: 910.8317941505636\n",
      "Rbeta: 1090.839524369021\n",
      "\n",
      "Train set: Avg. loss: 0.002733727, Accuracy: 512/512 (100%)\n",
      "\n",
      "59.13376 63.479984\n",
      "l2 norm: 1085.310432694249\n",
      "l1 norm: 906.8926830838654\n",
      "Rbeta: 1086.0209085390986\n",
      "\n",
      "Train set: Avg. loss: 0.002584348, Accuracy: 512/512 (100%)\n",
      "\n",
      "59.769547 64.11673\n",
      "l2 norm: 1080.8332189987343\n",
      "l1 norm: 903.2185543175192\n",
      "Rbeta: 1081.526994279354\n",
      "\n",
      "Train set: Avg. loss: 0.002449199, Accuracy: 512/512 (100%)\n",
      "\n",
      "60.37737 64.72547\n",
      "l2 norm: 1076.6407542975996\n",
      "l1 norm: 899.7779058739806\n",
      "Rbeta: 1077.318994932032\n",
      "\n",
      "Train set: Avg. loss: 0.002326393, Accuracy: 512/512 (100%)\n",
      "\n",
      "60.959652 65.308556\n",
      "l2 norm: 1072.7090992089602\n",
      "l1 norm: 896.5512665701305\n",
      "Rbeta: 1073.3729237219702\n",
      "\n",
      "Train set: Avg. loss: 0.002214381, Accuracy: 512/512 (100%)\n",
      "\n",
      "61.518257 65.86799\n",
      "l2 norm: 1069.008290018156\n",
      "l1 norm: 893.513949137392\n",
      "Rbeta: 1069.6587269457084\n",
      "\n",
      "Train set: Avg. loss: 0.002111834, Accuracy: 512/512 (100%)\n",
      "\n",
      "62.05513 66.40561\n",
      "l2 norm: 1065.5186175452948\n",
      "l1 norm: 890.6498778965762\n",
      "Rbeta: 1066.1565953399845\n",
      "\n",
      "Train set: Avg. loss: 0.002017641, Accuracy: 512/512 (100%)\n",
      "\n",
      "62.57184 66.92302\n",
      "l2 norm: 1062.2204843206332\n",
      "l1 norm: 887.9429390186331\n",
      "Rbeta: 1062.8467589559357\n",
      "\n",
      "Train set: Avg. loss: 0.001930843, Accuracy: 512/512 (100%)\n",
      "\n",
      "63.069904 67.42165\n",
      "l2 norm: 1059.0934187473704\n",
      "l1 norm: 885.3762928056467\n",
      "Rbeta: 1059.7087318017882\n",
      "\n",
      "Train set: Avg. loss: 0.001850653, Accuracy: 512/512 (100%)\n",
      "\n",
      "63.55034 67.90281\n",
      "l2 norm: 1056.1234763489526\n",
      "l1 norm: 882.9385562630407\n",
      "Rbeta: 1056.7284898582275\n",
      "\n",
      "Train set: Avg. loss: 0.001776335, Accuracy: 512/512 (100%)\n",
      "\n",
      "64.01476 68.36763\n",
      "l2 norm: 1053.301756238318\n",
      "l1 norm: 880.6225406401916\n",
      "Rbeta: 1053.8968942774575\n",
      "\n",
      "Train set: Avg. loss: 0.001707311, Accuracy: 512/512 (100%)\n",
      "\n",
      "64.46386 68.8172\n",
      "l2 norm: 1050.6123874607497\n",
      "l1 norm: 878.4150952798334\n",
      "Rbeta: 1051.198488889265\n",
      "\n",
      "Train set: Avg. loss: 0.001643045, Accuracy: 512/512 (100%)\n",
      "\n",
      "64.898636 69.25246\n",
      "l2 norm: 1048.0444145336123\n",
      "l1 norm: 876.3072101591799\n",
      "Rbeta: 1048.6216694217649\n",
      "\n",
      "Train set: Avg. loss: 0.001583082, Accuracy: 512/512 (100%)\n",
      "\n",
      "65.31999 69.67426\n",
      "l2 norm: 1045.5911966687731\n",
      "l1 norm: 874.2935107525561\n",
      "Rbeta: 1046.1602615276047\n",
      "\n",
      "Train set: Avg. loss: 0.001527015, Accuracy: 512/512 (100%)\n",
      "\n",
      "65.728676 70.0834\n",
      "l2 norm: 1043.2433481957023\n",
      "l1 norm: 872.3662897933056\n",
      "Rbeta: 1043.8044890654817\n",
      "\n",
      "Train set: Avg. loss: 0.001474477, Accuracy: 512/512 (100%)\n",
      "\n",
      "66.12561 70.48062\n",
      "l2 norm: 1040.9944647157113\n",
      "l1 norm: 870.5202519754437\n",
      "Rbeta: 1041.5481376460768\n",
      "\n",
      "Train set: Avg. loss: 0.001425167, Accuracy: 512/512 (100%)\n",
      "\n",
      "66.51127 70.86656\n",
      "l2 norm: 1038.835866438832\n",
      "l1 norm: 868.7483144282473\n",
      "Rbeta: 1039.3824831030615\n",
      "\n",
      "Train set: Avg. loss: 0.001378815, Accuracy: 512/512 (100%)\n",
      "\n",
      "66.88607 71.24185\n",
      "l2 norm: 1036.7612482401478\n",
      "l1 norm: 867.0452638404153\n",
      "Rbeta: 1037.3010466552287\n",
      "\n",
      "Train set: Avg. loss: 0.001335143, Accuracy: 512/512 (100%)\n",
      "\n",
      "67.25107 71.607086\n",
      "l2 norm: 1034.766527770895\n",
      "l1 norm: 865.407787623632\n",
      "Rbeta: 1035.2997541130405\n",
      "\n",
      "Train set: Avg. loss: 0.001293927, Accuracy: 512/512 (100%)\n",
      "\n",
      "67.60685 71.96276\n",
      "l2 norm: 1032.8468695941172\n",
      "l1 norm: 863.8319556836007\n",
      "Rbeta: 1033.373854985877\n",
      "\n",
      "Train set: Avg. loss: 0.001255047, Accuracy: 512/512 (100%)\n",
      "\n",
      "67.95255 72.30933\n",
      "l2 norm: 1030.9908912212115\n",
      "l1 norm: 862.3082507403888\n",
      "Rbeta: 1031.5120743956884\n",
      "\n",
      "Train set: Avg. loss: 0.001218226, Accuracy: 512/512 (100%)\n",
      "\n",
      "68.29033 72.64726\n",
      "l2 norm: 1029.2075153582468\n",
      "l1 norm: 860.8442752404803\n",
      "Rbeta: 1029.7229337086637\n",
      "\n",
      "Train set: Avg. loss: 0.001183344, Accuracy: 512/512 (100%)\n",
      "\n",
      "68.61989 72.97699\n",
      "l2 norm: 1027.483710578398\n",
      "l1 norm: 859.4292400654213\n",
      "Rbeta: 1027.9936882021177\n",
      "\n",
      "Train set: Avg. loss: 0.001150247, Accuracy: 512/512 (100%)\n",
      "\n",
      "68.94184 73.29881\n",
      "l2 norm: 1025.8148222645782\n",
      "l1 norm: 858.0591936782848\n",
      "Rbeta: 1026.3194315080061\n",
      "\n",
      "Train set: Avg. loss: 0.001118861, Accuracy: 512/512 (100%)\n",
      "\n",
      "69.25537 73.6132\n",
      "l2 norm: 1024.1990059443526\n",
      "l1 norm: 856.732610608138\n",
      "Rbeta: 1024.6985610556599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.001088952, Accuracy: 512/512 (100%)\n",
      "\n",
      "69.56312 73.92044\n",
      "l2 norm: 1022.6447081336214\n",
      "l1 norm: 855.4567230300287\n",
      "Rbeta: 1023.1392360355145\n",
      "\n",
      "Train set: Avg. loss: 0.001060548, Accuracy: 512/512 (100%)\n",
      "\n",
      "69.8626 74.220894\n",
      "l2 norm: 1021.1352591204029\n",
      "l1 norm: 854.2175852036037\n",
      "Rbeta: 1021.6253890812548\n",
      "\n",
      "Train set: Avg. loss: 0.001033421, Accuracy: 512/512 (100%)\n",
      "\n",
      "70.15687 74.51478\n",
      "l2 norm: 1019.670263423306\n",
      "l1 norm: 853.0148624374159\n",
      "Rbeta: 1020.1556409472382\n",
      "\n",
      "Train set: Avg. loss: 0.001007590, Accuracy: 512/512 (100%)\n",
      "\n",
      "70.44387 74.80241\n",
      "l2 norm: 1018.2522844512853\n",
      "l1 norm: 851.8507736059646\n",
      "Rbeta: 1018.7334164570284\n",
      "\n",
      "Train set: Avg. loss: 0.000982891, Accuracy: 512/512 (100%)\n",
      "\n",
      "70.7257 75.083984\n",
      "l2 norm: 1016.8693127560778\n",
      "l1 norm: 850.7153351751649\n",
      "Rbeta: 1017.3461121907101\n",
      "\n",
      "Train set: Avg. loss: 0.000959328, Accuracy: 512/512 (100%)\n",
      "\n",
      "71.00079 75.35978\n",
      "l2 norm: 1015.5330629943929\n",
      "l1 norm: 849.6184028633138\n",
      "Rbeta: 1016.0058497891409\n",
      "\n",
      "Train set: Avg. loss: 0.000936753, Accuracy: 512/512 (100%)\n",
      "\n",
      "71.27119 75.63002\n",
      "l2 norm: 1014.2312188423924\n",
      "l1 norm: 848.5496519972742\n",
      "Rbeta: 1014.7001017131911\n",
      "\n",
      "Train set: Avg. loss: 0.000915140, Accuracy: 512/512 (100%)\n",
      "\n",
      "71.53623 75.89497\n",
      "l2 norm: 1012.9690095646708\n",
      "l1 norm: 847.513500718198\n",
      "Rbeta: 1013.4339952890506\n",
      "\n",
      "Train set: Avg. loss: 0.000894471, Accuracy: 512/512 (100%)\n",
      "\n",
      "71.79506 76.15484\n",
      "l2 norm: 1011.7351069291501\n",
      "l1 norm: 846.500495263241\n",
      "Rbeta: 1012.1964193908257\n",
      "\n",
      "Train set: Avg. loss: 0.000874581, Accuracy: 512/512 (100%)\n",
      "\n",
      "72.050766 76.409744\n",
      "l2 norm: 1010.5397529701241\n",
      "l1 norm: 845.5191220568863\n",
      "Rbeta: 1010.9973759571759\n",
      "\n",
      "Train set: Avg. loss: 0.000855553, Accuracy: 512/512 (100%)\n",
      "\n",
      "72.3001 76.65989\n",
      "l2 norm: 1009.3711201927035\n",
      "l1 norm: 844.5596801014433\n",
      "Rbeta: 1009.8253495812621\n",
      "\n",
      "Train set: Avg. loss: 0.000837249, Accuracy: 512/512 (100%)\n",
      "\n",
      "72.54561 76.905464\n",
      "l2 norm: 1008.2320896102667\n",
      "l1 norm: 843.6245541888311\n",
      "Rbeta: 1008.6829799311205\n",
      "\n",
      "Train set: Avg. loss: 0.000819704, Accuracy: 512/512 (100%)\n",
      "\n",
      "72.78537 77.14661\n",
      "l2 norm: 1007.1182913251444\n",
      "l1 norm: 842.7100599628177\n",
      "Rbeta: 1007.5659950637925\n",
      "\n",
      "Train set: Avg. loss: 0.000802811, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.021355 77.38353\n",
      "l2 norm: 1006.0397363783006\n",
      "l1 norm: 841.824662592634\n",
      "Rbeta: 1006.4843792753273\n",
      "\n",
      "Train set: Avg. loss: 0.000786603, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.251686 77.61635\n",
      "l2 norm: 1004.9883054236673\n",
      "l1 norm: 840.9615208375358\n",
      "Rbeta: 1005.4304330162596\n",
      "\n",
      "Train set: Avg. loss: 0.000770891, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.48107 77.8452\n",
      "l2 norm: 1003.9536893539818\n",
      "l1 norm: 840.1121070633378\n",
      "Rbeta: 1004.3926400367128\n",
      "\n",
      "Train set: Avg. loss: 0.000755726, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.70739 78.070145\n",
      "l2 norm: 1002.9418651883504\n",
      "l1 norm: 839.2812974323854\n",
      "Rbeta: 1003.3775283260885\n",
      "\n",
      "Train set: Avg. loss: 0.000741117, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.92954 78.29135\n",
      "l2 norm: 1001.9576425583057\n",
      "l1 norm: 838.473182371308\n",
      "Rbeta: 1002.3902191460476\n",
      "\n",
      "Train set: Avg. loss: 0.000727014, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.14832 78.50896\n",
      "l2 norm: 1000.9970239865526\n",
      "l1 norm: 837.6844851877111\n",
      "Rbeta: 1001.4265945380786\n",
      "\n",
      "Train set: Avg. loss: 0.000713392, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.3638 78.723114\n",
      "l2 norm: 1000.0535791295015\n",
      "l1 norm: 836.9098039401606\n",
      "Rbeta: 1000.48019361628\n",
      "\n",
      "Train set: Avg. loss: 0.000700341, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.57245 78.933815\n",
      "l2 norm: 999.1375394643813\n",
      "l1 norm: 836.157774320141\n",
      "Rbeta: 999.5617628974243\n",
      "\n",
      "Train set: Avg. loss: 0.000687753, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.7769 79.141335\n",
      "l2 norm: 998.2424831742843\n",
      "l1 norm: 835.4228725000532\n",
      "Rbeta: 998.6648536684595\n",
      "\n",
      "Train set: Avg. loss: 0.000675511, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.98049 79.345726\n",
      "l2 norm: 997.3662811421004\n",
      "l1 norm: 834.7035752114103\n",
      "Rbeta: 997.7861605166115\n",
      "\n",
      "Train set: Avg. loss: 0.000663654, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.181465 79.54699\n",
      "l2 norm: 996.5011968613111\n",
      "l1 norm: 833.9933017784377\n",
      "Rbeta: 996.9185899886576\n",
      "\n",
      "Train set: Avg. loss: 0.000652145, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.38058 79.74533\n",
      "l2 norm: 995.659244763992\n",
      "l1 norm: 833.3021073142565\n",
      "Rbeta: 996.0741898131093\n",
      "\n",
      "Train set: Avg. loss: 0.000640968, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.57796 79.9407\n",
      "l2 norm: 994.8339800880259\n",
      "l1 norm: 832.624645373995\n",
      "Rbeta: 995.2460622825545\n",
      "\n",
      "Train set: Avg. loss: 0.000630146, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.77223 80.13327\n",
      "l2 norm: 994.0182732681417\n",
      "l1 norm: 831.9549039463759\n",
      "Rbeta: 994.4277773363439\n",
      "\n",
      "Train set: Avg. loss: 0.000619725, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.96133 80.32301\n",
      "l2 norm: 993.2249339061331\n",
      "l1 norm: 831.3036150250944\n",
      "Rbeta: 993.6322845909207\n",
      "\n",
      "Train set: Avg. loss: 0.000609630, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.1473 80.51015\n",
      "l2 norm: 992.452484920096\n",
      "l1 norm: 830.6695906630205\n",
      "Rbeta: 992.8577693453323\n",
      "\n",
      "Train set: Avg. loss: 0.000599792, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.33243 80.69467\n",
      "l2 norm: 991.6804469930653\n",
      "l1 norm: 830.0356347685703\n",
      "Rbeta: 992.0835307930269\n",
      "\n",
      "Train set: Avg. loss: 0.000590266, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.51415 80.87666\n",
      "l2 norm: 990.9362000710227\n",
      "l1 norm: 829.4247233135378\n",
      "Rbeta: 991.3371212370073\n",
      "\n",
      "Train set: Avg. loss: 0.000581044, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.69237 81.05624\n",
      "l2 norm: 990.1979224003154\n",
      "l1 norm: 828.8185265320474\n",
      "Rbeta: 990.5971140515566\n",
      "\n",
      "Train set: Avg. loss: 0.000572038, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.87006 81.23346\n",
      "l2 norm: 989.4717105697164\n",
      "l1 norm: 828.222260695905\n",
      "Rbeta: 989.8687110856853\n",
      "\n",
      "Train set: Avg. loss: 0.000563326, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.04391 81.408295\n",
      "l2 norm: 988.7740166076293\n",
      "l1 norm: 827.6496878865963\n",
      "Rbeta: 989.1692593231354\n",
      "\n",
      "Train set: Avg. loss: 0.000554854, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.21568 81.58087\n",
      "l2 norm: 988.0747731117108\n",
      "l1 norm: 827.0755818531841\n",
      "Rbeta: 988.4682453728697\n",
      "\n",
      "Train set: Avg. loss: 0.000546616, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.38519 81.7512\n",
      "l2 norm: 987.3863813802786\n",
      "l1 norm: 826.5103472512573\n",
      "Rbeta: 987.778025474673\n",
      "\n",
      "Train set: Avg. loss: 0.000538553, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.55459 81.919464\n",
      "l2 norm: 986.7086043695083\n",
      "l1 norm: 825.9538722470891\n",
      "Rbeta: 987.0982346068125\n",
      "\n",
      "Train set: Avg. loss: 0.000530697, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.7224 82.0856\n",
      "l2 norm: 986.0394300654805\n",
      "l1 norm: 825.4043893498388\n",
      "Rbeta: 986.4268425737728\n",
      "\n",
      "Train set: Avg. loss: 0.000523111, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.88547 82.24968\n",
      "l2 norm: 985.3897715163232\n",
      "l1 norm: 824.8710977814624\n",
      "Rbeta: 985.7755909339915\n",
      "\n",
      "Train set: Avg. loss: 0.000515715, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.046906 82.41179\n",
      "l2 norm: 984.7458917222133\n",
      "l1 norm: 824.3425694430082\n",
      "Rbeta: 985.1300739016755\n",
      "\n",
      "Train set: Avg. loss: 0.000508468, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.208405 82.57194\n",
      "l2 norm: 984.1167756393586\n",
      "l1 norm: 823.8260500487332\n",
      "Rbeta: 984.4989521584223\n",
      "\n",
      "Train set: Avg. loss: 0.000501441, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.366486 82.73007\n",
      "l2 norm: 983.5096693773448\n",
      "l1 norm: 823.32770364573\n",
      "Rbeta: 983.8901829646865\n",
      "\n",
      "Train set: Avg. loss: 0.000494585, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.52306 82.88638\n",
      "l2 norm: 982.8973468699887\n",
      "l1 norm: 822.8249146674419\n",
      "Rbeta: 983.2762436686644\n",
      "\n",
      "Train set: Avg. loss: 0.000487868, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.67937 83.0408\n",
      "l2 norm: 982.2903805607092\n",
      "l1 norm: 822.3264828120666\n",
      "Rbeta: 982.667219471428\n",
      "\n",
      "Train set: Avg. loss: 0.000481326, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.83355 83.19345\n",
      "l2 norm: 981.7047976397943\n",
      "l1 norm: 821.8457081265011\n",
      "Rbeta: 982.0798323099881\n",
      "\n",
      "Train set: Avg. loss: 0.000475000, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.98328 83.34437\n",
      "l2 norm: 981.1242648090189\n",
      "l1 norm: 821.3691510802051\n",
      "Rbeta: 981.4977842556509\n",
      "\n",
      "Train set: Avg. loss: 0.000468895, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.12802 83.493614\n",
      "l2 norm: 980.550950209221\n",
      "l1 norm: 820.8985444534208\n",
      "Rbeta: 980.9238584894442\n",
      "\n",
      "Train set: Avg. loss: 0.000462901, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.27274 83.641266\n",
      "l2 norm: 979.9954211116909\n",
      "l1 norm: 820.4425284893745\n",
      "Rbeta: 980.3671052817999\n",
      "\n",
      "Train set: Avg. loss: 0.000457014, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.41759 83.78726\n",
      "l2 norm: 979.4394263443019\n",
      "l1 norm: 819.9861010561092\n",
      "Rbeta: 979.809866381659\n",
      "\n",
      "Train set: Avg. loss: 0.000451237, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.56236 83.931625\n",
      "l2 norm: 978.8923477430807\n",
      "l1 norm: 819.5370247451859\n",
      "Rbeta: 979.2612601387793\n",
      "\n",
      "Train set: Avg. loss: 0.000445564, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.70705 84.07448\n",
      "l2 norm: 978.3546280027858\n",
      "l1 norm: 819.0955803690931\n",
      "Rbeta: 978.7217502882864\n",
      "\n",
      "Train set: Avg. loss: 0.000440037, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.84955 84.21569\n",
      "l2 norm: 977.8280229096981\n",
      "l1 norm: 818.663292314033\n",
      "Rbeta: 978.1936121484641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000434630, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.99083 84.35542\n",
      "l2 norm: 977.3092522755214\n",
      "l1 norm: 818.2375450112852\n",
      "Rbeta: 977.6731130748724\n",
      "\n",
      "Train set: Avg. loss: 0.000429337, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.13096 84.49383\n",
      "l2 norm: 976.7845488251033\n",
      "l1 norm: 817.8064974473099\n",
      "Rbeta: 977.1467513520279\n",
      "\n",
      "Train set: Avg. loss: 0.000424178, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.268776 84.63079\n",
      "l2 norm: 976.2688104610728\n",
      "l1 norm: 817.3830638914918\n",
      "Rbeta: 976.6295460767305\n",
      "\n",
      "Train set: Avg. loss: 0.000419177, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.4029 84.76621\n",
      "l2 norm: 975.7609964720027\n",
      "l1 norm: 816.965960548597\n",
      "Rbeta: 976.1205522552383\n",
      "\n",
      "Train set: Avg. loss: 0.000414277, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.536064 84.900314\n",
      "l2 norm: 975.2657592485157\n",
      "l1 norm: 816.5593538419612\n",
      "Rbeta: 975.6241665044618\n",
      "\n",
      "Train set: Avg. loss: 0.000409461, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.66911 85.033066\n",
      "l2 norm: 974.7805044943494\n",
      "l1 norm: 816.1609669720638\n",
      "Rbeta: 975.1376005098738\n",
      "\n",
      "Train set: Avg. loss: 0.000404745, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.800964 85.16453\n",
      "l2 norm: 974.2987561871041\n",
      "l1 norm: 815.7654517044202\n",
      "Rbeta: 974.6544145823688\n",
      "\n",
      "Train set: Avg. loss: 0.000400141, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.9308 85.294785\n",
      "l2 norm: 973.8285323479229\n",
      "l1 norm: 815.3796153296987\n",
      "Rbeta: 974.1831263833598\n",
      "\n",
      "Train set: Avg. loss: 0.000395629, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.05961 85.42368\n",
      "l2 norm: 973.3590622474915\n",
      "l1 norm: 814.9941616700914\n",
      "Rbeta: 973.7123972427718\n",
      "\n",
      "Train set: Avg. loss: 0.000391208, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.18738 85.55135\n",
      "l2 norm: 972.8978168464973\n",
      "l1 norm: 814.6155668597875\n",
      "Rbeta: 973.2499483787267\n",
      "\n",
      "Train set: Avg. loss: 0.000386869, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.31465 85.67768\n",
      "l2 norm: 972.4490837181777\n",
      "l1 norm: 814.2471765000164\n",
      "Rbeta: 972.7998755556852\n",
      "\n",
      "Train set: Avg. loss: 0.000382636, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.43944 85.80295\n",
      "l2 norm: 971.9905837946413\n",
      "l1 norm: 813.8706455739527\n",
      "Rbeta: 972.3401778229664\n",
      "\n",
      "Train set: Avg. loss: 0.000378503, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.5623 85.926994\n",
      "l2 norm: 971.5434059041892\n",
      "l1 norm: 813.5034522298361\n",
      "Rbeta: 971.8919451369501\n",
      "\n",
      "Train set: Avg. loss: 0.000374434, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.68509 86.0499\n",
      "l2 norm: 971.1140734309412\n",
      "l1 norm: 813.1511468117736\n",
      "Rbeta: 971.4616010240279\n",
      "\n",
      "Train set: Avg. loss: 0.000370429, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.80784 86.171616\n",
      "l2 norm: 970.6819673409613\n",
      "l1 norm: 812.7963930494172\n",
      "Rbeta: 971.0282642695898\n",
      "\n",
      "Train set: Avg. loss: 0.000366520, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.92831 86.2923\n",
      "l2 norm: 970.2588391787361\n",
      "l1 norm: 812.449205780945\n",
      "Rbeta: 970.6039991921588\n",
      "\n",
      "Train set: Avg. loss: 0.000362693, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.047485 86.41175\n",
      "l2 norm: 969.8233721636611\n",
      "l1 norm: 812.0914778523247\n",
      "Rbeta: 970.1673963520508\n",
      "\n",
      "Train set: Avg. loss: 0.000358946, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.16516 86.530136\n",
      "l2 norm: 969.4072082341928\n",
      "l1 norm: 811.7498086370755\n",
      "Rbeta: 969.7502502979248\n",
      "\n",
      "Train set: Avg. loss: 0.000355282, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.28085 86.647675\n",
      "l2 norm: 969.0007397676661\n",
      "l1 norm: 811.4163253933074\n",
      "Rbeta: 969.3430407282889\n",
      "\n",
      "Train set: Avg. loss: 0.000351692, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.39535 86.764084\n",
      "l2 norm: 968.5972309302709\n",
      "l1 norm: 811.0852057837399\n",
      "Rbeta: 968.9387256464679\n",
      "\n",
      "Train set: Avg. loss: 0.000348155, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.50972 86.879486\n",
      "l2 norm: 968.205594775291\n",
      "l1 norm: 810.763993788067\n",
      "Rbeta: 968.5462314726141\n",
      "\n",
      "Train set: Avg. loss: 0.000344667, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.62418 86.99396\n",
      "l2 norm: 967.8013204907498\n",
      "l1 norm: 810.4320867123542\n",
      "Rbeta: 968.1408528353438\n",
      "\n",
      "Train set: Avg. loss: 0.000341228, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.738716 87.10736\n",
      "l2 norm: 967.4103671999891\n",
      "l1 norm: 810.1111909677967\n",
      "Rbeta: 967.7486724864268\n",
      "\n",
      "Train set: Avg. loss: 0.000337859, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.85191 87.21981\n",
      "l2 norm: 967.0185098653891\n",
      "l1 norm: 809.7894272754186\n",
      "Rbeta: 967.3557457270738\n",
      "\n",
      "Train set: Avg. loss: 0.000334543, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.964676 87.331276\n",
      "l2 norm: 966.627707353721\n",
      "l1 norm: 809.4685762186399\n",
      "Rbeta: 966.9638975680813\n",
      "\n",
      "Train set: Avg. loss: 0.000331276, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.07737 87.44169\n",
      "l2 norm: 966.2412903235179\n",
      "l1 norm: 809.1511365314881\n",
      "Rbeta: 966.5760530745378\n",
      "\n",
      "Train set: Avg. loss: 0.000328073, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.18877 87.55122\n",
      "l2 norm: 965.8603925510766\n",
      "l1 norm: 808.838266850782\n",
      "Rbeta: 966.19389099117\n",
      "\n",
      "Train set: Avg. loss: 0.000324944, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.298035 87.65985\n",
      "l2 norm: 965.4733302382771\n",
      "l1 norm: 808.5202962963049\n",
      "Rbeta: 965.8057823473425\n",
      "\n",
      "Train set: Avg. loss: 0.000321883, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.405365 87.76766\n",
      "l2 norm: 965.1126960537123\n",
      "l1 norm: 808.2243541733168\n",
      "Rbeta: 965.4443749672224\n",
      "\n",
      "Train set: Avg. loss: 0.000318903, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.51006 87.874466\n",
      "l2 norm: 964.750006754653\n",
      "l1 norm: 807.9266357433672\n",
      "Rbeta: 965.080933354624\n",
      "\n",
      "Train set: Avg. loss: 0.000315972, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.613945 87.98051\n",
      "l2 norm: 964.3890265896928\n",
      "l1 norm: 807.630344883042\n",
      "Rbeta: 964.7193658584522\n",
      "\n",
      "Train set: Avg. loss: 0.000313081, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.71769 88.08578\n",
      "l2 norm: 964.0241799731095\n",
      "l1 norm: 807.3307362733533\n",
      "Rbeta: 964.353893495242\n",
      "\n",
      "Train set: Avg. loss: 0.000310233, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.82104 88.190155\n",
      "l2 norm: 963.6647191350623\n",
      "l1 norm: 807.0354560101194\n",
      "Rbeta: 963.9937527965354\n",
      "\n",
      "Train set: Avg. loss: 0.000307437, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.92325 88.293785\n",
      "l2 norm: 963.330221242484\n",
      "l1 norm: 806.7610688689684\n",
      "Rbeta: 963.658550747936\n",
      "\n",
      "Train set: Avg. loss: 0.000304680, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.02539 88.396515\n",
      "l2 norm: 962.9944860289904\n",
      "l1 norm: 806.4855637173221\n",
      "Rbeta: 963.3219727596656\n",
      "\n",
      "Train set: Avg. loss: 0.000301956, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.12759 88.498505\n",
      "l2 norm: 962.647330744891\n",
      "l1 norm: 806.2004293754289\n",
      "Rbeta: 962.973957394288\n",
      "\n",
      "Train set: Avg. loss: 0.000299267, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.22985 88.59966\n",
      "l2 norm: 962.3120525186895\n",
      "l1 norm: 805.9251558609919\n",
      "Rbeta: 962.6376208599288\n",
      "\n",
      "Train set: Avg. loss: 0.000296625, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.33106 88.700066\n",
      "l2 norm: 961.9859761587895\n",
      "l1 norm: 805.6575096468019\n",
      "Rbeta: 962.3105578902617\n",
      "\n",
      "Train set: Avg. loss: 0.000294028, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.43138 88.799706\n",
      "l2 norm: 961.657281890937\n",
      "l1 norm: 805.3877714004232\n",
      "Rbeta: 961.9809729467439\n",
      "\n",
      "Train set: Avg. loss: 0.000291487, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.5299 88.898544\n",
      "l2 norm: 961.3400280599969\n",
      "l1 norm: 805.127449386928\n",
      "Rbeta: 961.6628596769131\n",
      "\n",
      "Train set: Avg. loss: 0.000288992, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.62717 88.99679\n",
      "l2 norm: 961.0279428732672\n",
      "l1 norm: 804.8715186512479\n",
      "Rbeta: 961.3502145353341\n",
      "\n",
      "Train set: Avg. loss: 0.000286527, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.72449 89.09427\n",
      "l2 norm: 960.710820282134\n",
      "l1 norm: 804.6112076521108\n",
      "Rbeta: 961.0322880004218\n",
      "\n",
      "Train set: Avg. loss: 0.000284093, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.82184 89.19092\n",
      "l2 norm: 960.3842912940958\n",
      "l1 norm: 804.3430229984256\n",
      "Rbeta: 960.7048164160781\n",
      "\n",
      "Train set: Avg. loss: 0.000281691, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.919 89.28686\n",
      "l2 norm: 960.0736353616363\n",
      "l1 norm: 804.0881884791328\n",
      "Rbeta: 960.3932148875789\n",
      "\n",
      "Train set: Avg. loss: 0.000279317, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.01618 89.38211\n",
      "l2 norm: 959.7773504121267\n",
      "l1 norm: 803.8452444587958\n",
      "Rbeta: 960.0958206519434\n",
      "\n",
      "Train set: Avg. loss: 0.000276973, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.11328 89.47668\n",
      "l2 norm: 959.4798686530952\n",
      "l1 norm: 803.6012313239808\n",
      "Rbeta: 959.7972780387737\n",
      "\n",
      "Train set: Avg. loss: 0.000274655, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.21043 89.57067\n",
      "l2 norm: 959.1633284191937\n",
      "l1 norm: 803.3413831957403\n",
      "Rbeta: 959.4795276976669\n",
      "\n",
      "Train set: Avg. loss: 0.000272381, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.3062 89.66399\n",
      "l2 norm: 958.8513319070331\n",
      "l1 norm: 803.085177049769\n",
      "Rbeta: 959.1664374544803\n",
      "\n",
      "Train set: Avg. loss: 0.000270169, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.39911 89.75653\n",
      "l2 norm: 958.5552090836834\n",
      "l1 norm: 802.8422413170383\n",
      "Rbeta: 958.8694660438384\n",
      "\n",
      "Train set: Avg. loss: 0.000267992, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.49115 89.84842\n",
      "l2 norm: 958.2590577899125\n",
      "l1 norm: 802.5993152164542\n",
      "Rbeta: 958.5726169202475\n",
      "\n",
      "Train set: Avg. loss: 0.000265841, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.58324 89.93959\n",
      "l2 norm: 957.9751272906731\n",
      "l1 norm: 802.3663550995775\n",
      "Rbeta: 958.2877979879039\n",
      "\n",
      "Train set: Avg. loss: 0.000263717, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.67509 90.030205\n",
      "l2 norm: 957.6760525652604\n",
      "l1 norm: 802.1205797502125\n",
      "Rbeta: 957.987818651014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000261616, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.76697 90.120255\n",
      "l2 norm: 957.3872184635813\n",
      "l1 norm: 801.8834219385842\n",
      "Rbeta: 957.6980180547413\n",
      "\n",
      "Train set: Avg. loss: 0.000259540, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.85862 90.20974\n",
      "l2 norm: 957.1008339284726\n",
      "l1 norm: 801.6482737348169\n",
      "Rbeta: 957.4106499463026\n",
      "\n",
      "Train set: Avg. loss: 0.000257518, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.94777 90.298454\n",
      "l2 norm: 956.7994108603599\n",
      "l1 norm: 801.4004211375627\n",
      "Rbeta: 957.1083974511168\n",
      "\n",
      "Train set: Avg. loss: 0.000255540, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.03498 90.38663\n",
      "l2 norm: 956.5130732872801\n",
      "l1 norm: 801.1651744068242\n",
      "Rbeta: 956.8215299959625\n",
      "\n",
      "Train set: Avg. loss: 0.000253591, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.12168 90.47406\n",
      "l2 norm: 956.2342208373946\n",
      "l1 norm: 800.936194411157\n",
      "Rbeta: 956.5421372811938\n",
      "\n",
      "Train set: Avg. loss: 0.000251679, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.206825 90.56103\n",
      "l2 norm: 955.9671847235707\n",
      "l1 norm: 800.7171007838451\n",
      "Rbeta: 956.2746852566447\n",
      "\n",
      "Train set: Avg. loss: 0.000249788, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.291855 90.647446\n",
      "l2 norm: 955.697095085628\n",
      "l1 norm: 800.4954759135275\n",
      "Rbeta: 956.0040442314873\n",
      "\n",
      "Train set: Avg. loss: 0.000247936, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.375275 90.7333\n",
      "l2 norm: 955.4308058167354\n",
      "l1 norm: 800.2767869663943\n",
      "Rbeta: 955.7375272635808\n",
      "\n",
      "Train set: Avg. loss: 0.000246104, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.45859 90.81858\n",
      "l2 norm: 955.1475230754664\n",
      "l1 norm: 800.0438995432489\n",
      "Rbeta: 955.4537651493333\n",
      "\n",
      "Train set: Avg. loss: 0.000244303, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.54086 90.90335\n",
      "l2 norm: 954.8679789610272\n",
      "l1 norm: 799.8142345824951\n",
      "Rbeta: 955.1739122797181\n",
      "\n",
      "Train set: Avg. loss: 0.000242531, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.62214 90.98762\n",
      "l2 norm: 954.609376037692\n",
      "l1 norm: 799.6019236450984\n",
      "Rbeta: 954.9150862494408\n",
      "\n",
      "Train set: Avg. loss: 0.000240778, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.703445 91.071304\n",
      "l2 norm: 954.3564328580625\n",
      "l1 norm: 799.394108555719\n",
      "Rbeta: 954.6618005855238\n",
      "\n",
      "Train set: Avg. loss: 0.000239041, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.7848 91.15453\n",
      "l2 norm: 954.0756398361979\n",
      "l1 norm: 799.1631023695636\n",
      "Rbeta: 954.380668952454\n",
      "\n",
      "Train set: Avg. loss: 0.000237324, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.86609 91.23717\n",
      "l2 norm: 953.8221241018236\n",
      "l1 norm: 798.9549002136151\n",
      "Rbeta: 954.1267155000096\n",
      "\n",
      "Train set: Avg. loss: 0.000235626, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.947365 91.31923\n",
      "l2 norm: 953.5719650881857\n",
      "l1 norm: 798.7495854404152\n",
      "Rbeta: 953.8760414819502\n",
      "\n",
      "Train set: Avg. loss: 0.000233945, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.02868 91.40079\n",
      "l2 norm: 953.3340462849991\n",
      "l1 norm: 798.5545826363136\n",
      "Rbeta: 953.6375559060115\n",
      "\n",
      "Train set: Avg. loss: 0.000232280, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.11002 91.48187\n",
      "l2 norm: 953.075503567612\n",
      "l1 norm: 798.3422942598528\n",
      "Rbeta: 953.378220518503\n",
      "\n",
      "Train set: Avg. loss: 0.000230631, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.191414 91.56252\n",
      "l2 norm: 952.8366273447205\n",
      "l1 norm: 798.1463542589014\n",
      "Rbeta: 953.1387487348795\n",
      "\n",
      "Train set: Avg. loss: 0.000229017, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.271194 91.64248\n",
      "l2 norm: 952.6071488451943\n",
      "l1 norm: 797.958284923023\n",
      "Rbeta: 952.9086531906312\n",
      "\n",
      "Train set: Avg. loss: 0.000227420, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.350815 91.72194\n",
      "l2 norm: 952.3584221414454\n",
      "l1 norm: 797.7541148321642\n",
      "Rbeta: 952.6592811745737\n",
      "\n",
      "Train set: Avg. loss: 0.000225855, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.42881 91.80105\n",
      "l2 norm: 952.1105103506153\n",
      "l1 norm: 797.5507648932818\n",
      "Rbeta: 952.4109516973357\n",
      "\n",
      "Train set: Avg. loss: 0.000224312, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.50611 91.87965\n",
      "l2 norm: 951.869440862453\n",
      "l1 norm: 797.3529082146994\n",
      "Rbeta: 952.16951538784\n",
      "\n",
      "Train set: Avg. loss: 0.000222794, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.582344 91.95784\n",
      "l2 norm: 951.6396285066072\n",
      "l1 norm: 797.1644613121657\n",
      "Rbeta: 951.939430402504\n",
      "\n",
      "Train set: Avg. loss: 0.000221291, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.658615 92.035614\n",
      "l2 norm: 951.4015884649867\n",
      "l1 norm: 796.9689704800466\n",
      "Rbeta: 951.7009908235102\n",
      "\n",
      "Train set: Avg. loss: 0.000219803, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.7348 92.112816\n",
      "l2 norm: 951.1748949458298\n",
      "l1 norm: 796.7828097828684\n",
      "Rbeta: 951.4738647149529\n",
      "\n",
      "Train set: Avg. loss: 0.000218330, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.810974 92.18963\n",
      "l2 norm: 950.9412486673181\n",
      "l1 norm: 796.5911102776096\n",
      "Rbeta: 951.2396632258263\n",
      "\n",
      "Train set: Avg. loss: 0.000216875, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.886536 92.26609\n",
      "l2 norm: 950.7074848005783\n",
      "l1 norm: 796.3992749525677\n",
      "Rbeta: 951.0053825739964\n",
      "\n",
      "Train set: Avg. loss: 0.000215446, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.96095 92.34206\n",
      "l2 norm: 950.4900420098172\n",
      "l1 norm: 796.2209251987782\n",
      "Rbeta: 950.7876623508777\n",
      "\n",
      "Train set: Avg. loss: 0.000214031, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.03538 92.41753\n",
      "l2 norm: 950.260694430871\n",
      "l1 norm: 796.0327192897767\n",
      "Rbeta: 950.5579087268982\n",
      "\n",
      "Train set: Avg. loss: 0.000212630, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.1097 92.492615\n",
      "l2 norm: 950.0150740104061\n",
      "l1 norm: 795.8307555203262\n",
      "Rbeta: 950.3118458088248\n",
      "\n",
      "Train set: Avg. loss: 0.000211243, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.18406 92.56711\n",
      "l2 norm: 949.815001675025\n",
      "l1 norm: 795.6670167701818\n",
      "Rbeta: 950.1111928848685\n",
      "\n",
      "Train set: Avg. loss: 0.000209868, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.25844 92.64122\n",
      "l2 norm: 949.6111250202359\n",
      "l1 norm: 795.4998151285205\n",
      "Rbeta: 949.9068134461435\n",
      "\n",
      "Train set: Avg. loss: 0.000208508, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.3327 92.714905\n",
      "l2 norm: 949.4011493841814\n",
      "l1 norm: 795.3275527103324\n",
      "Rbeta: 949.6961895918844\n",
      "\n",
      "Train set: Avg. loss: 0.000207158, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.407 92.78833\n",
      "l2 norm: 949.1662110880897\n",
      "l1 norm: 795.1343542902955\n",
      "Rbeta: 949.4606346461338\n",
      "\n",
      "Train set: Avg. loss: 0.000205820, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.48132 92.861374\n",
      "l2 norm: 948.9425166500788\n",
      "l1 norm: 794.950522578388\n",
      "Rbeta: 949.2362556276969\n",
      "\n",
      "Train set: Avg. loss: 0.000204505, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.5546 92.93392\n",
      "l2 norm: 948.7193401419602\n",
      "l1 norm: 794.7671374280958\n",
      "Rbeta: 949.0124960894447\n",
      "\n",
      "Train set: Avg. loss: 0.000203208, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.62726 93.00604\n",
      "l2 norm: 948.504265521162\n",
      "l1 norm: 794.5906345888297\n",
      "Rbeta: 948.7966993730995\n",
      "\n",
      "Train set: Avg. loss: 0.000201935, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.69832 93.07783\n",
      "l2 norm: 948.2891385289582\n",
      "l1 norm: 794.4139596280299\n",
      "Rbeta: 948.5812691999821\n",
      "\n",
      "Train set: Avg. loss: 0.000200676, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.76924 93.14917\n",
      "l2 norm: 948.068901007516\n",
      "l1 norm: 794.2329865972894\n",
      "Rbeta: 948.3605205211675\n",
      "\n",
      "Train set: Avg. loss: 0.000199428, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.84019 93.220184\n",
      "l2 norm: 947.856390631794\n",
      "l1 norm: 794.0585773291812\n",
      "Rbeta: 948.1474594440898\n",
      "\n",
      "Train set: Avg. loss: 0.000198191, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.91116 93.29081\n",
      "l2 norm: 947.6523053306222\n",
      "l1 norm: 793.8912010678237\n",
      "Rbeta: 947.9428475916136\n",
      "\n",
      "Train set: Avg. loss: 0.000196964, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.98217 93.36108\n",
      "l2 norm: 947.4386617672517\n",
      "l1 norm: 793.7158098394538\n",
      "Rbeta: 947.7286818229238\n",
      "\n",
      "Train set: Avg. loss: 0.000195748, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.05312 93.43094\n",
      "l2 norm: 947.2244524826519\n",
      "l1 norm: 793.5398572472243\n",
      "Rbeta: 947.513813626891\n",
      "\n",
      "Train set: Avg. loss: 0.000194544, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.12397 93.50042\n",
      "l2 norm: 947.0153345178126\n",
      "l1 norm: 793.3682260371811\n",
      "Rbeta: 947.3040109548483\n",
      "\n",
      "Train set: Avg. loss: 0.000193353, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.19464 93.56943\n",
      "l2 norm: 946.8140562420606\n",
      "l1 norm: 793.2029629206827\n",
      "Rbeta: 947.102052767989\n",
      "\n",
      "Train set: Avg. loss: 0.000192172, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.2653 93.63817\n",
      "l2 norm: 946.6083317567717\n",
      "l1 norm: 793.0339188606166\n",
      "Rbeta: 946.8956337996334\n",
      "\n",
      "Train set: Avg. loss: 0.000191001, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.335976 93.70655\n",
      "l2 norm: 946.3981290725172\n",
      "l1 norm: 792.8610086287015\n",
      "Rbeta: 946.6845936834856\n",
      "\n",
      "Train set: Avg. loss: 0.000189859, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.40431 93.774536\n",
      "l2 norm: 946.2032760076113\n",
      "l1 norm: 792.7009691811105\n",
      "Rbeta: 946.4892229840071\n",
      "\n",
      "Train set: Avg. loss: 0.000188736, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.47156 93.842255\n",
      "l2 norm: 946.0126431350379\n",
      "l1 norm: 792.5446041534987\n",
      "Rbeta: 946.2982043768703\n",
      "\n",
      "Train set: Avg. loss: 0.000187624, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.53877 93.90947\n",
      "l2 norm: 945.8139496409847\n",
      "l1 norm: 792.3815906366998\n",
      "Rbeta: 946.0990455015042\n",
      "\n",
      "Train set: Avg. loss: 0.000186521, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.60602 93.97645\n",
      "l2 norm: 945.627155928381\n",
      "l1 norm: 792.2285656174345\n",
      "Rbeta: 945.9117253185194\n",
      "\n",
      "Train set: Avg. loss: 0.000185426, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.67329 94.04301\n",
      "l2 norm: 945.4505623518212\n",
      "l1 norm: 792.0840502849032\n",
      "Rbeta: 945.734613910048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000184340, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.74058 94.10937\n",
      "l2 norm: 945.2663739630634\n",
      "l1 norm: 791.9331304161888\n",
      "Rbeta: 945.5498556878514\n",
      "\n",
      "Train set: Avg. loss: 0.000183262, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.8079 94.17548\n",
      "l2 norm: 945.0900826490246\n",
      "l1 norm: 791.7887351380394\n",
      "Rbeta: 945.3730263566927\n",
      "\n",
      "Train set: Avg. loss: 0.000182194, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.87521 94.24123\n",
      "l2 norm: 944.9121258087694\n",
      "l1 norm: 791.6428631844125\n",
      "Rbeta: 945.1943440623977\n",
      "\n",
      "Train set: Avg. loss: 0.000181137, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.942276 94.30652\n",
      "l2 norm: 944.7320826002205\n",
      "l1 norm: 791.4952320218712\n",
      "Rbeta: 945.013687218257\n",
      "\n",
      "Train set: Avg. loss: 0.000180100, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.00782 94.37152\n",
      "l2 norm: 944.5313252103109\n",
      "l1 norm: 791.3301345051291\n",
      "Rbeta: 944.8123664215632\n",
      "\n",
      "Train set: Avg. loss: 0.000179072, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.07338 94.43615\n",
      "l2 norm: 944.3412398876167\n",
      "l1 norm: 791.174137955171\n",
      "Rbeta: 944.6217259059836\n",
      "\n",
      "Train set: Avg. loss: 0.000178054, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.13881 94.500496\n",
      "l2 norm: 944.1530047534045\n",
      "l1 norm: 791.0195652485315\n",
      "Rbeta: 944.4330009240873\n",
      "\n",
      "Train set: Avg. loss: 0.000177043, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.20426 94.564606\n",
      "l2 norm: 943.9568232019094\n",
      "l1 norm: 790.8584700113266\n",
      "Rbeta: 944.2361063104657\n",
      "\n",
      "Train set: Avg. loss: 0.000176041, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.269745 94.6283\n",
      "l2 norm: 943.7834470500602\n",
      "l1 norm: 790.716277923301\n",
      "Rbeta: 944.0621203546874\n",
      "\n",
      "Train set: Avg. loss: 0.000175047, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.33524 94.69158\n",
      "l2 norm: 943.605745842671\n",
      "l1 norm: 790.5705111926657\n",
      "Rbeta: 943.8837627035722\n",
      "\n",
      "Train set: Avg. loss: 0.000174064, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.4003 94.754684\n",
      "l2 norm: 943.4252714909859\n",
      "l1 norm: 790.4222469138934\n",
      "Rbeta: 943.7026751230899\n",
      "\n",
      "Train set: Avg. loss: 0.000173100, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.46399 94.81734\n",
      "l2 norm: 943.2305630402145\n",
      "l1 norm: 790.2621418671765\n",
      "Rbeta: 943.5073627454641\n",
      "\n",
      "Train set: Avg. loss: 0.000172155, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.5261 94.87978\n",
      "l2 norm: 943.0432982414737\n",
      "l1 norm: 790.1082230448692\n",
      "Rbeta: 943.3197629414427\n",
      "\n",
      "Train set: Avg. loss: 0.000171218, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.58798 94.941986\n",
      "l2 norm: 942.841589304948\n",
      "l1 norm: 789.9424018192453\n",
      "Rbeta: 943.1176542682632\n",
      "\n",
      "Train set: Avg. loss: 0.000170303, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.64801 95.003845\n",
      "l2 norm: 942.6586461554747\n",
      "l1 norm: 789.7920339645293\n",
      "Rbeta: 942.9345006786936\n",
      "\n",
      "Train set: Avg. loss: 0.000169397, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.70791 95.06538\n",
      "l2 norm: 942.4809974895342\n",
      "l1 norm: 789.6458503471534\n",
      "Rbeta: 942.7566783204594\n",
      "\n",
      "Train set: Avg. loss: 0.000168497, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.76777 95.12657\n",
      "l2 norm: 942.30661247974\n",
      "l1 norm: 789.5025396591193\n",
      "Rbeta: 942.5820699368119\n",
      "\n",
      "Train set: Avg. loss: 0.000167606, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.82755 95.18748\n",
      "l2 norm: 942.1212469943664\n",
      "l1 norm: 789.3502019706418\n",
      "Rbeta: 942.3964296118816\n",
      "\n",
      "Train set: Avg. loss: 0.000166721, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.88735 95.24808\n",
      "l2 norm: 941.9620857211293\n",
      "l1 norm: 789.2198070467925\n",
      "Rbeta: 942.2370043300544\n",
      "\n",
      "Train set: Avg. loss: 0.000165843, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.94717 95.30839\n",
      "l2 norm: 941.7993032364938\n",
      "l1 norm: 789.0863109521019\n",
      "Rbeta: 942.0738164665995\n",
      "\n",
      "Train set: Avg. loss: 0.000164972, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.00701 95.36841\n",
      "l2 norm: 941.6293777021581\n",
      "l1 norm: 788.9469084165321\n",
      "Rbeta: 941.9035612889622\n",
      "\n",
      "Train set: Avg. loss: 0.000164108, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.06687 95.42798\n",
      "l2 norm: 941.4638392989942\n",
      "l1 norm: 788.8111355751136\n",
      "Rbeta: 941.7375106576488\n",
      "\n",
      "Train set: Avg. loss: 0.000163251, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.126656 95.4873\n",
      "l2 norm: 941.3090683452119\n",
      "l1 norm: 788.6844810854255\n",
      "Rbeta: 941.5823674873669\n",
      "\n",
      "Train set: Avg. loss: 0.000162400, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.1864 95.54642\n",
      "l2 norm: 941.146971451473\n",
      "l1 norm: 788.5514576055182\n",
      "Rbeta: 941.4198064787547\n",
      "\n",
      "Train set: Avg. loss: 0.000161555, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.24616 95.60538\n",
      "l2 norm: 940.9683198880346\n",
      "l1 norm: 788.404630899819\n",
      "Rbeta: 941.2406307127397\n",
      "\n",
      "Train set: Avg. loss: 0.000160715, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.30595 95.66417\n",
      "l2 norm: 940.7858771415238\n",
      "l1 norm: 788.2547225556375\n",
      "Rbeta: 941.0577482869135\n",
      "\n",
      "Train set: Avg. loss: 0.000159882, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.36575 95.72268\n",
      "l2 norm: 940.6129429849311\n",
      "l1 norm: 788.1125489265112\n",
      "Rbeta: 940.8843636232833\n",
      "\n",
      "Train set: Avg. loss: 0.000159054, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.42557 95.78087\n",
      "l2 norm: 940.4413855781291\n",
      "l1 norm: 787.9714942650858\n",
      "Rbeta: 940.7121170563746\n",
      "\n",
      "Train set: Avg. loss: 0.000158233, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.485405 95.838776\n",
      "l2 norm: 940.2824368467341\n",
      "l1 norm: 787.8409079910458\n",
      "Rbeta: 940.5525763471006\n",
      "\n",
      "Train set: Avg. loss: 0.000157419, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.545166 95.8965\n",
      "l2 norm: 940.1347971283056\n",
      "l1 norm: 787.7200320239497\n",
      "Rbeta: 940.4043558696002\n",
      "\n",
      "Train set: Avg. loss: 0.000156621, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.603226 95.95404\n",
      "l2 norm: 939.9773423046674\n",
      "l1 norm: 787.5907575838514\n",
      "Rbeta: 940.2464672466722\n",
      "\n",
      "Train set: Avg. loss: 0.000155829, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.661255 96.01135\n",
      "l2 norm: 939.8196936742744\n",
      "l1 norm: 787.4613562976733\n",
      "Rbeta: 940.088368711531\n",
      "\n",
      "Train set: Avg. loss: 0.000155043, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.7193 96.06835\n",
      "l2 norm: 939.6669507391667\n",
      "l1 norm: 787.3359765755059\n",
      "Rbeta: 939.9351654270904\n",
      "\n",
      "Train set: Avg. loss: 0.000154263, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.77737 96.125\n",
      "l2 norm: 939.5044012415044\n",
      "l1 norm: 787.2022403486434\n",
      "Rbeta: 939.7721160203832\n",
      "\n",
      "Train set: Avg. loss: 0.000153489, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.83545 96.18137\n",
      "l2 norm: 939.3457812552326\n",
      "l1 norm: 787.0718625767614\n",
      "Rbeta: 939.6128474177019\n",
      "\n",
      "Train set: Avg. loss: 0.000152720, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.893555 96.23766\n",
      "l2 norm: 939.1992068548576\n",
      "l1 norm: 786.95157561167\n",
      "Rbeta: 939.4658377420923\n",
      "\n",
      "Train set: Avg. loss: 0.000151956, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.95164 96.29358\n",
      "l2 norm: 939.0593105815447\n",
      "l1 norm: 786.8372329083908\n",
      "Rbeta: 939.325162276802\n",
      "\n",
      "Train set: Avg. loss: 0.000151200, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.00962 96.34917\n",
      "l2 norm: 938.8998189115338\n",
      "l1 norm: 786.7065674338202\n",
      "Rbeta: 939.1651366017744\n",
      "\n",
      "Train set: Avg. loss: 0.000150448, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.06762 96.40457\n",
      "l2 norm: 938.7584920994136\n",
      "l1 norm: 786.5910737705007\n",
      "Rbeta: 939.0231988894004\n",
      "\n",
      "Train set: Avg. loss: 0.000149713, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.12397 96.45975\n",
      "l2 norm: 938.6024802217202\n",
      "l1 norm: 786.4629985479861\n",
      "Rbeta: 938.8666220860438\n",
      "\n",
      "Train set: Avg. loss: 0.000148990, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.179184 96.514656\n",
      "l2 norm: 938.4437069930758\n",
      "l1 norm: 786.3327478116718\n",
      "Rbeta: 938.7075562556042\n",
      "\n",
      "Train set: Avg. loss: 0.000148277, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.233795 96.56923\n",
      "l2 norm: 938.294200813403\n",
      "l1 norm: 786.2101184109588\n",
      "Rbeta: 938.557647114772\n",
      "\n",
      "Train set: Avg. loss: 0.000147568, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.28842 96.623634\n",
      "l2 norm: 938.1619161201228\n",
      "l1 norm: 786.1018341092379\n",
      "Rbeta: 938.4249835042276\n",
      "\n",
      "Train set: Avg. loss: 0.000146868, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.342445 96.67784\n",
      "l2 norm: 938.0120145160444\n",
      "l1 norm: 785.9788395530311\n",
      "Rbeta: 938.2748362245557\n",
      "\n",
      "Train set: Avg. loss: 0.000146180, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.39528 96.731834\n",
      "l2 norm: 937.856934757209\n",
      "l1 norm: 785.8514325703701\n",
      "Rbeta: 938.1194583077654\n",
      "\n",
      "Train set: Avg. loss: 0.000145496, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.44807 96.785736\n",
      "l2 norm: 937.7210815112467\n",
      "l1 norm: 785.7399835998925\n",
      "Rbeta: 937.9834466623252\n",
      "\n",
      "Train set: Avg. loss: 0.000144816, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.500885 96.83948\n",
      "l2 norm: 937.573395183604\n",
      "l1 norm: 785.6184122486093\n",
      "Rbeta: 937.8356016140635\n",
      "\n",
      "Train set: Avg. loss: 0.000144141, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.5537 96.892944\n",
      "l2 norm: 937.4351016052655\n",
      "l1 norm: 785.5050411177303\n",
      "Rbeta: 937.6970530901066\n",
      "\n",
      "Train set: Avg. loss: 0.000143471, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.606544 96.946144\n",
      "l2 norm: 937.2927389561781\n",
      "l1 norm: 785.3883291475724\n",
      "Rbeta: 937.5543773354257\n",
      "\n",
      "Train set: Avg. loss: 0.000142805, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.65939 96.99917\n",
      "l2 norm: 937.1615578255847\n",
      "l1 norm: 785.2809695466033\n",
      "Rbeta: 937.4228874057386\n",
      "\n",
      "Train set: Avg. loss: 0.000142144, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.71218 97.052\n",
      "l2 norm: 937.0276708152154\n",
      "l1 norm: 785.1712042407003\n",
      "Rbeta: 937.2887131380552\n",
      "\n",
      "Train set: Avg. loss: 0.000141488, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.764915 97.10451\n",
      "l2 norm: 936.871339307525\n",
      "l1 norm: 785.0425118947796\n",
      "Rbeta: 937.1319530004295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000140836, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.81765 97.15675\n",
      "l2 norm: 936.723073941299\n",
      "l1 norm: 784.9206304922611\n",
      "Rbeta: 936.9833046164465\n",
      "\n",
      "Train set: Avg. loss: 0.000140189, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.870415 97.20874\n",
      "l2 norm: 936.5729378908924\n",
      "l1 norm: 784.7971175747646\n",
      "Rbeta: 936.8328513788764\n",
      "\n",
      "Train set: Avg. loss: 0.000139546, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.92319 97.260635\n",
      "l2 norm: 936.4193026721373\n",
      "l1 norm: 784.6707914303041\n",
      "Rbeta: 936.6787863985327\n",
      "\n",
      "Train set: Avg. loss: 0.000138907, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.97597 97.312355\n",
      "l2 norm: 936.2717378573935\n",
      "l1 norm: 784.5496765423346\n",
      "Rbeta: 936.5307840597454\n",
      "\n",
      "Train set: Avg. loss: 0.000138271, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.02878 97.36389\n",
      "l2 norm: 936.1377138628286\n",
      "l1 norm: 784.4398718604474\n",
      "Rbeta: 936.3963021500631\n",
      "\n",
      "Train set: Avg. loss: 0.000137640, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.0816 97.41521\n",
      "l2 norm: 935.9959418212652\n",
      "l1 norm: 784.3234480150852\n",
      "Rbeta: 936.2540307954065\n",
      "\n",
      "Train set: Avg. loss: 0.000137012, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.134415 97.466354\n",
      "l2 norm: 935.8585637539439\n",
      "l1 norm: 784.2107505819794\n",
      "Rbeta: 936.1161413312037\n",
      "\n",
      "Train set: Avg. loss: 0.000136390, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.1871 97.517265\n",
      "l2 norm: 935.7138343740982\n",
      "l1 norm: 784.0919220348999\n",
      "Rbeta: 935.9709638794943\n",
      "\n",
      "Train set: Avg. loss: 0.000135772, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.239815 97.56789\n",
      "l2 norm: 935.5810205072112\n",
      "l1 norm: 783.9830264223874\n",
      "Rbeta: 935.8376368596802\n",
      "\n",
      "Train set: Avg. loss: 0.000135158, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.29251 97.61836\n",
      "l2 norm: 935.4468614658626\n",
      "l1 norm: 783.8729602050449\n",
      "Rbeta: 935.7028493636514\n",
      "\n",
      "Train set: Avg. loss: 0.000134548, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.345085 97.66867\n",
      "l2 norm: 935.3149367237743\n",
      "l1 norm: 783.7647472930862\n",
      "Rbeta: 935.5704516396675\n",
      "\n",
      "Train set: Avg. loss: 0.000133942, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.397675 97.718864\n",
      "l2 norm: 935.185235230188\n",
      "l1 norm: 783.6583744570553\n",
      "Rbeta: 935.4401672260805\n",
      "\n",
      "Train set: Avg. loss: 0.000133346, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.44913 97.768906\n",
      "l2 norm: 935.0629837308901\n",
      "l1 norm: 783.558307609101\n",
      "Rbeta: 935.31741757442\n",
      "\n",
      "Train set: Avg. loss: 0.000132758, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.50001 97.81875\n",
      "l2 norm: 934.9269317254395\n",
      "l1 norm: 783.4465096818168\n",
      "Rbeta: 935.1809514555625\n",
      "\n",
      "Train set: Avg. loss: 0.000132173, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.5509 97.86835\n",
      "l2 norm: 934.7939536881631\n",
      "l1 norm: 783.3373479947709\n",
      "Rbeta: 935.0474982172054\n",
      "\n",
      "Train set: Avg. loss: 0.000131600, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.60042 97.91761\n",
      "l2 norm: 934.6615552719544\n",
      "l1 norm: 783.2288263136645\n",
      "Rbeta: 934.9148574087586\n",
      "\n",
      "Train set: Avg. loss: 0.000131034, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.649445 97.96672\n",
      "l2 norm: 934.5402980192309\n",
      "l1 norm: 783.1295659550862\n",
      "Rbeta: 934.793256478418\n",
      "\n",
      "Train set: Avg. loss: 0.000130471, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.6985 98.01562\n",
      "l2 norm: 934.4029411389639\n",
      "l1 norm: 783.0167416634514\n",
      "Rbeta: 934.6555732773854\n",
      "\n",
      "Train set: Avg. loss: 0.000129920, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.74614 98.064285\n",
      "l2 norm: 934.2779732661123\n",
      "l1 norm: 782.9144270509817\n",
      "Rbeta: 934.5304704807348\n",
      "\n",
      "Train set: Avg. loss: 0.000129374, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.79349 98.112854\n",
      "l2 norm: 934.1571780631945\n",
      "l1 norm: 782.8156151562703\n",
      "Rbeta: 934.40956102339\n",
      "\n",
      "Train set: Avg. loss: 0.000128830, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.840836 98.16126\n",
      "l2 norm: 934.0270118742529\n",
      "l1 norm: 782.7088775199488\n",
      "Rbeta: 934.2792543715204\n",
      "\n",
      "Train set: Avg. loss: 0.000128290, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.8882 98.20956\n",
      "l2 norm: 933.8746815382348\n",
      "l1 norm: 782.583464648359\n",
      "Rbeta: 934.1266566912188\n",
      "\n",
      "Train set: Avg. loss: 0.000127752, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.93558 98.25769\n",
      "l2 norm: 933.7539260793952\n",
      "l1 norm: 782.4845185846464\n",
      "Rbeta: 934.0057936742022\n",
      "\n",
      "Train set: Avg. loss: 0.000127219, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.98288 98.30554\n",
      "l2 norm: 933.6233235341557\n",
      "l1 norm: 782.3773654905021\n",
      "Rbeta: 933.8749228902765\n",
      "\n",
      "Train set: Avg. loss: 0.000126690, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.03013 98.35307\n",
      "l2 norm: 933.5065025180372\n",
      "l1 norm: 782.2817852142855\n",
      "Rbeta: 933.7579732853868\n",
      "\n",
      "Train set: Avg. loss: 0.000126170, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.07647 98.40042\n",
      "l2 norm: 933.39982184062\n",
      "l1 norm: 782.1946186237597\n",
      "Rbeta: 933.6510905228195\n",
      "\n",
      "Train set: Avg. loss: 0.000125657, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.121994 98.44756\n",
      "l2 norm: 933.2842186681673\n",
      "l1 norm: 782.1000211552077\n",
      "Rbeta: 933.5353273605367\n",
      "\n",
      "Train set: Avg. loss: 0.000125146, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.167534 98.49468\n",
      "l2 norm: 933.1594590174793\n",
      "l1 norm: 781.9978152454851\n",
      "Rbeta: 933.4104787837931\n",
      "\n",
      "Train set: Avg. loss: 0.000124638, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.21308 98.54171\n",
      "l2 norm: 933.0231870859835\n",
      "l1 norm: 781.8855978110314\n",
      "Rbeta: 933.2740954213236\n",
      "\n",
      "Train set: Avg. loss: 0.000124133, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.258644 98.588615\n",
      "l2 norm: 932.8921936410452\n",
      "l1 norm: 781.7777492610646\n",
      "Rbeta: 933.1430176652967\n",
      "\n",
      "Train set: Avg. loss: 0.000123630, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.304214 98.6354\n",
      "l2 norm: 932.7782066077543\n",
      "l1 norm: 781.6842149236982\n",
      "Rbeta: 933.028858054127\n",
      "\n",
      "Train set: Avg. loss: 0.000123130, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.34979 98.68203\n",
      "l2 norm: 932.6740446373925\n",
      "l1 norm: 781.5990630515412\n",
      "Rbeta: 932.9245922588158\n",
      "\n",
      "Train set: Avg. loss: 0.000122633, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.395386 98.72849\n",
      "l2 norm: 932.5630464219907\n",
      "l1 norm: 781.5082146095933\n",
      "Rbeta: 932.813388071414\n",
      "\n",
      "Train set: Avg. loss: 0.000122139, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.440994 98.77464\n",
      "l2 norm: 932.4654318082561\n",
      "l1 norm: 781.4285846974308\n",
      "Rbeta: 932.715616471465\n",
      "\n",
      "Train set: Avg. loss: 0.000121649, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.4865 98.82065\n",
      "l2 norm: 932.3568485807463\n",
      "l1 norm: 781.339818873027\n",
      "Rbeta: 932.6068574651725\n",
      "\n",
      "Train set: Avg. loss: 0.000121162, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.53197 98.86652\n",
      "l2 norm: 932.2395234739229\n",
      "l1 norm: 781.2436761656345\n",
      "Rbeta: 932.4893041539132\n",
      "\n",
      "Train set: Avg. loss: 0.000120677, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.577446 98.91221\n",
      "l2 norm: 932.1336735126347\n",
      "l1 norm: 781.1571254992506\n",
      "Rbeta: 932.3831938430646\n",
      "\n",
      "Train set: Avg. loss: 0.000120195, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.62293 98.95775\n",
      "l2 norm: 932.0220226288701\n",
      "l1 norm: 781.0656112103661\n",
      "Rbeta: 932.2713343599744\n",
      "\n",
      "Train set: Avg. loss: 0.000119716, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.668434 99.00315\n",
      "l2 norm: 931.8997305703022\n",
      "l1 norm: 780.9652662350242\n",
      "Rbeta: 932.1487171469267\n",
      "\n",
      "Train set: Avg. loss: 0.000119239, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.713936 99.048416\n",
      "l2 norm: 931.7833175121577\n",
      "l1 norm: 780.8698340758001\n",
      "Rbeta: 932.0320064865073\n",
      "\n",
      "Train set: Avg. loss: 0.000118770, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.75856 99.09354\n",
      "l2 norm: 931.6571258502007\n",
      "l1 norm: 780.7660892281151\n",
      "Rbeta: 931.905639746655\n",
      "\n",
      "Train set: Avg. loss: 0.000118308, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.80234 99.138466\n",
      "l2 norm: 931.5171141358785\n",
      "l1 norm: 780.6509345157222\n",
      "Rbeta: 931.7653670111953\n",
      "\n",
      "Train set: Avg. loss: 0.000117848, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.84611 99.18325\n",
      "l2 norm: 931.3872265981214\n",
      "l1 norm: 780.5442270406338\n",
      "Rbeta: 931.6354702465758\n",
      "\n",
      "Train set: Avg. loss: 0.000117392, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.889755 99.22786\n",
      "l2 norm: 931.2789221898332\n",
      "l1 norm: 780.4554353259989\n",
      "Rbeta: 931.5269775371858\n",
      "\n",
      "Train set: Avg. loss: 0.000116947, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.93185 99.272285\n",
      "l2 norm: 931.1823141542103\n",
      "l1 norm: 780.3764168718722\n",
      "Rbeta: 931.4304354792671\n",
      "\n",
      "Train set: Avg. loss: 0.000116505, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.97376 99.31646\n",
      "l2 norm: 931.088311887568\n",
      "l1 norm: 780.2994084967726\n",
      "Rbeta: 931.3364085029884\n",
      "\n",
      "Train set: Avg. loss: 0.000116066, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.01569 99.36045\n",
      "l2 norm: 930.9795573076163\n",
      "l1 norm: 780.2101408913843\n",
      "Rbeta: 931.2275857303116\n",
      "\n",
      "Train set: Avg. loss: 0.000115629, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.05763 99.40431\n",
      "l2 norm: 930.8640191000827\n",
      "l1 norm: 780.1152413273608\n",
      "Rbeta: 931.1119887662552\n",
      "\n",
      "Train set: Avg. loss: 0.000115194, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.09958 99.448074\n",
      "l2 norm: 930.7640654216366\n",
      "l1 norm: 780.0332754144061\n",
      "Rbeta: 931.0120177030932\n",
      "\n",
      "Train set: Avg. loss: 0.000114761, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.14155 99.49179\n",
      "l2 norm: 930.6729229038652\n",
      "l1 norm: 779.9589315114225\n",
      "Rbeta: 930.9208343721149\n",
      "\n",
      "Train set: Avg. loss: 0.000114331, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.18349 99.535385\n",
      "l2 norm: 930.5753011699687\n",
      "l1 norm: 779.8792292326244\n",
      "Rbeta: 930.8231251454037\n",
      "\n",
      "Train set: Avg. loss: 0.000113903, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.22531 99.57884\n",
      "l2 norm: 930.4681186791009\n",
      "l1 norm: 779.7913939194874\n",
      "Rbeta: 930.7159339406026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000113477, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.267136 99.622246\n",
      "l2 norm: 930.3631221931539\n",
      "l1 norm: 779.7052482920368\n",
      "Rbeta: 930.6108211185427\n",
      "\n",
      "Train set: Avg. loss: 0.000113054, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.30887 99.66548\n",
      "l2 norm: 930.2445933106857\n",
      "l1 norm: 779.6077786872733\n",
      "Rbeta: 930.4922184168307\n",
      "\n",
      "Train set: Avg. loss: 0.000112634, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.35056 99.708565\n",
      "l2 norm: 930.1308766536924\n",
      "l1 norm: 779.514414945332\n",
      "Rbeta: 930.3784899153177\n",
      "\n",
      "Train set: Avg. loss: 0.000112216, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.39226 99.751526\n",
      "l2 norm: 930.0185659361508\n",
      "l1 norm: 779.4221471772503\n",
      "Rbeta: 930.2660902694721\n",
      "\n",
      "Train set: Avg. loss: 0.000111800, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.43396 99.79432\n",
      "l2 norm: 929.8959057551489\n",
      "l1 norm: 779.3212584023604\n",
      "Rbeta: 930.1432421787033\n",
      "\n",
      "Train set: Avg. loss: 0.000111386, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.47567 99.83693\n",
      "l2 norm: 929.7928102448191\n",
      "l1 norm: 779.236900568652\n",
      "Rbeta: 930.0399927227129\n",
      "\n",
      "Train set: Avg. loss: 0.000110975, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.51739 99.8794\n",
      "l2 norm: 929.6971894951507\n",
      "l1 norm: 779.1588567229671\n",
      "Rbeta: 929.9442542213002\n",
      "\n",
      "Train set: Avg. loss: 0.000110569, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.55859 99.921646\n",
      "l2 norm: 929.5981550765399\n",
      "l1 norm: 779.0777242030008\n",
      "Rbeta: 929.8450613677787\n",
      "\n",
      "Train set: Avg. loss: 0.000110179, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.59683 99.96369\n",
      "l2 norm: 929.4956909161474\n",
      "l1 norm: 778.9935711811283\n",
      "Rbeta: 929.7427411513461\n",
      "\n",
      "Train set: Avg. loss: 0.000109792, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.63508 100.005646\n",
      "l2 norm: 929.387508074906\n",
      "l1 norm: 778.9046505014257\n",
      "Rbeta: 929.6347672455288\n",
      "\n",
      "Train set: Avg. loss: 0.000109406, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.673325 100.04755\n",
      "l2 norm: 929.2631369316341\n",
      "l1 norm: 778.8021933880095\n",
      "Rbeta: 929.5105731731861\n",
      "\n",
      "Train set: Avg. loss: 0.000109022, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.71159 100.0893\n",
      "l2 norm: 929.1464336504832\n",
      "l1 norm: 778.7063827659115\n",
      "Rbeta: 929.3940857327282\n",
      "\n",
      "Train set: Avg. loss: 0.000108640, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.749855 100.13089\n",
      "l2 norm: 929.0293802328235\n",
      "l1 norm: 778.6102827984582\n",
      "Rbeta: 929.2770443505208\n",
      "\n",
      "Train set: Avg. loss: 0.000108260, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.78813 100.17228\n",
      "l2 norm: 928.9232113043429\n",
      "l1 norm: 778.5231983903858\n",
      "Rbeta: 929.1710900456521\n",
      "\n",
      "Train set: Avg. loss: 0.000107882, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.826416 100.21352\n",
      "l2 norm: 928.8338275917822\n",
      "l1 norm: 778.4500930157542\n",
      "Rbeta: 929.0817718009785\n",
      "\n",
      "Train set: Avg. loss: 0.000107507, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.8647 100.25459\n",
      "l2 norm: 928.7463195688946\n",
      "l1 norm: 778.3785321200968\n",
      "Rbeta: 928.9943121139084\n",
      "\n",
      "Train set: Avg. loss: 0.000107133, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.903 100.29559\n",
      "l2 norm: 928.6415656011737\n",
      "l1 norm: 778.2925684292927\n",
      "Rbeta: 928.8896120589549\n",
      "\n",
      "Train set: Avg. loss: 0.000106761, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.94131 100.33646\n",
      "l2 norm: 928.5421857466389\n",
      "l1 norm: 778.2110721862164\n",
      "Rbeta: 928.7903367286557\n",
      "\n",
      "Train set: Avg. loss: 0.000106391, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.97962 100.37714\n",
      "l2 norm: 928.4501502394739\n",
      "l1 norm: 778.1357723088756\n",
      "Rbeta: 928.6982750060134\n",
      "\n",
      "Train set: Avg. loss: 0.000106022, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.01794 100.41779\n",
      "l2 norm: 928.3442586847686\n",
      "l1 norm: 778.0486991265523\n",
      "Rbeta: 928.5924382279272\n",
      "\n",
      "Train set: Avg. loss: 0.000105655, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.05627 100.45835\n",
      "l2 norm: 928.2321320614012\n",
      "l1 norm: 777.9563624650416\n",
      "Rbeta: 928.4803294461352\n",
      "\n",
      "Train set: Avg. loss: 0.000105290, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.094604 100.49884\n",
      "l2 norm: 928.1297284858942\n",
      "l1 norm: 777.8720991812058\n",
      "Rbeta: 928.3779138994115\n",
      "\n",
      "Train set: Avg. loss: 0.000104930, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.1322 100.53919\n",
      "l2 norm: 928.0278247907792\n",
      "l1 norm: 777.7883775407568\n",
      "Rbeta: 928.275995982045\n",
      "\n",
      "Train set: Avg. loss: 0.000104576, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.16881 100.57938\n",
      "l2 norm: 927.9316136292531\n",
      "l1 norm: 777.7091981141928\n",
      "Rbeta: 928.1800702977814\n",
      "\n",
      "Train set: Avg. loss: 0.000104225, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.20541 100.619354\n",
      "l2 norm: 927.8370589989521\n",
      "l1 norm: 777.6314564724139\n",
      "Rbeta: 928.0856447551799\n",
      "\n",
      "Train set: Avg. loss: 0.000103876, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.24185 100.65927\n",
      "l2 norm: 927.741450650469\n",
      "l1 norm: 777.5527946578108\n",
      "Rbeta: 927.9901679669414\n",
      "\n",
      "Train set: Avg. loss: 0.000103528, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.278305 100.69913\n",
      "l2 norm: 927.6585254311215\n",
      "l1 norm: 777.484684107322\n",
      "Rbeta: 927.9074005542624\n",
      "\n",
      "Train set: Avg. loss: 0.000103182, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.31477 100.73872\n",
      "l2 norm: 927.5739106945219\n",
      "l1 norm: 777.4153510112028\n",
      "Rbeta: 927.8229114798054\n",
      "\n",
      "Train set: Avg. loss: 0.000102839, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.351234 100.77816\n",
      "l2 norm: 927.4902266115645\n",
      "l1 norm: 777.3469600104174\n",
      "Rbeta: 927.739339570237\n",
      "\n",
      "Train set: Avg. loss: 0.000102497, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.38771 100.817474\n",
      "l2 norm: 927.3827535849927\n",
      "l1 norm: 777.2586615937629\n",
      "Rbeta: 927.6319364571978\n",
      "\n",
      "Train set: Avg. loss: 0.000102156, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.42419 100.85672\n",
      "l2 norm: 927.2652877784727\n",
      "l1 norm: 777.1619503498523\n",
      "Rbeta: 927.5145417330814\n",
      "\n",
      "Train set: Avg. loss: 0.000101817, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.46068 100.895966\n",
      "l2 norm: 927.1548957648622\n",
      "l1 norm: 777.071110021256\n",
      "Rbeta: 927.4042152126617\n",
      "\n",
      "Train set: Avg. loss: 0.000101479, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.49718 100.93518\n",
      "l2 norm: 927.0577238247529\n",
      "l1 norm: 776.9914609007175\n",
      "Rbeta: 927.3071674170345\n",
      "\n",
      "Train set: Avg. loss: 0.000101142, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.533676 100.97425\n",
      "l2 norm: 926.9565348203\n",
      "l1 norm: 776.9084898489813\n",
      "Rbeta: 927.2060670723432\n",
      "\n",
      "Train set: Avg. loss: 0.000100813, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.56878 101.01319\n",
      "l2 norm: 926.8618072726808\n",
      "l1 norm: 776.8309552043656\n",
      "Rbeta: 927.1114475998028\n",
      "\n",
      "Train set: Avg. loss: 0.000100488, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.60354 101.052055\n",
      "l2 norm: 926.7656990403862\n",
      "l1 norm: 776.7522290919206\n",
      "Rbeta: 927.0156339215465\n",
      "\n",
      "Train set: Avg. loss: 0.000100164, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.63829 101.090836\n",
      "l2 norm: 926.6634242458356\n",
      "l1 norm: 776.6682867130505\n",
      "Rbeta: 926.9135253161259\n",
      "\n",
      "Train set: Avg. loss: 0.000099842, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.67296 101.129456\n",
      "l2 norm: 926.557438924\n",
      "l1 norm: 776.5812650721115\n",
      "Rbeta: 926.807782777957\n",
      "\n",
      "Train set: Avg. loss: 0.000099522, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.707504 101.16797\n",
      "l2 norm: 926.4548619003023\n",
      "l1 norm: 776.4971387149903\n",
      "Rbeta: 926.7054037095996\n",
      "\n",
      "Train set: Avg. loss: 0.000099205, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.741615 101.206345\n",
      "l2 norm: 926.3705524162915\n",
      "l1 norm: 776.4282780265482\n",
      "Rbeta: 926.6214215660972\n",
      "\n",
      "Train set: Avg. loss: 0.000098897, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.77419 101.244606\n",
      "l2 norm: 926.2872503343092\n",
      "l1 norm: 776.3603004752498\n",
      "Rbeta: 926.5384550488432\n",
      "\n",
      "Train set: Avg. loss: 0.000098590, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.80674 101.2828\n",
      "l2 norm: 926.1871433325945\n",
      "l1 norm: 776.2781814539858\n",
      "Rbeta: 926.438799476009\n",
      "\n",
      "Train set: Avg. loss: 0.000098284, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.839294 101.32088\n",
      "l2 norm: 926.0900305278384\n",
      "l1 norm: 776.198503896296\n",
      "Rbeta: 926.3420280575327\n",
      "\n",
      "Train set: Avg. loss: 0.000097980, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.87185 101.35889\n",
      "l2 norm: 925.9892372607877\n",
      "l1 norm: 776.1156515426379\n",
      "Rbeta: 926.2416425794432\n",
      "\n",
      "Train set: Avg. loss: 0.000097677, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.9044 101.39679\n",
      "l2 norm: 925.8813170034574\n",
      "l1 norm: 776.026784779281\n",
      "Rbeta: 926.1340125658006\n",
      "\n",
      "Train set: Avg. loss: 0.000097375, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.93698 101.43462\n",
      "l2 norm: 925.7706435808282\n",
      "l1 norm: 775.9355386093498\n",
      "Rbeta: 926.0237649289015\n",
      "\n",
      "Train set: Avg. loss: 0.000097074, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.96955 101.47235\n",
      "l2 norm: 925.6794403134458\n",
      "l1 norm: 775.8604618678511\n",
      "Rbeta: 925.9328093625022\n",
      "\n",
      "Train set: Avg. loss: 0.000096775, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.00214 101.50999\n",
      "l2 norm: 925.6039857116446\n",
      "l1 norm: 775.7985896248617\n",
      "Rbeta: 925.8577376080846\n",
      "\n",
      "Train set: Avg. loss: 0.000096478, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.03472 101.54746\n",
      "l2 norm: 925.5294238433287\n",
      "l1 norm: 775.737555434988\n",
      "Rbeta: 925.7835632562712\n",
      "\n",
      "Train set: Avg. loss: 0.000096182, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.06731 101.584816\n",
      "l2 norm: 925.440884044262\n",
      "l1 norm: 775.6648378241662\n",
      "Rbeta: 925.6953213820507\n",
      "\n",
      "Train set: Avg. loss: 0.000095887, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.09991 101.62203\n",
      "l2 norm: 925.3636316785585\n",
      "l1 norm: 775.6015853085364\n",
      "Rbeta: 925.6183396760198\n",
      "\n",
      "Train set: Avg. loss: 0.000095593, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.13251 101.659134\n",
      "l2 norm: 925.2908378371765\n",
      "l1 norm: 775.5421669260412\n",
      "Rbeta: 925.5458718460548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000095301, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.165115 101.69611\n",
      "l2 norm: 925.2162476316145\n",
      "l1 norm: 775.4812706340558\n",
      "Rbeta: 925.4715571146187\n",
      "\n",
      "Train set: Avg. loss: 0.000095011, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.19772 101.73304\n",
      "l2 norm: 925.1341491738027\n",
      "l1 norm: 775.413999981101\n",
      "Rbeta: 925.3896387269593\n",
      "\n",
      "Train set: Avg. loss: 0.000094721, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.23035 101.76994\n",
      "l2 norm: 925.0473020263127\n",
      "l1 norm: 775.3427666109659\n",
      "Rbeta: 925.3031187507647\n",
      "\n",
      "Train set: Avg. loss: 0.000094432, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.26297 101.80676\n",
      "l2 norm: 924.9574500226763\n",
      "l1 norm: 775.2689061122323\n",
      "Rbeta: 925.2135586459084\n",
      "\n",
      "Train set: Avg. loss: 0.000094145, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.2956 101.84353\n",
      "l2 norm: 924.8642343728058\n",
      "l1 norm: 775.1922175392914\n",
      "Rbeta: 925.1205355010485\n",
      "\n",
      "Train set: Avg. loss: 0.000093858, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.32823 101.8802\n",
      "l2 norm: 924.7742603882729\n",
      "l1 norm: 775.1181432969329\n",
      "Rbeta: 925.0307550811244\n",
      "\n",
      "Train set: Avg. loss: 0.000093574, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.36087 101.91674\n",
      "l2 norm: 924.6944500784601\n",
      "l1 norm: 775.0526659336135\n",
      "Rbeta: 924.9512353322067\n",
      "\n",
      "Train set: Avg. loss: 0.000093290, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.39352 101.953094\n",
      "l2 norm: 924.6311544287623\n",
      "l1 norm: 775.0011019132155\n",
      "Rbeta: 924.8881351287242\n",
      "\n",
      "Train set: Avg. loss: 0.000093008, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.42617 101.98932\n",
      "l2 norm: 924.5457301683111\n",
      "l1 norm: 774.9311436507479\n",
      "Rbeta: 924.8029194437146\n",
      "\n",
      "Train set: Avg. loss: 0.000092728, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.45883 102.02536\n",
      "l2 norm: 924.4570808882688\n",
      "l1 norm: 774.8585156846959\n",
      "Rbeta: 924.7144383891991\n",
      "\n",
      "Train set: Avg. loss: 0.000092449, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.491486 102.06132\n",
      "l2 norm: 924.3684803095457\n",
      "l1 norm: 774.7860226665493\n",
      "Rbeta: 924.6259301573239\n",
      "\n",
      "Train set: Avg. loss: 0.000092171, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.524155 102.09726\n",
      "l2 norm: 924.2829597034529\n",
      "l1 norm: 774.7162162025377\n",
      "Rbeta: 924.5405692504808\n",
      "\n",
      "Train set: Avg. loss: 0.000091893, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.55682 102.13315\n",
      "l2 norm: 924.197197589987\n",
      "l1 norm: 774.6460866426635\n",
      "Rbeta: 924.4549364193367\n",
      "\n",
      "Train set: Avg. loss: 0.000091617, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.5895 102.16899\n",
      "l2 norm: 924.1001933532897\n",
      "l1 norm: 774.5664837893135\n",
      "Rbeta: 924.3580121810152\n",
      "\n",
      "Train set: Avg. loss: 0.000091342, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.622185 102.2048\n",
      "l2 norm: 923.9980193235323\n",
      "l1 norm: 774.4825901868394\n",
      "Rbeta: 924.2560785468913\n",
      "\n",
      "Train set: Avg. loss: 0.000091068, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.65484 102.24051\n",
      "l2 norm: 923.9038718124503\n",
      "l1 norm: 774.405469392882\n",
      "Rbeta: 924.1620630850351\n",
      "\n",
      "Train set: Avg. loss: 0.000090796, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.68737 102.27609\n",
      "l2 norm: 923.8169417808491\n",
      "l1 norm: 774.3343096186932\n",
      "Rbeta: 924.075275039919\n",
      "\n",
      "Train set: Avg. loss: 0.000090525, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.71991 102.31157\n",
      "l2 norm: 923.7409931831546\n",
      "l1 norm: 774.2723395880744\n",
      "Rbeta: 923.9993898688493\n",
      "\n",
      "Train set: Avg. loss: 0.000090255, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.75245 102.346855\n",
      "l2 norm: 923.679212402007\n",
      "l1 norm: 774.2221571406852\n",
      "Rbeta: 923.9377258165725\n",
      "\n",
      "Train set: Avg. loss: 0.000089987, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.785 102.38201\n",
      "l2 norm: 923.6057565334268\n",
      "l1 norm: 774.1620657450824\n",
      "Rbeta: 923.8644356888229\n",
      "\n",
      "Train set: Avg. loss: 0.000089720, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.81756 102.41706\n",
      "l2 norm: 923.5290184568464\n",
      "l1 norm: 774.0991156113041\n",
      "Rbeta: 923.7877222157676\n",
      "\n",
      "Train set: Avg. loss: 0.000089455, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.85011 102.451996\n",
      "l2 norm: 923.4606432069293\n",
      "l1 norm: 774.0433071377181\n",
      "Rbeta: 923.7194038601606\n",
      "\n",
      "Train set: Avg. loss: 0.000089190, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.88268 102.486824\n",
      "l2 norm: 923.3947788171307\n",
      "l1 norm: 773.9894804079336\n",
      "Rbeta: 923.6535746646368\n",
      "\n",
      "Train set: Avg. loss: 0.000088926, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.91526 102.52168\n",
      "l2 norm: 923.3194492668312\n",
      "l1 norm: 773.9276505861319\n",
      "Rbeta: 923.5783709116706\n",
      "\n",
      "Train set: Avg. loss: 0.000088664, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.94783 102.55644\n",
      "l2 norm: 923.2363369391015\n",
      "l1 norm: 773.8592670178044\n",
      "Rbeta: 923.4952519891132\n",
      "\n",
      "Train set: Avg. loss: 0.000088402, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.980415 102.59105\n",
      "l2 norm: 923.1621493108576\n",
      "l1 norm: 773.7984118764306\n",
      "Rbeta: 923.4210809470618\n",
      "\n",
      "Train set: Avg. loss: 0.000088143, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.01289 102.625534\n",
      "l2 norm: 923.0805696450399\n",
      "l1 norm: 773.7314431870724\n",
      "Rbeta: 923.3395160781538\n",
      "\n",
      "Train set: Avg. loss: 0.000087884, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.04533 102.65998\n",
      "l2 norm: 923.001224310153\n",
      "l1 norm: 773.666323294291\n",
      "Rbeta: 923.2602005095199\n",
      "\n",
      "Train set: Avg. loss: 0.000087626, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.07776 102.69439\n",
      "l2 norm: 922.9335534930577\n",
      "l1 norm: 773.6108810800082\n",
      "Rbeta: 923.1925765226802\n",
      "\n",
      "Train set: Avg. loss: 0.000087370, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.11021 102.72868\n",
      "l2 norm: 922.8570497026741\n",
      "l1 norm: 773.5480321254006\n",
      "Rbeta: 923.116107579793\n",
      "\n",
      "Train set: Avg. loss: 0.000087115, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.14265 102.76283\n",
      "l2 norm: 922.7749306082401\n",
      "l1 norm: 773.4805930980517\n",
      "Rbeta: 923.033945643265\n",
      "\n",
      "Train set: Avg. loss: 0.000086861, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.1751 102.79692\n",
      "l2 norm: 922.6849749189282\n",
      "l1 norm: 773.4065814369978\n",
      "Rbeta: 922.9439301620349\n",
      "\n",
      "Train set: Avg. loss: 0.000086607, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.207565 102.83089\n",
      "l2 norm: 922.5929644217023\n",
      "l1 norm: 773.3309625876606\n",
      "Rbeta: 922.8519302615704\n",
      "\n",
      "Train set: Avg. loss: 0.000086355, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.24002 102.86478\n",
      "l2 norm: 922.5127607376405\n",
      "l1 norm: 773.2652972463494\n",
      "Rbeta: 922.7717032206276\n",
      "\n",
      "Train set: Avg. loss: 0.000086104, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.2725 102.898575\n",
      "l2 norm: 922.4433443578498\n",
      "l1 norm: 773.2086857081246\n",
      "Rbeta: 922.7022990594636\n",
      "\n",
      "Train set: Avg. loss: 0.000085854, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.30496 102.932335\n",
      "l2 norm: 922.3758397067999\n",
      "l1 norm: 773.153549320983\n",
      "Rbeta: 922.6346882809714\n",
      "\n",
      "Train set: Avg. loss: 0.000085604, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.33745 102.966095\n",
      "l2 norm: 922.2893731888731\n",
      "l1 norm: 773.0825068018969\n",
      "Rbeta: 922.5481751485361\n",
      "\n",
      "Train set: Avg. loss: 0.000085356, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.369934 102.9997\n",
      "l2 norm: 922.2056184860336\n",
      "l1 norm: 773.0136716496658\n",
      "Rbeta: 922.4643105078313\n",
      "\n",
      "Train set: Avg. loss: 0.000085109, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.402374 103.0332\n",
      "l2 norm: 922.1187685048263\n",
      "l1 norm: 772.9422231585845\n",
      "Rbeta: 922.3773698326436\n",
      "\n",
      "Train set: Avg. loss: 0.000084864, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.434715 103.06666\n",
      "l2 norm: 922.045758868581\n",
      "l1 norm: 772.882232867361\n",
      "Rbeta: 922.3043350361627\n",
      "\n",
      "Train set: Avg. loss: 0.000084619, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.46706 103.099976\n",
      "l2 norm: 921.9628923050113\n",
      "l1 norm: 772.8138758809164\n",
      "Rbeta: 922.2214257065473\n",
      "\n",
      "Train set: Avg. loss: 0.000084376, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.4994 103.13322\n",
      "l2 norm: 921.8956012822096\n",
      "l1 norm: 772.7585757458824\n",
      "Rbeta: 922.1540100062575\n",
      "\n",
      "Train set: Avg. loss: 0.000084133, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.531746 103.16643\n",
      "l2 norm: 921.8235673318646\n",
      "l1 norm: 772.6993540997241\n",
      "Rbeta: 922.0819299682272\n",
      "\n",
      "Train set: Avg. loss: 0.000083892, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.5641 103.19948\n",
      "l2 norm: 921.7405039601371\n",
      "l1 norm: 772.6309583954462\n",
      "Rbeta: 921.9987190191877\n",
      "\n",
      "Train set: Avg. loss: 0.000083651, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.59646 103.232376\n",
      "l2 norm: 921.6655492009672\n",
      "l1 norm: 772.5693585578379\n",
      "Rbeta: 921.9236024364452\n",
      "\n",
      "Train set: Avg. loss: 0.000083413, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.62882 103.265076\n",
      "l2 norm: 921.5980138525816\n",
      "l1 norm: 772.5139466721897\n",
      "Rbeta: 921.8559271402263\n",
      "\n",
      "Train set: Avg. loss: 0.000083175, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.661194 103.29762\n",
      "l2 norm: 921.5299761998024\n",
      "l1 norm: 772.4582281719336\n",
      "Rbeta: 921.7877412647059\n",
      "\n",
      "Train set: Avg. loss: 0.000082938, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.69356 103.330154\n",
      "l2 norm: 921.4662155397966\n",
      "l1 norm: 772.4061507587556\n",
      "Rbeta: 921.7238565305279\n",
      "\n",
      "Train set: Avg. loss: 0.000082703, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.72578 103.36266\n",
      "l2 norm: 921.4005956829378\n",
      "l1 norm: 772.3525198341183\n",
      "Rbeta: 921.6580661085674\n",
      "\n",
      "Train set: Avg. loss: 0.000082468, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.757996 103.39521\n",
      "l2 norm: 921.3268986577878\n",
      "l1 norm: 772.2918781094611\n",
      "Rbeta: 921.5842023819819\n",
      "\n",
      "Train set: Avg. loss: 0.000082233, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.79022 103.42772\n",
      "l2 norm: 921.2503215714335\n",
      "l1 norm: 772.2287539397425\n",
      "Rbeta: 921.5074498150957\n",
      "\n",
      "Train set: Avg. loss: 0.000082000, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.82246 103.460205\n",
      "l2 norm: 921.1852976354953\n",
      "l1 norm: 772.1751955514964\n",
      "Rbeta: 921.442253498182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000081768, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.8547 103.49258\n",
      "l2 norm: 921.1140698676624\n",
      "l1 norm: 772.1164688732532\n",
      "Rbeta: 921.3709397018973\n",
      "\n",
      "Train set: Avg. loss: 0.000081536, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.88694 103.52482\n",
      "l2 norm: 921.0466379025481\n",
      "l1 norm: 772.0609567653869\n",
      "Rbeta: 921.3033064949276\n",
      "\n",
      "Train set: Avg. loss: 0.000081306, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.91919 103.556946\n",
      "l2 norm: 920.979645406665\n",
      "l1 norm: 772.0058758634045\n",
      "Rbeta: 921.2361147213575\n",
      "\n",
      "Train set: Avg. loss: 0.000081077, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.95145 103.58899\n",
      "l2 norm: 920.9016506175207\n",
      "l1 norm: 771.9416704248501\n",
      "Rbeta: 921.1579532194577\n",
      "\n",
      "Train set: Avg. loss: 0.000080849, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.9837 103.62091\n",
      "l2 norm: 920.8302793919848\n",
      "l1 norm: 771.883016040149\n",
      "Rbeta: 921.0862881590326\n",
      "\n",
      "Train set: Avg. loss: 0.000080622, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.015976 103.652725\n",
      "l2 norm: 920.7593948900728\n",
      "l1 norm: 771.824714182418\n",
      "Rbeta: 921.0152614697357\n",
      "\n",
      "Train set: Avg. loss: 0.000080395, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.04825 103.68439\n",
      "l2 norm: 920.6966950178055\n",
      "l1 norm: 771.7733494903523\n",
      "Rbeta: 920.9523250715058\n",
      "\n",
      "Train set: Avg. loss: 0.000080170, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.08052 103.716064\n",
      "l2 norm: 920.6439830154192\n",
      "l1 norm: 771.7303911076096\n",
      "Rbeta: 920.8993517038884\n",
      "\n",
      "Train set: Avg. loss: 0.000079945, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.1128 103.74764\n",
      "l2 norm: 920.5789196459212\n",
      "l1 norm: 771.6772426094617\n",
      "Rbeta: 920.8340433061551\n",
      "\n",
      "Train set: Avg. loss: 0.000079721, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.14509 103.779205\n",
      "l2 norm: 920.5006794590399\n",
      "l1 norm: 771.6130680011086\n",
      "Rbeta: 920.7555177643778\n",
      "\n",
      "Train set: Avg. loss: 0.000079498, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.17737 103.81083\n",
      "l2 norm: 920.4099252042625\n",
      "l1 norm: 771.5384286233536\n",
      "Rbeta: 920.6645136767844\n",
      "\n",
      "Train set: Avg. loss: 0.000079276, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.2095 103.84236\n",
      "l2 norm: 920.3230828061678\n",
      "l1 norm: 771.467056507744\n",
      "Rbeta: 920.5774602966354\n",
      "\n",
      "Train set: Avg. loss: 0.000079055, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.24164 103.87378\n",
      "l2 norm: 920.2459940779819\n",
      "l1 norm: 771.40375309098\n",
      "Rbeta: 920.5001191916363\n",
      "\n",
      "Train set: Avg. loss: 0.000078834, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.27379 103.905205\n",
      "l2 norm: 920.180646269229\n",
      "l1 norm: 771.3503150730694\n",
      "Rbeta: 920.4345274290633\n",
      "\n",
      "Train set: Avg. loss: 0.000078615, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.30593 103.93663\n",
      "l2 norm: 920.118695292069\n",
      "l1 norm: 771.2996982907209\n",
      "Rbeta: 920.3722776333989\n",
      "\n",
      "Train set: Avg. loss: 0.000078396, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.33809 103.967964\n",
      "l2 norm: 920.0493138777136\n",
      "l1 norm: 771.2428772880444\n",
      "Rbeta: 920.3027220474593\n",
      "\n",
      "Train set: Avg. loss: 0.000078178, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.370155 103.999176\n",
      "l2 norm: 919.9918195152718\n",
      "l1 norm: 771.1959438090291\n",
      "Rbeta: 920.244897799559\n",
      "\n",
      "Train set: Avg. loss: 0.000077962, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.40215 104.0303\n",
      "l2 norm: 919.9335429600319\n",
      "l1 norm: 771.1483827039146\n",
      "Rbeta: 920.1863715481858\n",
      "\n",
      "Train set: Avg. loss: 0.000077746, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.43416 104.06131\n",
      "l2 norm: 919.8584133540439\n",
      "l1 norm: 771.0867795489909\n",
      "Rbeta: 920.1109775268513\n",
      "\n",
      "Train set: Avg. loss: 0.000077532, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.46617 104.09217\n",
      "l2 norm: 919.7858242611965\n",
      "l1 norm: 771.0272608660234\n",
      "Rbeta: 920.0381200485189\n",
      "\n",
      "Train set: Avg. loss: 0.000077318, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.49819 104.123\n",
      "l2 norm: 919.714383875371\n",
      "l1 norm: 770.9686094617589\n",
      "Rbeta: 919.9663509895129\n",
      "\n",
      "Train set: Avg. loss: 0.000077105, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.53022 104.15373\n",
      "l2 norm: 919.6455015470257\n",
      "l1 norm: 770.9120822399505\n",
      "Rbeta: 919.8972094722297\n",
      "\n",
      "Train set: Avg. loss: 0.000076893, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.56224 104.18441\n",
      "l2 norm: 919.5800932620119\n",
      "l1 norm: 770.8585620200082\n",
      "Rbeta: 919.8314203710969\n",
      "\n",
      "Train set: Avg. loss: 0.000076682, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.59427 104.21494\n",
      "l2 norm: 919.4949268240046\n",
      "l1 norm: 770.7885430034806\n",
      "Rbeta: 919.7458948381636\n",
      "\n",
      "Train set: Avg. loss: 0.000076472, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.626144 104.24538\n",
      "l2 norm: 919.4221188428885\n",
      "l1 norm: 770.7289491272372\n",
      "Rbeta: 919.6728129717986\n",
      "\n",
      "Train set: Avg. loss: 0.000076263, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.65803 104.27573\n",
      "l2 norm: 919.3494441952867\n",
      "l1 norm: 770.6693614313695\n",
      "Rbeta: 919.5998070728574\n",
      "\n",
      "Train set: Avg. loss: 0.000076056, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.68991 104.305954\n",
      "l2 norm: 919.2939363641979\n",
      "l1 norm: 770.6241281717719\n",
      "Rbeta: 919.5439785718974\n",
      "\n",
      "Train set: Avg. loss: 0.000075848, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.7218 104.33614\n",
      "l2 norm: 919.240222127154\n",
      "l1 norm: 770.5803208822969\n",
      "Rbeta: 919.48996722989\n",
      "\n",
      "Train set: Avg. loss: 0.000075642, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.7537 104.366325\n",
      "l2 norm: 919.1852797031092\n",
      "l1 norm: 770.5353439671064\n",
      "Rbeta: 919.434630379341\n",
      "\n",
      "Train set: Avg. loss: 0.000075436, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.7856 104.39646\n",
      "l2 norm: 919.1219853716551\n",
      "l1 norm: 770.4832771704959\n",
      "Rbeta: 919.3710538373556\n",
      "\n",
      "Train set: Avg. loss: 0.000075231, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.817505 104.426575\n",
      "l2 norm: 919.0424205188687\n",
      "l1 norm: 770.4176098983885\n",
      "Rbeta: 919.2911098183133\n",
      "\n",
      "Train set: Avg. loss: 0.000075027, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.84941 104.45657\n",
      "l2 norm: 918.9504051343325\n",
      "l1 norm: 770.3416538286388\n",
      "Rbeta: 919.1986689705057\n",
      "\n",
      "Train set: Avg. loss: 0.000074823, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.88133 104.48655\n",
      "l2 norm: 918.8617672155999\n",
      "l1 norm: 770.2686112774604\n",
      "Rbeta: 919.1097519785636\n",
      "\n",
      "Train set: Avg. loss: 0.000074621, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.913246 104.51639\n",
      "l2 norm: 918.7806838425275\n",
      "l1 norm: 770.2019406311873\n",
      "Rbeta: 919.0281548435\n",
      "\n",
      "Train set: Avg. loss: 0.000074419, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.945175 104.5461\n",
      "l2 norm: 918.693098865893\n",
      "l1 norm: 770.129755538683\n",
      "Rbeta: 918.9402411284706\n",
      "\n",
      "Train set: Avg. loss: 0.000074218, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.97711 104.57562\n",
      "l2 norm: 918.6066952948202\n",
      "l1 norm: 770.0584540205347\n",
      "Rbeta: 918.8534198273996\n",
      "\n",
      "Train set: Avg. loss: 0.000074019, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.00899 104.60505\n",
      "l2 norm: 918.5410414740023\n",
      "l1 norm: 770.0045402102066\n",
      "Rbeta: 918.7874369946103\n",
      "\n",
      "Train set: Avg. loss: 0.000073821, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.04077 104.63432\n",
      "l2 norm: 918.4810945188425\n",
      "l1 norm: 769.955534602462\n",
      "Rbeta: 918.7270368959303\n",
      "\n",
      "Train set: Avg. loss: 0.000073623, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.07255 104.66349\n",
      "l2 norm: 918.4303825968724\n",
      "l1 norm: 769.9144199377463\n",
      "Rbeta: 918.6758452212465\n",
      "\n",
      "Train set: Avg. loss: 0.000073426, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.10433 104.69272\n",
      "l2 norm: 918.378712316029\n",
      "l1 norm: 769.8725496941145\n",
      "Rbeta: 918.6238039213747\n",
      "\n",
      "Train set: Avg. loss: 0.000073230, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.13613 104.721954\n",
      "l2 norm: 918.3160798646272\n",
      "l1 norm: 769.8213519709207\n",
      "Rbeta: 918.5608205220307\n",
      "\n",
      "Train set: Avg. loss: 0.000073034, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.16793 104.75109\n",
      "l2 norm: 918.234096310747\n",
      "l1 norm: 769.753851839519\n",
      "Rbeta: 918.4783259774962\n",
      "\n",
      "Train set: Avg. loss: 0.000072839, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.19973 104.78018\n",
      "l2 norm: 918.1427436635802\n",
      "l1 norm: 769.6784483612273\n",
      "Rbeta: 918.3865613497402\n",
      "\n",
      "Train set: Avg. loss: 0.000072645, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.23154 104.809235\n",
      "l2 norm: 918.0725829402513\n",
      "l1 norm: 769.6207379741015\n",
      "Rbeta: 918.315930423956\n",
      "\n",
      "Train set: Avg. loss: 0.000072451, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.26335 104.83819\n",
      "l2 norm: 918.0071402781454\n",
      "l1 norm: 769.5669291819706\n",
      "Rbeta: 918.2499656259955\n",
      "\n",
      "Train set: Avg. loss: 0.000072258, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.295166 104.86711\n",
      "l2 norm: 917.9332970360878\n",
      "l1 norm: 769.505992232343\n",
      "Rbeta: 918.1757147648207\n",
      "\n",
      "Train set: Avg. loss: 0.000072066, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.32699 104.89595\n",
      "l2 norm: 917.8587567045687\n",
      "l1 norm: 769.444535522892\n",
      "Rbeta: 918.1006818553312\n",
      "\n",
      "Train set: Avg. loss: 0.000071874, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.35882 104.92467\n",
      "l2 norm: 917.7931552791846\n",
      "l1 norm: 769.3906588279367\n",
      "Rbeta: 918.0345877762882\n",
      "\n",
      "Train set: Avg. loss: 0.000071683, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.390656 104.953354\n",
      "l2 norm: 917.7306320565581\n",
      "l1 norm: 769.3393537465954\n",
      "Rbeta: 917.9716537581512\n",
      "\n",
      "Train set: Avg. loss: 0.000071493, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.42249 104.98212\n",
      "l2 norm: 917.6673946750451\n",
      "l1 norm: 769.2874443367252\n",
      "Rbeta: 917.9079379817954\n",
      "\n",
      "Train set: Avg. loss: 0.000071303, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.45433 105.01087\n",
      "l2 norm: 917.5984289576669\n",
      "l1 norm: 769.2307206032974\n",
      "Rbeta: 917.8385937034251\n",
      "\n",
      "Train set: Avg. loss: 0.000071113, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.48619 105.03954\n",
      "l2 norm: 917.5194359638652\n",
      "l1 norm: 769.1656105827742\n",
      "Rbeta: 917.7590599658877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000070925, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.51804 105.06818\n",
      "l2 norm: 917.4515660313789\n",
      "l1 norm: 769.1099178751426\n",
      "Rbeta: 917.6907429728266\n",
      "\n",
      "Train set: Avg. loss: 0.000070737, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.54985 105.09676\n",
      "l2 norm: 917.3866515415231\n",
      "l1 norm: 769.0567748680562\n",
      "Rbeta: 917.6253015374353\n",
      "\n",
      "Train set: Avg. loss: 0.000070550, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.58154 105.125275\n",
      "l2 norm: 917.3128852917214\n",
      "l1 norm: 768.9962733364291\n",
      "Rbeta: 917.5511018389107\n",
      "\n",
      "Train set: Avg. loss: 0.000070364, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.61325 105.153625\n",
      "l2 norm: 917.244284008026\n",
      "l1 norm: 768.9401349707555\n",
      "Rbeta: 917.4820073274813\n",
      "\n",
      "Train set: Avg. loss: 0.000070179, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.64496 105.18189\n",
      "l2 norm: 917.1831101962497\n",
      "l1 norm: 768.890142052911\n",
      "Rbeta: 917.420293930641\n",
      "\n",
      "Train set: Avg. loss: 0.000069995, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.676674 105.21004\n",
      "l2 norm: 917.1346100062659\n",
      "l1 norm: 768.8506243342267\n",
      "Rbeta: 917.3713730270654\n",
      "\n",
      "Train set: Avg. loss: 0.000069811, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.7084 105.23813\n",
      "l2 norm: 917.0801518669975\n",
      "l1 norm: 768.8060204578019\n",
      "Rbeta: 917.3164298934637\n",
      "\n",
      "Train set: Avg. loss: 0.000069628, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.74012 105.26622\n",
      "l2 norm: 917.0147930910084\n",
      "l1 norm: 768.7522919260971\n",
      "Rbeta: 917.2505707250557\n",
      "\n",
      "Train set: Avg. loss: 0.000069446, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.77185 105.294205\n",
      "l2 norm: 916.944109019654\n",
      "l1 norm: 768.6940233732737\n",
      "Rbeta: 917.1792956914352\n",
      "\n",
      "Train set: Avg. loss: 0.000069264, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.80359 105.3221\n",
      "l2 norm: 916.8848806707332\n",
      "l1 norm: 768.6454043081395\n",
      "Rbeta: 917.1195904034896\n",
      "\n",
      "Train set: Avg. loss: 0.000069083, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.835335 105.34985\n",
      "l2 norm: 916.8246992500464\n",
      "l1 norm: 768.595920585002\n",
      "Rbeta: 917.058830375389\n",
      "\n",
      "Train set: Avg. loss: 0.000068903, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.86707 105.37753\n",
      "l2 norm: 916.767303733047\n",
      "l1 norm: 768.5488177151052\n",
      "Rbeta: 917.000872687993\n",
      "\n",
      "Train set: Avg. loss: 0.000068724, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.89882 105.405136\n",
      "l2 norm: 916.7091170433724\n",
      "l1 norm: 768.5009770615386\n",
      "Rbeta: 916.9421378646812\n",
      "\n",
      "Train set: Avg. loss: 0.000068545, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.93057 105.432724\n",
      "l2 norm: 916.6547118349472\n",
      "l1 norm: 768.4563105530943\n",
      "Rbeta: 916.8872009059759\n",
      "\n",
      "Train set: Avg. loss: 0.000068367, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.9623 105.46028\n",
      "l2 norm: 916.596156575373\n",
      "l1 norm: 768.4080910033799\n",
      "Rbeta: 916.8280868657984\n",
      "\n",
      "Train set: Avg. loss: 0.000068189, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.993904 105.487755\n",
      "l2 norm: 916.5328168923644\n",
      "l1 norm: 768.3558165100105\n",
      "Rbeta: 916.764191179983\n",
      "\n",
      "Train set: Avg. loss: 0.000068013, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.02551 105.515144\n",
      "l2 norm: 916.4775771273498\n",
      "l1 norm: 768.3103706229163\n",
      "Rbeta: 916.7084225193458\n",
      "\n",
      "Train set: Avg. loss: 0.000067837, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.05712 105.54251\n",
      "l2 norm: 916.4073217030661\n",
      "l1 norm: 768.2522960460668\n",
      "Rbeta: 916.6376629773291\n",
      "\n",
      "Train set: Avg. loss: 0.000067662, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.08874 105.56987\n",
      "l2 norm: 916.3438559061221\n",
      "l1 norm: 768.1999816497516\n",
      "Rbeta: 916.5736038676768\n",
      "\n",
      "Train set: Avg. loss: 0.000067487, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.12035 105.59719\n",
      "l2 norm: 916.2846350923961\n",
      "l1 norm: 768.1512185954424\n",
      "Rbeta: 916.5138216890884\n",
      "\n",
      "Train set: Avg. loss: 0.000067313, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.15198 105.62448\n",
      "l2 norm: 916.2303871261307\n",
      "l1 norm: 768.1066042549062\n",
      "Rbeta: 916.4590303211063\n",
      "\n",
      "Train set: Avg. loss: 0.000067139, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.18362 105.651726\n",
      "l2 norm: 916.1644476360996\n",
      "l1 norm: 768.0522590273522\n",
      "Rbeta: 916.3925252710509\n",
      "\n",
      "Train set: Avg. loss: 0.000066966, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.215256 105.67898\n",
      "l2 norm: 916.1016808110884\n",
      "l1 norm: 768.0006080073546\n",
      "Rbeta: 916.3292124013908\n",
      "\n",
      "Train set: Avg. loss: 0.000066793, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.246895 105.70614\n",
      "l2 norm: 916.034308496118\n",
      "l1 norm: 767.9450469656927\n",
      "Rbeta: 916.2613146377549\n",
      "\n",
      "Train set: Avg. loss: 0.000066621, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.278534 105.73317\n",
      "l2 norm: 915.9642240716838\n",
      "l1 norm: 767.8870975250529\n",
      "Rbeta: 916.1905616609337\n",
      "\n",
      "Train set: Avg. loss: 0.000066450, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.31019 105.76011\n",
      "l2 norm: 915.8860427901454\n",
      "l1 norm: 767.8224490277456\n",
      "Rbeta: 916.1118519554275\n",
      "\n",
      "Train set: Avg. loss: 0.000066280, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.34168 105.786995\n",
      "l2 norm: 915.8016938195884\n",
      "l1 norm: 767.7526893295653\n",
      "Rbeta: 916.026882885025\n",
      "\n",
      "Train set: Avg. loss: 0.000066110, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.373184 105.81388\n",
      "l2 norm: 915.7105928458635\n",
      "l1 norm: 767.677283800997\n",
      "Rbeta: 915.9352870569832\n",
      "\n",
      "Train set: Avg. loss: 0.000065941, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.404686 105.84068\n",
      "l2 norm: 915.6284513266914\n",
      "l1 norm: 767.6093534656809\n",
      "Rbeta: 915.8525164080356\n",
      "\n",
      "Train set: Avg. loss: 0.000065773, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.43619 105.867355\n",
      "l2 norm: 915.558016945636\n",
      "l1 norm: 767.5511693738149\n",
      "Rbeta: 915.7814521922886\n",
      "\n",
      "Train set: Avg. loss: 0.000065605, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.4677 105.89396\n",
      "l2 norm: 915.5006958101036\n",
      "l1 norm: 767.503860932207\n",
      "Rbeta: 915.7235465975947\n",
      "\n",
      "Train set: Avg. loss: 0.000065438, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.499214 105.920494\n",
      "l2 norm: 915.454318560958\n",
      "l1 norm: 767.4656861746715\n",
      "Rbeta: 915.6765476779076\n",
      "\n",
      "Train set: Avg. loss: 0.000065272, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.53074 105.947044\n",
      "l2 norm: 915.3976460973172\n",
      "l1 norm: 767.4188103996617\n",
      "Rbeta: 915.6192978712421\n",
      "\n",
      "Train set: Avg. loss: 0.000065106, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.56226 105.97357\n",
      "l2 norm: 915.3339361615125\n",
      "l1 norm: 767.3661625831708\n",
      "Rbeta: 915.554970990814\n",
      "\n",
      "Train set: Avg. loss: 0.000064940, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.59379 106.00007\n",
      "l2 norm: 915.2676341218317\n",
      "l1 norm: 767.3113131240759\n",
      "Rbeta: 915.4880349157191\n",
      "\n",
      "Train set: Avg. loss: 0.000064775, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.62533 106.02649\n",
      "l2 norm: 915.2059325554497\n",
      "l1 norm: 767.2603374135928\n",
      "Rbeta: 915.4258080417649\n",
      "\n",
      "Train set: Avg. loss: 0.000064611, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.656876 106.05277\n",
      "l2 norm: 915.1448415723438\n",
      "l1 norm: 767.209847908849\n",
      "Rbeta: 915.3641011416212\n",
      "\n",
      "Train set: Avg. loss: 0.000064447, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.688416 106.07908\n",
      "l2 norm: 915.0871826520755\n",
      "l1 norm: 767.1623424517004\n",
      "Rbeta: 915.3057416348037\n",
      "\n",
      "Train set: Avg. loss: 0.000064284, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.71996 106.10533\n",
      "l2 norm: 915.0394937460984\n",
      "l1 norm: 767.1232541840784\n",
      "Rbeta: 915.2574382079514\n",
      "\n",
      "Train set: Avg. loss: 0.000064121, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.75152 106.1315\n",
      "l2 norm: 914.9952393649472\n",
      "l1 norm: 767.0871726073512\n",
      "Rbeta: 915.2126205297606\n",
      "\n",
      "Train set: Avg. loss: 0.000063959, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.78308 106.15756\n",
      "l2 norm: 914.9446703629704\n",
      "l1 norm: 767.0458272465758\n",
      "Rbeta: 915.1614131537899\n",
      "\n",
      "Train set: Avg. loss: 0.000063798, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.814644 106.1835\n",
      "l2 norm: 914.896791954735\n",
      "l1 norm: 767.0067458300721\n",
      "Rbeta: 915.1128225041468\n",
      "\n",
      "Train set: Avg. loss: 0.000063637, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.84622 106.20935\n",
      "l2 norm: 914.8502967658243\n",
      "l1 norm: 766.9689064468043\n",
      "Rbeta: 915.0657345942491\n",
      "\n",
      "Train set: Avg. loss: 0.000063477, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.87779 106.235054\n",
      "l2 norm: 914.8002638650257\n",
      "l1 norm: 766.9279915044815\n",
      "Rbeta: 915.0150691452624\n",
      "\n",
      "Train set: Avg. loss: 0.000063318, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.90938 106.26073\n",
      "l2 norm: 914.7523955522367\n",
      "l1 norm: 766.8888676663573\n",
      "Rbeta: 914.9665202922929\n",
      "\n",
      "Train set: Avg. loss: 0.000063159, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.940956 106.286354\n",
      "l2 norm: 914.7150315163029\n",
      "l1 norm: 766.8586087356085\n",
      "Rbeta: 914.9284649583251\n",
      "\n",
      "Train set: Avg. loss: 0.000063001, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.97255 106.31196\n",
      "l2 norm: 914.6670217136219\n",
      "l1 norm: 766.8195082033693\n",
      "Rbeta: 914.8798345776099\n",
      "\n",
      "Train set: Avg. loss: 0.000062843, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.00415 106.337494\n",
      "l2 norm: 914.6167413350083\n",
      "l1 norm: 766.7784238322783\n",
      "Rbeta: 914.828865875771\n",
      "\n",
      "Train set: Avg. loss: 0.000062686, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.03574 106.36299\n",
      "l2 norm: 914.5599811218354\n",
      "l1 norm: 766.7317634045164\n",
      "Rbeta: 914.7714543537942\n",
      "\n",
      "Train set: Avg. loss: 0.000062529, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.06735 106.38843\n",
      "l2 norm: 914.5092562234422\n",
      "l1 norm: 766.6901215101595\n",
      "Rbeta: 914.720033980547\n",
      "\n",
      "Train set: Avg. loss: 0.000062373, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.09896 106.4138\n",
      "l2 norm: 914.4481734311091\n",
      "l1 norm: 766.6397686526108\n",
      "Rbeta: 914.6582159102719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000062217, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.13057 106.439125\n",
      "l2 norm: 914.3884623958116\n",
      "l1 norm: 766.5906202755041\n",
      "Rbeta: 914.5978599644612\n",
      "\n",
      "Train set: Avg. loss: 0.000062062, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.16219 106.46441\n",
      "l2 norm: 914.3311286810207\n",
      "l1 norm: 766.5434391069623\n",
      "Rbeta: 914.5397935398153\n",
      "\n",
      "Train set: Avg. loss: 0.000061907, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.19382 106.48967\n",
      "l2 norm: 914.272279913\n",
      "l1 norm: 766.4950967607097\n",
      "Rbeta: 914.4802661272797\n",
      "\n",
      "Train set: Avg. loss: 0.000061753, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.22545 106.514885\n",
      "l2 norm: 914.2154222892889\n",
      "l1 norm: 766.448461788875\n",
      "Rbeta: 914.4226675453832\n",
      "\n",
      "Train set: Avg. loss: 0.000061599, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.25709 106.54005\n",
      "l2 norm: 914.1542622723542\n",
      "l1 norm: 766.3981258422596\n",
      "Rbeta: 914.3609165862074\n",
      "\n",
      "Train set: Avg. loss: 0.000061446, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.28873 106.565186\n",
      "l2 norm: 914.0957352869121\n",
      "l1 norm: 766.3499343652571\n",
      "Rbeta: 914.3016982224543\n",
      "\n",
      "Train set: Avg. loss: 0.000061293, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.32037 106.5903\n",
      "l2 norm: 914.0335338482693\n",
      "l1 norm: 766.298704531263\n",
      "Rbeta: 914.2387565929686\n",
      "\n",
      "Train set: Avg. loss: 0.000061141, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.35202 106.61539\n",
      "l2 norm: 913.9659849198231\n",
      "l1 norm: 766.242984958272\n",
      "Rbeta: 914.1704800873625\n",
      "\n",
      "Train set: Avg. loss: 0.000060989, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.383675 106.64042\n",
      "l2 norm: 913.9003216942842\n",
      "l1 norm: 766.1887616414562\n",
      "Rbeta: 914.104135529654\n",
      "\n",
      "Train set: Avg. loss: 0.000060837, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.41533 106.66538\n",
      "l2 norm: 913.841030689739\n",
      "l1 norm: 766.1398640447799\n",
      "Rbeta: 914.0441791150562\n",
      "\n",
      "Train set: Avg. loss: 0.000060687, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.447 106.69025\n",
      "l2 norm: 913.7865874358755\n",
      "l1 norm: 766.0950324727767\n",
      "Rbeta: 913.988974534838\n",
      "\n",
      "Train set: Avg. loss: 0.000060536, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.47867 106.71508\n",
      "l2 norm: 913.7378654588365\n",
      "l1 norm: 766.0550722239136\n",
      "Rbeta: 913.9396372679037\n",
      "\n",
      "Train set: Avg. loss: 0.000060387, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.510345 106.739784\n",
      "l2 norm: 913.6791266391081\n",
      "l1 norm: 766.0067673978785\n",
      "Rbeta: 913.880090015237\n",
      "\n",
      "Train set: Avg. loss: 0.000060238, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.54202 106.76445\n",
      "l2 norm: 913.6175804859774\n",
      "l1 norm: 765.9561677399222\n",
      "Rbeta: 913.8178406805322\n",
      "\n",
      "Train set: Avg. loss: 0.000060089, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.5737 106.789085\n",
      "l2 norm: 913.5511142683724\n",
      "l1 norm: 765.9013917690734\n",
      "Rbeta: 913.7506271829462\n",
      "\n",
      "Train set: Avg. loss: 0.000059941, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.6054 106.81365\n",
      "l2 norm: 913.4814293522138\n",
      "l1 norm: 765.8439775847899\n",
      "Rbeta: 913.6802233732069\n",
      "\n",
      "Train set: Avg. loss: 0.000059793, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.637085 106.83812\n",
      "l2 norm: 913.4260675375182\n",
      "l1 norm: 765.7985481499888\n",
      "Rbeta: 913.624068398892\n",
      "\n",
      "Train set: Avg. loss: 0.000059646, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.66879 106.8625\n",
      "l2 norm: 913.3710935692288\n",
      "l1 norm: 765.7535343788057\n",
      "Rbeta: 913.5683965555551\n",
      "\n",
      "Train set: Avg. loss: 0.000059500, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.70049 106.886795\n",
      "l2 norm: 913.3203480662356\n",
      "l1 norm: 765.7121429502924\n",
      "Rbeta: 913.5168642725941\n",
      "\n",
      "Train set: Avg. loss: 0.000059354, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.73221 106.91099\n",
      "l2 norm: 913.2757465983286\n",
      "l1 norm: 765.6759143187849\n",
      "Rbeta: 913.4715569663188\n",
      "\n",
      "Train set: Avg. loss: 0.000059209, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.763916 106.93511\n",
      "l2 norm: 913.2211853147451\n",
      "l1 norm: 765.631306462842\n",
      "Rbeta: 913.4162299498631\n",
      "\n",
      "Train set: Avg. loss: 0.000059064, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.79564 106.95916\n",
      "l2 norm: 913.1586405666174\n",
      "l1 norm: 765.5800041210734\n",
      "Rbeta: 913.352895409379\n",
      "\n",
      "Train set: Avg. loss: 0.000058920, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.827354 106.98319\n",
      "l2 norm: 913.0967148882823\n",
      "l1 norm: 765.529240091176\n",
      "Rbeta: 913.2901956989152\n",
      "\n",
      "Train set: Avg. loss: 0.000058776, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.859085 107.007256\n",
      "l2 norm: 913.0365604300152\n",
      "l1 norm: 765.4799925911419\n",
      "Rbeta: 913.2292695224689\n",
      "\n",
      "Train set: Avg. loss: 0.000058632, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.89082 107.031235\n",
      "l2 norm: 912.9776588188535\n",
      "l1 norm: 765.4317372608082\n",
      "Rbeta: 913.1696573410899\n",
      "\n",
      "Train set: Avg. loss: 0.000058489, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.92256 107.05513\n",
      "l2 norm: 912.9239204357428\n",
      "l1 norm: 765.3878739156531\n",
      "Rbeta: 913.1151300126722\n",
      "\n",
      "Train set: Avg. loss: 0.000058347, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.9543 107.07893\n",
      "l2 norm: 912.8660636845427\n",
      "l1 norm: 765.3405071801103\n",
      "Rbeta: 913.0564547003219\n",
      "\n",
      "Train set: Avg. loss: 0.000058205, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.98605 107.102684\n",
      "l2 norm: 912.8144457577994\n",
      "l1 norm: 765.298326095516\n",
      "Rbeta: 913.0041889038537\n",
      "\n",
      "Train set: Avg. loss: 0.000058063, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.01781 107.126434\n",
      "l2 norm: 912.7624387993849\n",
      "l1 norm: 765.2557094121636\n",
      "Rbeta: 912.9512993940649\n",
      "\n",
      "Train set: Avg. loss: 0.000057922, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.04957 107.15024\n",
      "l2 norm: 912.7095378231855\n",
      "l1 norm: 765.2123777656342\n",
      "Rbeta: 912.8976455897001\n",
      "\n",
      "Train set: Avg. loss: 0.000057781, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.08133 107.17395\n",
      "l2 norm: 912.6605848443868\n",
      "l1 norm: 765.172405125395\n",
      "Rbeta: 912.8479472750149\n",
      "\n",
      "Train set: Avg. loss: 0.000057641, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.1131 107.19763\n",
      "l2 norm: 912.6080365753954\n",
      "l1 norm: 765.1294459417926\n",
      "Rbeta: 912.7946060597751\n",
      "\n",
      "Train set: Avg. loss: 0.000057501, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.144875 107.22122\n",
      "l2 norm: 912.5528766454469\n",
      "l1 norm: 765.0842980577766\n",
      "Rbeta: 912.7386595168716\n",
      "\n",
      "Train set: Avg. loss: 0.000057362, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.17665 107.24472\n",
      "l2 norm: 912.4969032668325\n",
      "l1 norm: 765.0385548043273\n",
      "Rbeta: 912.681946435838\n",
      "\n",
      "Train set: Avg. loss: 0.000057223, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.208435 107.26821\n",
      "l2 norm: 912.4511320911163\n",
      "l1 norm: 765.0013902235986\n",
      "Rbeta: 912.6354124413081\n",
      "\n",
      "Train set: Avg. loss: 0.000057084, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.24022 107.29168\n",
      "l2 norm: 912.4114504151422\n",
      "l1 norm: 764.9693104643495\n",
      "Rbeta: 912.5948985446661\n",
      "\n",
      "Train set: Avg. loss: 0.000056946, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.27201 107.31508\n",
      "l2 norm: 912.3680623452806\n",
      "l1 norm: 764.934014889533\n",
      "Rbeta: 912.5507417146804\n",
      "\n",
      "Train set: Avg. loss: 0.000056808, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.30381 107.33846\n",
      "l2 norm: 912.3267549356922\n",
      "l1 norm: 764.9003894086742\n",
      "Rbeta: 912.5086595590304\n",
      "\n",
      "Train set: Avg. loss: 0.000056671, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.33562 107.3618\n",
      "l2 norm: 912.2872481455609\n",
      "l1 norm: 764.8682110537809\n",
      "Rbeta: 912.468355440028\n",
      "\n",
      "Train set: Avg. loss: 0.000056534, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.367424 107.38516\n",
      "l2 norm: 912.2464214046147\n",
      "l1 norm: 764.8349875703832\n",
      "Rbeta: 912.4267953755233\n",
      "\n",
      "Train set: Avg. loss: 0.000056397, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.39923 107.408554\n",
      "l2 norm: 912.1975280828557\n",
      "l1 norm: 764.7950077337845\n",
      "Rbeta: 912.3770890344523\n",
      "\n",
      "Train set: Avg. loss: 0.000056260, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.431046 107.43193\n",
      "l2 norm: 912.1401846490154\n",
      "l1 norm: 764.7478535875098\n",
      "Rbeta: 912.3189441972471\n",
      "\n",
      "Train set: Avg. loss: 0.000056124, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.462875 107.45529\n",
      "l2 norm: 912.0812980999332\n",
      "l1 norm: 764.6993425181086\n",
      "Rbeta: 912.2593085135522\n",
      "\n",
      "Train set: Avg. loss: 0.000055989, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.4947 107.47858\n",
      "l2 norm: 912.0285044930448\n",
      "l1 norm: 764.656076346087\n",
      "Rbeta: 912.2056634135731\n",
      "\n",
      "Train set: Avg. loss: 0.000055854, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.526535 107.5018\n",
      "l2 norm: 911.9763375907495\n",
      "l1 norm: 764.6132896778013\n",
      "Rbeta: 912.1528019839535\n",
      "\n",
      "Train set: Avg. loss: 0.000055719, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.55837 107.52502\n",
      "l2 norm: 911.9184404669115\n",
      "l1 norm: 764.5657739323951\n",
      "Rbeta: 912.0940942128158\n",
      "\n",
      "Train set: Avg. loss: 0.000055585, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.59022 107.548225\n",
      "l2 norm: 911.8650837381003\n",
      "l1 norm: 764.5220808119024\n",
      "Rbeta: 912.0399449271649\n",
      "\n",
      "Train set: Avg. loss: 0.000055451, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.62207 107.57133\n",
      "l2 norm: 911.8145159490696\n",
      "l1 norm: 764.4806645165027\n",
      "Rbeta: 911.9885966585306\n",
      "\n",
      "Train set: Avg. loss: 0.000055317, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.653915 107.5944\n",
      "l2 norm: 911.7633789572701\n",
      "l1 norm: 764.4387773730858\n",
      "Rbeta: 911.936687050738\n",
      "\n",
      "Train set: Avg. loss: 0.000055184, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.685776 107.61741\n",
      "l2 norm: 911.710408044981\n",
      "l1 norm: 764.3953311219203\n",
      "Rbeta: 911.8829636613764\n",
      "\n",
      "Train set: Avg. loss: 0.000055052, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.717636 107.64028\n",
      "l2 norm: 911.6602990985957\n",
      "l1 norm: 764.3542247981932\n",
      "Rbeta: 911.8319574200382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000054920, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.749504 107.663055\n",
      "l2 norm: 911.6274966153031\n",
      "l1 norm: 764.3275496369708\n",
      "Rbeta: 911.798416473539\n",
      "\n",
      "Train set: Avg. loss: 0.000054789, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.78138 107.68581\n",
      "l2 norm: 911.5796711422067\n",
      "l1 norm: 764.2882982671692\n",
      "Rbeta: 911.7498263243184\n",
      "\n",
      "Train set: Avg. loss: 0.000054657, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.813255 107.70853\n",
      "l2 norm: 911.5295977528319\n",
      "l1 norm: 764.2471936936944\n",
      "Rbeta: 911.698968350176\n",
      "\n",
      "Train set: Avg. loss: 0.000054527, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.84514 107.7312\n",
      "l2 norm: 911.4808227024363\n",
      "l1 norm: 764.2071462189974\n",
      "Rbeta: 911.6493498646938\n",
      "\n",
      "Train set: Avg. loss: 0.000054397, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.87703 107.75381\n",
      "l2 norm: 911.4294675367851\n",
      "l1 norm: 764.1649126990897\n",
      "Rbeta: 911.5971589485316\n",
      "\n",
      "Train set: Avg. loss: 0.000054267, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.90891 107.77632\n",
      "l2 norm: 911.3690006384979\n",
      "l1 norm: 764.1149420269758\n",
      "Rbeta: 911.5358988470505\n",
      "\n",
      "Train set: Avg. loss: 0.000054138, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.94081 107.79875\n",
      "l2 norm: 911.3117524632098\n",
      "l1 norm: 764.0676228275129\n",
      "Rbeta: 911.4778508199265\n",
      "\n",
      "Train set: Avg. loss: 0.000054011, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.97183 107.82102\n",
      "l2 norm: 911.2562523769753\n",
      "l1 norm: 764.0217384750183\n",
      "Rbeta: 911.4215115108414\n",
      "\n",
      "Train set: Avg. loss: 0.000053887, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.001915 107.84326\n",
      "l2 norm: 911.1995676938619\n",
      "l1 norm: 763.9749775436196\n",
      "Rbeta: 911.3642340225715\n",
      "\n",
      "Train set: Avg. loss: 0.000053764, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.032 107.86542\n",
      "l2 norm: 911.1396381064374\n",
      "l1 norm: 763.9254995841266\n",
      "Rbeta: 911.3035541953835\n",
      "\n",
      "Train set: Avg. loss: 0.000053641, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.06209 107.88754\n",
      "l2 norm: 911.0777467384837\n",
      "l1 norm: 763.8743716817189\n",
      "Rbeta: 911.2409786258728\n",
      "\n",
      "Train set: Avg. loss: 0.000053518, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.09218 107.90957\n",
      "l2 norm: 911.0271862813784\n",
      "l1 norm: 763.8327610890883\n",
      "Rbeta: 911.1897131447505\n",
      "\n",
      "Train set: Avg. loss: 0.000053396, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.12228 107.93158\n",
      "l2 norm: 910.97962935691\n",
      "l1 norm: 763.793639499072\n",
      "Rbeta: 911.1415777103764\n",
      "\n",
      "Train set: Avg. loss: 0.000053274, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.152374 107.95359\n",
      "l2 norm: 910.9381676704138\n",
      "l1 norm: 763.7595839742149\n",
      "Rbeta: 911.0992882715806\n",
      "\n",
      "Train set: Avg. loss: 0.000053152, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.18248 107.97564\n",
      "l2 norm: 910.8837626160494\n",
      "l1 norm: 763.7146259473759\n",
      "Rbeta: 911.0442271120587\n",
      "\n",
      "Train set: Avg. loss: 0.000053031, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.212585 107.99758\n",
      "l2 norm: 910.8318108081106\n",
      "l1 norm: 763.6717306486057\n",
      "Rbeta: 910.9916038375658\n",
      "\n",
      "Train set: Avg. loss: 0.000052910, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.2427 108.0195\n",
      "l2 norm: 910.7796124069218\n",
      "l1 norm: 763.6288194372154\n",
      "Rbeta: 910.9387649755157\n",
      "\n",
      "Train set: Avg. loss: 0.000052790, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.27281 108.0414\n",
      "l2 norm: 910.7201315886682\n",
      "l1 norm: 763.5798548463846\n",
      "Rbeta: 910.8784899129433\n",
      "\n",
      "Train set: Avg. loss: 0.000052669, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.30294 108.0633\n",
      "l2 norm: 910.6656684853514\n",
      "l1 norm: 763.5350257093801\n",
      "Rbeta: 910.8233893210484\n",
      "\n",
      "Train set: Avg. loss: 0.000052549, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.33306 108.08519\n",
      "l2 norm: 910.625408542686\n",
      "l1 norm: 763.5020829005574\n",
      "Rbeta: 910.7823899756829\n",
      "\n",
      "Train set: Avg. loss: 0.000052430, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.3632 108.10698\n",
      "l2 norm: 910.5777075920129\n",
      "l1 norm: 763.462892883333\n",
      "Rbeta: 910.7341140380387\n",
      "\n",
      "Train set: Avg. loss: 0.000052311, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.39333 108.12868\n",
      "l2 norm: 910.5173974238181\n",
      "l1 norm: 763.4131492563167\n",
      "Rbeta: 910.6730160003157\n",
      "\n",
      "Train set: Avg. loss: 0.000052192, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.42348 108.15038\n",
      "l2 norm: 910.4526782155486\n",
      "l1 norm: 763.3597019295994\n",
      "Rbeta: 910.6076109222947\n",
      "\n",
      "Train set: Avg. loss: 0.000052073, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.45362 108.17207\n",
      "l2 norm: 910.381495550445\n",
      "l1 norm: 763.3009070550822\n",
      "Rbeta: 910.5357976536253\n",
      "\n",
      "Train set: Avg. loss: 0.000051955, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.483765 108.193695\n",
      "l2 norm: 910.312667136353\n",
      "l1 norm: 763.244081617702\n",
      "Rbeta: 910.4661942952328\n",
      "\n",
      "Train set: Avg. loss: 0.000051838, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.513916 108.21525\n",
      "l2 norm: 910.2597337211398\n",
      "l1 norm: 763.2005889637835\n",
      "Rbeta: 910.412514347945\n",
      "\n",
      "Train set: Avg. loss: 0.000051720, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.54408 108.23676\n",
      "l2 norm: 910.21237598293\n",
      "l1 norm: 763.1618732191441\n",
      "Rbeta: 910.3645545739383\n",
      "\n",
      "Train set: Avg. loss: 0.000051603, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.574234 108.25823\n",
      "l2 norm: 910.1659500320793\n",
      "l1 norm: 763.123891733749\n",
      "Rbeta: 910.3173339166322\n",
      "\n",
      "Train set: Avg. loss: 0.000051487, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.6044 108.27967\n",
      "l2 norm: 910.1322946485008\n",
      "l1 norm: 763.0966798467653\n",
      "Rbeta: 910.2829589201139\n",
      "\n",
      "Train set: Avg. loss: 0.000051370, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.634575 108.30111\n",
      "l2 norm: 910.1009914644706\n",
      "l1 norm: 763.071476712841\n",
      "Rbeta: 910.2510456674305\n",
      "\n",
      "Train set: Avg. loss: 0.000051254, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.66475 108.322495\n",
      "l2 norm: 910.0642752069184\n",
      "l1 norm: 763.0417259308799\n",
      "Rbeta: 910.2136145893671\n",
      "\n",
      "Train set: Avg. loss: 0.000051139, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.69493 108.34383\n",
      "l2 norm: 910.0269972631386\n",
      "l1 norm: 763.0114785654646\n",
      "Rbeta: 910.1756614659348\n",
      "\n",
      "Train set: Avg. loss: 0.000051023, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.72511 108.36509\n",
      "l2 norm: 909.9886140893287\n",
      "l1 norm: 762.980327433243\n",
      "Rbeta: 910.1366002823412\n",
      "\n",
      "Train set: Avg. loss: 0.000050908, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.7553 108.38634\n",
      "l2 norm: 909.9500309342966\n",
      "l1 norm: 762.9490072036848\n",
      "Rbeta: 910.0972572805526\n",
      "\n",
      "Train set: Avg. loss: 0.000050794, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.78549 108.40759\n",
      "l2 norm: 909.9030327831912\n",
      "l1 norm: 762.9106609168291\n",
      "Rbeta: 910.0495575360839\n",
      "\n",
      "Train set: Avg. loss: 0.000050679, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.81569 108.4288\n",
      "l2 norm: 909.8662360270315\n",
      "l1 norm: 762.8808843706606\n",
      "Rbeta: 910.0120486685475\n",
      "\n",
      "Train set: Avg. loss: 0.000050565, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.845894 108.45001\n",
      "l2 norm: 909.834683943721\n",
      "l1 norm: 762.8554378280008\n",
      "Rbeta: 909.9797674136224\n",
      "\n",
      "Train set: Avg. loss: 0.000050451, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.8761 108.47118\n",
      "l2 norm: 909.8049998392584\n",
      "l1 norm: 762.8315629882876\n",
      "Rbeta: 909.9494630358582\n",
      "\n",
      "Train set: Avg. loss: 0.000050338, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.9063 108.49223\n",
      "l2 norm: 909.7792561428097\n",
      "l1 norm: 762.8109543440092\n",
      "Rbeta: 909.922918537595\n",
      "\n",
      "Train set: Avg. loss: 0.000050225, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.93652 108.5132\n",
      "l2 norm: 909.7372845549146\n",
      "l1 norm: 762.7767749756288\n",
      "Rbeta: 909.8802991907266\n",
      "\n",
      "Train set: Avg. loss: 0.000050113, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.96674 108.534134\n",
      "l2 norm: 909.6946685439658\n",
      "l1 norm: 762.7420279415608\n",
      "Rbeta: 909.836996460045\n",
      "\n",
      "Train set: Avg. loss: 0.000050000, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.99696 108.55505\n",
      "l2 norm: 909.6436351381776\n",
      "l1 norm: 762.7002656803015\n",
      "Rbeta: 909.7852139423727\n",
      "\n",
      "Train set: Avg. loss: 0.000049889, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.02719 108.57593\n",
      "l2 norm: 909.5872366186898\n",
      "l1 norm: 762.6539660799228\n",
      "Rbeta: 909.7281359926748\n",
      "\n",
      "Train set: Avg. loss: 0.000049777, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.05743 108.596725\n",
      "l2 norm: 909.5332838213749\n",
      "l1 norm: 762.6096828068348\n",
      "Rbeta: 909.6734224957613\n",
      "\n",
      "Train set: Avg. loss: 0.000049666, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.087654 108.61749\n",
      "l2 norm: 909.4795124625305\n",
      "l1 norm: 762.5654802461916\n",
      "Rbeta: 909.6189002723052\n",
      "\n",
      "Train set: Avg. loss: 0.000049555, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.117905 108.638275\n",
      "l2 norm: 909.420951639048\n",
      "l1 norm: 762.5172241418469\n",
      "Rbeta: 909.5597032456751\n",
      "\n",
      "Train set: Avg. loss: 0.000049444, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.14814 108.65902\n",
      "l2 norm: 909.3556497173611\n",
      "l1 norm: 762.4632874504033\n",
      "Rbeta: 909.4936527815757\n",
      "\n",
      "Train set: Avg. loss: 0.000049337, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.17685 108.67971\n",
      "l2 norm: 909.3004210093225\n",
      "l1 norm: 762.417817360441\n",
      "Rbeta: 909.4378450391919\n",
      "\n",
      "Train set: Avg. loss: 0.000049231, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.20526 108.70042\n",
      "l2 norm: 909.2564233537962\n",
      "l1 norm: 762.3816800875235\n",
      "Rbeta: 909.3931585980932\n",
      "\n",
      "Train set: Avg. loss: 0.000049125, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.23368 108.7211\n",
      "l2 norm: 909.2176859597522\n",
      "l1 norm: 762.3498840860593\n",
      "Rbeta: 909.3538531029535\n",
      "\n",
      "Train set: Avg. loss: 0.000049020, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.2621 108.74177\n",
      "l2 norm: 909.1765359715539\n",
      "l1 norm: 762.3160858319175\n",
      "Rbeta: 909.3122085102902\n",
      "\n",
      "Train set: Avg. loss: 0.000048914, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.29053 108.762375\n",
      "l2 norm: 909.1293677676615\n",
      "l1 norm: 762.2772219698088\n",
      "Rbeta: 909.2643921501259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000048809, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.31896 108.782875\n",
      "l2 norm: 909.0848410357506\n",
      "l1 norm: 762.2405765680528\n",
      "Rbeta: 909.2193208355629\n",
      "\n",
      "Train set: Avg. loss: 0.000048705, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.3474 108.8033\n",
      "l2 norm: 909.0492353641517\n",
      "l1 norm: 762.2113353512768\n",
      "Rbeta: 909.1831534577803\n",
      "\n",
      "Train set: Avg. loss: 0.000048601, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.375824 108.82366\n",
      "l2 norm: 909.0231045989543\n",
      "l1 norm: 762.1900325527462\n",
      "Rbeta: 909.1564065404028\n",
      "\n",
      "Train set: Avg. loss: 0.000048497, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.40427 108.84405\n",
      "l2 norm: 909.0026681488031\n",
      "l1 norm: 762.1735491162847\n",
      "Rbeta: 909.1353214688475\n",
      "\n",
      "Train set: Avg. loss: 0.000048393, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.43272 108.864456\n",
      "l2 norm: 908.9779280633062\n",
      "l1 norm: 762.1534038580965\n",
      "Rbeta: 909.1100087540123\n",
      "\n",
      "Train set: Avg. loss: 0.000048290, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.46117 108.88483\n",
      "l2 norm: 908.9364820528903\n",
      "l1 norm: 762.1191974380905\n",
      "Rbeta: 909.067968656734\n",
      "\n",
      "Train set: Avg. loss: 0.000048187, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.48962 108.90511\n",
      "l2 norm: 908.8879987506868\n",
      "l1 norm: 762.0790756057261\n",
      "Rbeta: 909.0188511115239\n",
      "\n",
      "Train set: Avg. loss: 0.000048084, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.518074 108.925385\n",
      "l2 norm: 908.8265373937959\n",
      "l1 norm: 762.0280054849836\n",
      "Rbeta: 908.9567788986931\n",
      "\n",
      "Train set: Avg. loss: 0.000047981, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.54654 108.94562\n",
      "l2 norm: 908.7572615993483\n",
      "l1 norm: 761.9704402600489\n",
      "Rbeta: 908.8869311210723\n",
      "\n",
      "Train set: Avg. loss: 0.000047879, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.575 108.96579\n",
      "l2 norm: 908.6959311547674\n",
      "l1 norm: 761.9195783180651\n",
      "Rbeta: 908.8250061655799\n",
      "\n",
      "Train set: Avg. loss: 0.000047778, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.60347 108.98588\n",
      "l2 norm: 908.6315562804803\n",
      "l1 norm: 761.8661123162165\n",
      "Rbeta: 908.7600492216045\n",
      "\n",
      "Train set: Avg. loss: 0.000047676, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.63194 109.005936\n",
      "l2 norm: 908.5698789537089\n",
      "l1 norm: 761.8149814735291\n",
      "Rbeta: 908.6977461254138\n",
      "\n",
      "Train set: Avg. loss: 0.000047575, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.660416 109.02592\n",
      "l2 norm: 908.5164732400101\n",
      "l1 norm: 761.7707220776088\n",
      "Rbeta: 908.6437599753933\n",
      "\n",
      "Train set: Avg. loss: 0.000047474, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.688896 109.04587\n",
      "l2 norm: 908.460551112819\n",
      "l1 norm: 761.7243765788232\n",
      "Rbeta: 908.5872121575327\n",
      "\n",
      "Train set: Avg. loss: 0.000047373, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.717384 109.065765\n",
      "l2 norm: 908.4020903478317\n",
      "l1 norm: 761.6758934318407\n",
      "Rbeta: 908.5280907856636\n",
      "\n",
      "Train set: Avg. loss: 0.000047273, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.745865 109.08566\n",
      "l2 norm: 908.3474468737408\n",
      "l1 norm: 761.630495643511\n",
      "Rbeta: 908.4728464783515\n",
      "\n",
      "Train set: Avg. loss: 0.000047176, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.77303 109.1055\n",
      "l2 norm: 908.3051446643989\n",
      "l1 norm: 761.5955022551055\n",
      "Rbeta: 908.430083523064\n",
      "\n",
      "Train set: Avg. loss: 0.000047080, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.79968 109.125305\n",
      "l2 norm: 908.25708573108\n",
      "l1 norm: 761.5557079523923\n",
      "Rbeta: 908.3814673040225\n",
      "\n",
      "Train set: Avg. loss: 0.000046984, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.82633 109.145096\n",
      "l2 norm: 908.2008389585749\n",
      "l1 norm: 761.5090566838817\n",
      "Rbeta: 908.3247562115919\n",
      "\n",
      "Train set: Avg. loss: 0.000046889, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.85262 109.164894\n",
      "l2 norm: 908.156305737421\n",
      "l1 norm: 761.4722296133798\n",
      "Rbeta: 908.2797363709237\n",
      "\n",
      "Train set: Avg. loss: 0.000046798, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.87744 109.18465\n",
      "l2 norm: 908.1126696393594\n",
      "l1 norm: 761.4362308746765\n",
      "Rbeta: 908.2357337066819\n",
      "\n",
      "Train set: Avg. loss: 0.000046706, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.90226 109.20436\n",
      "l2 norm: 908.0636473860312\n",
      "l1 norm: 761.3956872848984\n",
      "Rbeta: 908.1863680708815\n",
      "\n",
      "Train set: Avg. loss: 0.000046615, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.92708 109.224045\n",
      "l2 norm: 908.0211155660164\n",
      "l1 norm: 761.3605738242148\n",
      "Rbeta: 908.143403728027\n",
      "\n",
      "Train set: Avg. loss: 0.000046525, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.951904 109.24372\n",
      "l2 norm: 907.9876892860798\n",
      "l1 norm: 761.3331357358056\n",
      "Rbeta: 908.109540589381\n",
      "\n",
      "Train set: Avg. loss: 0.000046434, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.97674 109.2634\n",
      "l2 norm: 907.9507483414575\n",
      "l1 norm: 761.3028177370452\n",
      "Rbeta: 908.0723974879918\n",
      "\n",
      "Train set: Avg. loss: 0.000046344, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.00156 109.28298\n",
      "l2 norm: 907.9097145936215\n",
      "l1 norm: 761.2690209235506\n",
      "Rbeta: 908.0308771322932\n",
      "\n",
      "Train set: Avg. loss: 0.000046253, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.0264 109.302536\n",
      "l2 norm: 907.873375457819\n",
      "l1 norm: 761.239193701352\n",
      "Rbeta: 907.9941628911282\n",
      "\n",
      "Train set: Avg. loss: 0.000046164, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.05123 109.32204\n",
      "l2 norm: 907.8467701206176\n",
      "l1 norm: 761.2175745624158\n",
      "Rbeta: 907.9671340438451\n",
      "\n",
      "Train set: Avg. loss: 0.000046076, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.07524 109.34151\n",
      "l2 norm: 907.8141193469693\n",
      "l1 norm: 761.1909691912003\n",
      "Rbeta: 907.9341857662256\n",
      "\n",
      "Train set: Avg. loss: 0.000045990, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.09823 109.36093\n",
      "l2 norm: 907.7720298900945\n",
      "l1 norm: 761.1564420189065\n",
      "Rbeta: 907.8918498057409\n",
      "\n",
      "Train set: Avg. loss: 0.000045905, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.12122 109.38029\n",
      "l2 norm: 907.7285902076753\n",
      "l1 norm: 761.1207364736359\n",
      "Rbeta: 907.84807325729\n",
      "\n",
      "Train set: Avg. loss: 0.000045819, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.144226 109.39958\n",
      "l2 norm: 907.6841423916153\n",
      "l1 norm: 761.0842114233601\n",
      "Rbeta: 907.8033277105412\n",
      "\n",
      "Train set: Avg. loss: 0.000045735, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.16723 109.418846\n",
      "l2 norm: 907.6418350508436\n",
      "l1 norm: 761.0494795084842\n",
      "Rbeta: 907.7607538792922\n",
      "\n",
      "Train set: Avg. loss: 0.000045650, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.19023 109.438065\n",
      "l2 norm: 907.603048504024\n",
      "l1 norm: 761.0176756500757\n",
      "Rbeta: 907.7216521915753\n",
      "\n",
      "Train set: Avg. loss: 0.000045565, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.21324 109.457275\n",
      "l2 norm: 907.5686666142076\n",
      "l1 norm: 760.9895416301667\n",
      "Rbeta: 907.6870092932318\n",
      "\n",
      "Train set: Avg. loss: 0.000045481, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.23625 109.476456\n",
      "l2 norm: 907.5335624890038\n",
      "l1 norm: 760.960763663865\n",
      "Rbeta: 907.6515712788989\n",
      "\n",
      "Train set: Avg. loss: 0.000045397, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.25926 109.49561\n",
      "l2 norm: 907.5021877947846\n",
      "l1 norm: 760.9350189950129\n",
      "Rbeta: 907.6199544856872\n",
      "\n",
      "Train set: Avg. loss: 0.000045313, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.28227 109.514755\n",
      "l2 norm: 907.4713478816094\n",
      "l1 norm: 760.9097617524758\n",
      "Rbeta: 907.5887926937123\n",
      "\n",
      "Train set: Avg. loss: 0.000045230, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.3053 109.533806\n",
      "l2 norm: 907.4404137276721\n",
      "l1 norm: 760.8843301673828\n",
      "Rbeta: 907.5576160261572\n",
      "\n",
      "Train set: Avg. loss: 0.000045146, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.328316 109.55276\n",
      "l2 norm: 907.4163302936926\n",
      "l1 norm: 760.8646430181016\n",
      "Rbeta: 907.5332172361692\n",
      "\n",
      "Train set: Avg. loss: 0.000045063, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.35133 109.57164\n",
      "l2 norm: 907.3932699990377\n",
      "l1 norm: 760.8458564187399\n",
      "Rbeta: 907.5098231055636\n",
      "\n",
      "Train set: Avg. loss: 0.000044981, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.37436 109.59055\n",
      "l2 norm: 907.3615771177733\n",
      "l1 norm: 760.8199237736231\n",
      "Rbeta: 907.4778169728856\n",
      "\n",
      "Train set: Avg. loss: 0.000044898, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.397385 109.60946\n",
      "l2 norm: 907.3214967094619\n",
      "l1 norm: 760.7870637678858\n",
      "Rbeta: 907.4374183157715\n",
      "\n",
      "Train set: Avg. loss: 0.000044815, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.42042 109.62837\n",
      "l2 norm: 907.2777930167355\n",
      "l1 norm: 760.7511429799099\n",
      "Rbeta: 907.3934758468926\n",
      "\n",
      "Train set: Avg. loss: 0.000044733, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.44344 109.64728\n",
      "l2 norm: 907.2329390780842\n",
      "l1 norm: 760.7142688094956\n",
      "Rbeta: 907.3482911202323\n",
      "\n",
      "Train set: Avg. loss: 0.000044650, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.466484 109.66626\n",
      "l2 norm: 907.1913390214036\n",
      "l1 norm: 760.6800952544284\n",
      "Rbeta: 907.3064072703497\n",
      "\n",
      "Train set: Avg. loss: 0.000044568, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.48952 109.6853\n",
      "l2 norm: 907.1494413261729\n",
      "l1 norm: 760.6456276289573\n",
      "Rbeta: 907.2641682961122\n",
      "\n",
      "Train set: Avg. loss: 0.000044486, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.51256 109.704315\n",
      "l2 norm: 907.104932559515\n",
      "l1 norm: 760.6089649351902\n",
      "Rbeta: 907.2193898687771\n",
      "\n",
      "Train set: Avg. loss: 0.000044404, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.5356 109.72333\n",
      "l2 norm: 907.0605721696661\n",
      "l1 norm: 760.5723959007106\n",
      "Rbeta: 907.1747466855024\n",
      "\n",
      "Train set: Avg. loss: 0.000044322, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.55864 109.74231\n",
      "l2 norm: 907.0144504068554\n",
      "l1 norm: 760.534422843597\n",
      "Rbeta: 907.1282965921143\n",
      "\n",
      "Train set: Avg. loss: 0.000044240, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.58169 109.76128\n",
      "l2 norm: 906.9613948891531\n",
      "l1 norm: 760.4907201438659\n",
      "Rbeta: 907.074891361786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000044159, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.60474 109.78021\n",
      "l2 norm: 906.9160701897924\n",
      "l1 norm: 760.4534015164998\n",
      "Rbeta: 907.0293241216358\n",
      "\n",
      "Train set: Avg. loss: 0.000044078, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.62779 109.79908\n",
      "l2 norm: 906.8741757376514\n",
      "l1 norm: 760.4188639877224\n",
      "Rbeta: 906.987148202944\n",
      "\n",
      "Train set: Avg. loss: 0.000043997, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.65085 109.8179\n",
      "l2 norm: 906.829407213109\n",
      "l1 norm: 760.3819251589302\n",
      "Rbeta: 906.9420755884335\n",
      "\n",
      "Train set: Avg. loss: 0.000043916, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.6739 109.83664\n",
      "l2 norm: 906.7817584980861\n",
      "l1 norm: 760.3425799529402\n",
      "Rbeta: 906.8940375633447\n",
      "\n",
      "Train set: Avg. loss: 0.000043836, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.69696 109.855316\n",
      "l2 norm: 906.7258005516243\n",
      "l1 norm: 760.2962844054065\n",
      "Rbeta: 906.8378599376796\n",
      "\n",
      "Train set: Avg. loss: 0.000043756, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.720024 109.87396\n",
      "l2 norm: 906.6757312281602\n",
      "l1 norm: 760.2548633163319\n",
      "Rbeta: 906.787466382358\n",
      "\n",
      "Train set: Avg. loss: 0.000043676, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.74309 109.89255\n",
      "l2 norm: 906.6280405530257\n",
      "l1 norm: 760.2154700821618\n",
      "Rbeta: 906.7394364822418\n",
      "\n",
      "Train set: Avg. loss: 0.000043596, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.76615 109.91115\n",
      "l2 norm: 906.5810843981335\n",
      "l1 norm: 760.1767509244714\n",
      "Rbeta: 906.6921797268519\n",
      "\n",
      "Train set: Avg. loss: 0.000043516, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.78922 109.92975\n",
      "l2 norm: 906.5403605247402\n",
      "l1 norm: 760.1432134479435\n",
      "Rbeta: 906.651099410262\n",
      "\n",
      "Train set: Avg. loss: 0.000043437, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.812294 109.94831\n",
      "l2 norm: 906.487594169065\n",
      "l1 norm: 760.0996581060417\n",
      "Rbeta: 906.5980525529984\n",
      "\n",
      "Train set: Avg. loss: 0.000043358, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.835365 109.966835\n",
      "l2 norm: 906.4367225457968\n",
      "l1 norm: 760.057733763836\n",
      "Rbeta: 906.5468178534528\n",
      "\n",
      "Train set: Avg. loss: 0.000043279, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.858444 109.985344\n",
      "l2 norm: 906.3905414245793\n",
      "l1 norm: 760.0197058346968\n",
      "Rbeta: 906.5002881686403\n",
      "\n",
      "Train set: Avg. loss: 0.000043200, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.88152 110.00379\n",
      "l2 norm: 906.347268509603\n",
      "l1 norm: 759.9841612013533\n",
      "Rbeta: 906.4567511156854\n",
      "\n",
      "Train set: Avg. loss: 0.000043121, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.9046 110.02221\n",
      "l2 norm: 906.3156552969191\n",
      "l1 norm: 759.9584257373075\n",
      "Rbeta: 906.4248463194116\n",
      "\n",
      "Train set: Avg. loss: 0.000043043, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.92768 110.04057\n",
      "l2 norm: 906.2773052182984\n",
      "l1 norm: 759.9270455984247\n",
      "Rbeta: 906.3861559685809\n",
      "\n",
      "Train set: Avg. loss: 0.000042965, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.950775 110.058975\n",
      "l2 norm: 906.2379129800825\n",
      "l1 norm: 759.8948263019629\n",
      "Rbeta: 906.3464339374242\n",
      "\n",
      "Train set: Avg. loss: 0.000042887, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.97386 110.07736\n",
      "l2 norm: 906.1990676065025\n",
      "l1 norm: 759.8631733599836\n",
      "Rbeta: 906.3072307657848\n",
      "\n",
      "Train set: Avg. loss: 0.000042809, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.996956 110.095726\n",
      "l2 norm: 906.1573601238349\n",
      "l1 norm: 759.8291474076254\n",
      "Rbeta: 906.2652198862942\n",
      "\n",
      "Train set: Avg. loss: 0.000042731, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.02005 110.11402\n",
      "l2 norm: 906.1248999448861\n",
      "l1 norm: 759.8029050743893\n",
      "Rbeta: 906.2323789120227\n",
      "\n",
      "Train set: Avg. loss: 0.000042654, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.043144 110.13229\n",
      "l2 norm: 906.0890701297308\n",
      "l1 norm: 759.7738405939799\n",
      "Rbeta: 906.1962848209049\n",
      "\n",
      "Train set: Avg. loss: 0.000042576, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.06624 110.15056\n",
      "l2 norm: 906.052112419334\n",
      "l1 norm: 759.7437550300784\n",
      "Rbeta: 906.1589534556715\n",
      "\n",
      "Train set: Avg. loss: 0.000042499, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.08935 110.16885\n",
      "l2 norm: 906.0135498681608\n",
      "l1 norm: 759.7123158258391\n",
      "Rbeta: 906.120148201476\n",
      "\n",
      "Train set: Avg. loss: 0.000042422, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.11245 110.187164\n",
      "l2 norm: 905.9680384799392\n",
      "l1 norm: 759.675073359767\n",
      "Rbeta: 906.0742675251842\n",
      "\n",
      "Train set: Avg. loss: 0.000042345, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.13556 110.20546\n",
      "l2 norm: 905.9275362821851\n",
      "l1 norm: 759.6420506021923\n",
      "Rbeta: 906.0334669693838\n",
      "\n",
      "Train set: Avg. loss: 0.000042269, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.15866 110.22364\n",
      "l2 norm: 905.8856506327342\n",
      "l1 norm: 759.607822024804\n",
      "Rbeta: 905.9911640844839\n",
      "\n",
      "Train set: Avg. loss: 0.000042192, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.18178 110.24166\n",
      "l2 norm: 905.8383591447837\n",
      "l1 norm: 759.5690389747896\n",
      "Rbeta: 905.9435277895609\n",
      "\n",
      "Train set: Avg. loss: 0.000042117, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.204895 110.25958\n",
      "l2 norm: 905.78523633724\n",
      "l1 norm: 759.525387502999\n",
      "Rbeta: 905.8901348450834\n",
      "\n",
      "Train set: Avg. loss: 0.000042041, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.228004 110.27741\n",
      "l2 norm: 905.7481740837542\n",
      "l1 norm: 759.4951835050416\n",
      "Rbeta: 905.8526822664567\n",
      "\n",
      "Train set: Avg. loss: 0.000041966, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.25112 110.29518\n",
      "l2 norm: 905.7202351706304\n",
      "l1 norm: 759.4725873930242\n",
      "Rbeta: 905.8243222768128\n",
      "\n",
      "Train set: Avg. loss: 0.000041892, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.2738 110.31289\n",
      "l2 norm: 905.6916191874856\n",
      "l1 norm: 759.4494335946504\n",
      "Rbeta: 905.7954664683452\n",
      "\n",
      "Train set: Avg. loss: 0.000041820, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.29507 110.330605\n",
      "l2 norm: 905.6723447644989\n",
      "l1 norm: 759.4341028857311\n",
      "Rbeta: 905.7759331693859\n",
      "\n",
      "Train set: Avg. loss: 0.000041749, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.31634 110.3483\n",
      "l2 norm: 905.653414768954\n",
      "l1 norm: 759.4190275299916\n",
      "Rbeta: 905.7568008524287\n",
      "\n",
      "Train set: Avg. loss: 0.000041677, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.3376 110.36597\n",
      "l2 norm: 905.6293736569062\n",
      "l1 norm: 759.3995891275922\n",
      "Rbeta: 905.7324321620016\n",
      "\n",
      "Train set: Avg. loss: 0.000041606, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.35888 110.38359\n",
      "l2 norm: 905.6050503628823\n",
      "l1 norm: 759.3798747917465\n",
      "Rbeta: 905.7078904181692\n",
      "\n",
      "Train set: Avg. loss: 0.000041535, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.38015 110.401115\n",
      "l2 norm: 905.5782606466684\n",
      "l1 norm: 759.3581562501582\n",
      "Rbeta: 905.6808453073014\n",
      "\n",
      "Train set: Avg. loss: 0.000041465, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.40143 110.41857\n",
      "l2 norm: 905.5513470882763\n",
      "l1 norm: 759.3363420105654\n",
      "Rbeta: 905.6536698957185\n",
      "\n",
      "Train set: Avg. loss: 0.000041395, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.42241 110.436\n",
      "l2 norm: 905.5287659767905\n",
      "l1 norm: 759.3181104701673\n",
      "Rbeta: 905.6307688610132\n",
      "\n",
      "Train set: Avg. loss: 0.000041328, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.44183 110.453445\n",
      "l2 norm: 905.4959712814149\n",
      "l1 norm: 759.2912275951777\n",
      "Rbeta: 905.5978971224221\n",
      "\n",
      "Train set: Avg. loss: 0.000041261, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.46126 110.47087\n",
      "l2 norm: 905.4605808474039\n",
      "l1 norm: 759.2621219635324\n",
      "Rbeta: 905.5623328290316\n",
      "\n",
      "Train set: Avg. loss: 0.000041195, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.48068 110.488304\n",
      "l2 norm: 905.4265652114608\n",
      "l1 norm: 759.2341899926901\n",
      "Rbeta: 905.5281921077884\n",
      "\n",
      "Train set: Avg. loss: 0.000041128, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.500114 110.50574\n",
      "l2 norm: 905.3953935573256\n",
      "l1 norm: 759.2085558835226\n",
      "Rbeta: 905.4968676761683\n",
      "\n",
      "Train set: Avg. loss: 0.000041062, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.51954 110.523155\n",
      "l2 norm: 905.3595570654397\n",
      "l1 norm: 759.1790570617892\n",
      "Rbeta: 905.4607787882321\n",
      "\n",
      "Train set: Avg. loss: 0.000040995, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.53897 110.54059\n",
      "l2 norm: 905.321940280427\n",
      "l1 norm: 759.1480790402813\n",
      "Rbeta: 905.4230391701354\n",
      "\n",
      "Train set: Avg. loss: 0.000040929, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.5584 110.55798\n",
      "l2 norm: 905.2880223197942\n",
      "l1 norm: 759.1201619784645\n",
      "Rbeta: 905.3889968857803\n",
      "\n",
      "Train set: Avg. loss: 0.000040863, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.577835 110.57532\n",
      "l2 norm: 905.2379008463861\n",
      "l1 norm: 759.0786872968704\n",
      "Rbeta: 905.3386397024357\n",
      "\n",
      "Train set: Avg. loss: 0.000040797, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.597275 110.592636\n",
      "l2 norm: 905.1920913560722\n",
      "l1 norm: 759.0408322399219\n",
      "Rbeta: 905.2926708058252\n",
      "\n",
      "Train set: Avg. loss: 0.000040733, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.61594 110.60992\n",
      "l2 norm: 905.1560677277764\n",
      "l1 norm: 759.0112052361236\n",
      "Rbeta: 905.2565730962024\n",
      "\n",
      "Train set: Avg. loss: 0.000040671, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.63351 110.62718\n",
      "l2 norm: 905.1164092314928\n",
      "l1 norm: 758.9785807560585\n",
      "Rbeta: 905.2167581479528\n",
      "\n",
      "Train set: Avg. loss: 0.000040609, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.65109 110.64448\n",
      "l2 norm: 905.0758802537975\n",
      "l1 norm: 758.9452632298551\n",
      "Rbeta: 905.1762057550394\n",
      "\n",
      "Train set: Avg. loss: 0.000040547, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.66867 110.66176\n",
      "l2 norm: 905.0341224372951\n",
      "l1 norm: 758.9108718319521\n",
      "Rbeta: 905.1343765507675\n",
      "\n",
      "Train set: Avg. loss: 0.000040485, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.68626 110.679016\n",
      "l2 norm: 904.9861567742804\n",
      "l1 norm: 758.8712456001755\n",
      "Rbeta: 905.086344869193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000040423, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.70384 110.696266\n",
      "l2 norm: 904.9311237073756\n",
      "l1 norm: 758.8256879035198\n",
      "Rbeta: 905.0312993993044\n",
      "\n",
      "Train set: Avg. loss: 0.000040361, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.72143 110.71354\n",
      "l2 norm: 904.8796468439667\n",
      "l1 norm: 758.7831251489642\n",
      "Rbeta: 904.9796862193689\n",
      "\n",
      "Train set: Avg. loss: 0.000040299, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.73902 110.7308\n",
      "l2 norm: 904.8362601506291\n",
      "l1 norm: 758.7473597108624\n",
      "Rbeta: 904.9362501951988\n",
      "\n",
      "Train set: Avg. loss: 0.000040238, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.75661 110.74804\n",
      "l2 norm: 904.7939618036501\n",
      "l1 norm: 758.7125314385007\n",
      "Rbeta: 904.8938784756662\n",
      "\n",
      "Train set: Avg. loss: 0.000040176, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.77419 110.76524\n",
      "l2 norm: 904.7545863741758\n",
      "l1 norm: 758.6802105707698\n",
      "Rbeta: 904.854412539738\n",
      "\n",
      "Train set: Avg. loss: 0.000040115, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.791794 110.78246\n",
      "l2 norm: 904.724193431126\n",
      "l1 norm: 758.6554022039481\n",
      "Rbeta: 904.8239634040348\n",
      "\n",
      "Train set: Avg. loss: 0.000040054, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.80939 110.799675\n",
      "l2 norm: 904.6952386915841\n",
      "l1 norm: 758.6318116226353\n",
      "Rbeta: 904.7949768747169\n",
      "\n",
      "Train set: Avg. loss: 0.000039993, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.82698 110.81683\n",
      "l2 norm: 904.6673057763247\n",
      "l1 norm: 758.6091463260758\n",
      "Rbeta: 904.7669131020521\n",
      "\n",
      "Train set: Avg. loss: 0.000039932, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.84458 110.833984\n",
      "l2 norm: 904.6336019261311\n",
      "l1 norm: 758.5816949531971\n",
      "Rbeta: 904.7331707001304\n",
      "\n",
      "Train set: Avg. loss: 0.000039871, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.862175 110.85106\n",
      "l2 norm: 904.5965355988862\n",
      "l1 norm: 758.5513873025695\n",
      "Rbeta: 904.6960031775563\n",
      "\n",
      "Train set: Avg. loss: 0.000039810, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.879776 110.868095\n",
      "l2 norm: 904.5568827302305\n",
      "l1 norm: 758.518881493324\n",
      "Rbeta: 904.6562712800278\n",
      "\n",
      "Train set: Avg. loss: 0.000039751, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.89647 110.8851\n",
      "l2 norm: 904.5073203266685\n",
      "l1 norm: 758.4780810808358\n",
      "Rbeta: 904.6067431611145\n",
      "\n",
      "Train set: Avg. loss: 0.000039694, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.91221 110.90204\n",
      "l2 norm: 904.4627811633113\n",
      "l1 norm: 758.4414820239972\n",
      "Rbeta: 904.5621403350059\n",
      "\n",
      "Train set: Avg. loss: 0.000039637, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.92795 110.91897\n",
      "l2 norm: 904.4234540872892\n",
      "l1 norm: 758.4093601414259\n",
      "Rbeta: 904.5229105319444\n",
      "\n",
      "Train set: Avg. loss: 0.000039581, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.94369 110.9359\n",
      "l2 norm: 904.3856773251374\n",
      "l1 norm: 758.3784718453551\n",
      "Rbeta: 904.4850735046996\n",
      "\n",
      "Train set: Avg. loss: 0.000039524, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.95943 110.95279\n",
      "l2 norm: 904.3510171674093\n",
      "l1 norm: 758.3502505906488\n",
      "Rbeta: 904.4504174670648\n",
      "\n",
      "Train set: Avg. loss: 0.000039467, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.975174 110.96965\n",
      "l2 norm: 904.3066971159116\n",
      "l1 norm: 758.3138865072309\n",
      "Rbeta: 904.4061288772439\n",
      "\n",
      "Train set: Avg. loss: 0.000039411, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.99092 110.98644\n",
      "l2 norm: 904.2580739313356\n",
      "l1 norm: 758.2738514496872\n",
      "Rbeta: 904.3574799188724\n",
      "\n",
      "Train set: Avg. loss: 0.000039355, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.00667 111.00319\n",
      "l2 norm: 904.2197883927196\n",
      "l1 norm: 758.2424640005542\n",
      "Rbeta: 904.319240784221\n",
      "\n",
      "Train set: Avg. loss: 0.000039299, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.022415 111.01992\n",
      "l2 norm: 904.1855605718323\n",
      "l1 norm: 758.2144788866096\n",
      "Rbeta: 904.2849511546369\n",
      "\n",
      "Train set: Avg. loss: 0.000039243, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.03817 111.03663\n",
      "l2 norm: 904.1554433695634\n",
      "l1 norm: 758.1899313201127\n",
      "Rbeta: 904.254956796538\n",
      "\n",
      "Train set: Avg. loss: 0.000039187, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.05392 111.05329\n",
      "l2 norm: 904.1292633924801\n",
      "l1 norm: 758.1686930288299\n",
      "Rbeta: 904.2287250332259\n",
      "\n",
      "Train set: Avg. loss: 0.000039131, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.06967 111.06995\n",
      "l2 norm: 904.1080919064533\n",
      "l1 norm: 758.1516487406493\n",
      "Rbeta: 904.2075616639689\n",
      "\n",
      "Train set: Avg. loss: 0.000039075, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.08542 111.08659\n",
      "l2 norm: 904.0893034912062\n",
      "l1 norm: 758.1365587128878\n",
      "Rbeta: 904.1887883031474\n",
      "\n",
      "Train set: Avg. loss: 0.000039020, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.10118 111.10322\n",
      "l2 norm: 904.0700196795309\n",
      "l1 norm: 758.1210247899805\n",
      "Rbeta: 904.1695766790886\n",
      "\n",
      "Train set: Avg. loss: 0.000038964, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.11693 111.11981\n",
      "l2 norm: 904.0447999453907\n",
      "l1 norm: 758.1005688145351\n",
      "Rbeta: 904.1442301653349\n",
      "\n",
      "Train set: Avg. loss: 0.000038909, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.13269 111.136444\n",
      "l2 norm: 904.014197569482\n",
      "l1 norm: 758.075632772966\n",
      "Rbeta: 904.1137002242634\n",
      "\n",
      "Train set: Avg. loss: 0.000038854, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.14844 111.153015\n",
      "l2 norm: 903.9931610759495\n",
      "l1 norm: 758.0586890237136\n",
      "Rbeta: 904.092586200298\n",
      "\n",
      "Train set: Avg. loss: 0.000038798, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.1642 111.16957\n",
      "l2 norm: 903.9841538430129\n",
      "l1 norm: 758.0518708415516\n",
      "Rbeta: 904.083584876456\n",
      "\n",
      "Train set: Avg. loss: 0.000038743, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.17996 111.18613\n",
      "l2 norm: 903.9763258766266\n",
      "l1 norm: 758.0459334068003\n",
      "Rbeta: 904.0757523315184\n",
      "\n",
      "Train set: Avg. loss: 0.000038688, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.195724 111.20268\n",
      "l2 norm: 903.965262115059\n",
      "l1 norm: 758.0372408051189\n",
      "Rbeta: 904.0647025312204\n",
      "\n",
      "Train set: Avg. loss: 0.000038634, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.21149 111.21916\n",
      "l2 norm: 903.9558383392127\n",
      "l1 norm: 758.0298559663456\n",
      "Rbeta: 904.0552845095211\n",
      "\n",
      "Train set: Avg. loss: 0.000038579, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.22725 111.23566\n",
      "l2 norm: 903.9412017832232\n",
      "l1 norm: 758.0180620756132\n",
      "Rbeta: 904.0406049891966\n",
      "\n",
      "Train set: Avg. loss: 0.000038524, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.24301 111.25217\n",
      "l2 norm: 903.9265258953199\n",
      "l1 norm: 758.0062330081465\n",
      "Rbeta: 904.0259376231292\n",
      "\n",
      "Train set: Avg. loss: 0.000038470, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.25878 111.26868\n",
      "l2 norm: 903.9022390481114\n",
      "l1 norm: 757.9863524466762\n",
      "Rbeta: 904.0016857406785\n",
      "\n",
      "Train set: Avg. loss: 0.000038415, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.27455 111.2852\n",
      "l2 norm: 903.8775482303023\n",
      "l1 norm: 757.9661521775712\n",
      "Rbeta: 903.9769986090402\n",
      "\n",
      "Train set: Avg. loss: 0.000038360, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.29031 111.30171\n",
      "l2 norm: 903.8512042387298\n",
      "l1 norm: 757.9446294761665\n",
      "Rbeta: 903.9505938018482\n",
      "\n",
      "Train set: Avg. loss: 0.000038306, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.30608 111.31826\n",
      "l2 norm: 903.8207875924404\n",
      "l1 norm: 757.9197837760272\n",
      "Rbeta: 903.9201734693803\n",
      "\n",
      "Train set: Avg. loss: 0.000038252, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.32185 111.33479\n",
      "l2 norm: 903.7874650117568\n",
      "l1 norm: 757.8925343230575\n",
      "Rbeta: 903.8868746725221\n",
      "\n",
      "Train set: Avg. loss: 0.000038198, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.33763 111.35132\n",
      "l2 norm: 903.7529889290157\n",
      "l1 norm: 757.8642514588515\n",
      "Rbeta: 903.8524244583917\n",
      "\n",
      "Train set: Avg. loss: 0.000038143, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.3534 111.367836\n",
      "l2 norm: 903.722576303404\n",
      "l1 norm: 757.8392948136575\n",
      "Rbeta: 903.8219684520142\n",
      "\n",
      "Train set: Avg. loss: 0.000038089, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.36917 111.38439\n",
      "l2 norm: 903.6967432636476\n",
      "l1 norm: 757.8181773117303\n",
      "Rbeta: 903.7961428282775\n",
      "\n",
      "Train set: Avg. loss: 0.000038035, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.38495 111.400955\n",
      "l2 norm: 903.6667500240062\n",
      "l1 norm: 757.7935945158509\n",
      "Rbeta: 903.7661175779706\n",
      "\n",
      "Train set: Avg. loss: 0.000037981, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.40073 111.41748\n",
      "l2 norm: 903.629595364854\n",
      "l1 norm: 757.7630971811676\n",
      "Rbeta: 903.729001682942\n",
      "\n",
      "Train set: Avg. loss: 0.000037927, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.416504 111.43396\n",
      "l2 norm: 903.5933877362037\n",
      "l1 norm: 757.7333894392791\n",
      "Rbeta: 903.6927632681869\n",
      "\n",
      "Train set: Avg. loss: 0.000037874, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.43228 111.45042\n",
      "l2 norm: 903.5567485749318\n",
      "l1 norm: 757.7032400113857\n",
      "Rbeta: 903.6560809347268\n",
      "\n",
      "Train set: Avg. loss: 0.000037820, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.44807 111.46686\n",
      "l2 norm: 903.5196079719155\n",
      "l1 norm: 757.6726875900633\n",
      "Rbeta: 903.6189697135829\n",
      "\n",
      "Train set: Avg. loss: 0.000037767, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.463844 111.48329\n",
      "l2 norm: 903.4868150846568\n",
      "l1 norm: 757.6457339414342\n",
      "Rbeta: 903.5861869440625\n",
      "\n",
      "Train set: Avg. loss: 0.000037713, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.47963 111.49971\n",
      "l2 norm: 903.4491320478357\n",
      "l1 norm: 757.6146623930764\n",
      "Rbeta: 903.5484795050687\n",
      "\n",
      "Train set: Avg. loss: 0.000037660, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.49541 111.5161\n",
      "l2 norm: 903.4112841634508\n",
      "l1 norm: 757.5834708591728\n",
      "Rbeta: 903.5106056333615\n",
      "\n",
      "Train set: Avg. loss: 0.000037607, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.5112 111.5324\n",
      "l2 norm: 903.3800353066587\n",
      "l1 norm: 757.5577568173528\n",
      "Rbeta: 903.4794050311748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000037554, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.52698 111.548706\n",
      "l2 norm: 903.3484052732424\n",
      "l1 norm: 757.5317557114736\n",
      "Rbeta: 903.447703547394\n",
      "\n",
      "Train set: Avg. loss: 0.000037501, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.54277 111.564964\n",
      "l2 norm: 903.3168679309305\n",
      "l1 norm: 757.5058412191686\n",
      "Rbeta: 903.4161092834586\n",
      "\n",
      "Train set: Avg. loss: 0.000037449, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.558556 111.58121\n",
      "l2 norm: 903.2831192297087\n",
      "l1 norm: 757.4780190194591\n",
      "Rbeta: 903.3824272468682\n",
      "\n",
      "Train set: Avg. loss: 0.000037396, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.57435 111.59744\n",
      "l2 norm: 903.243295703692\n",
      "l1 norm: 757.4450894274448\n",
      "Rbeta: 903.3425019068927\n",
      "\n",
      "Train set: Avg. loss: 0.000037343, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.59014 111.61369\n",
      "l2 norm: 903.2060068189775\n",
      "l1 norm: 757.4142549043996\n",
      "Rbeta: 903.3052015989723\n",
      "\n",
      "Train set: Avg. loss: 0.000037291, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.60593 111.62995\n",
      "l2 norm: 903.1710307557524\n",
      "l1 norm: 757.3854155526109\n",
      "Rbeta: 903.2701785659827\n",
      "\n",
      "Train set: Avg. loss: 0.000037238, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.62172 111.6462\n",
      "l2 norm: 903.1437419074296\n",
      "l1 norm: 757.3630141601815\n",
      "Rbeta: 903.2428952843065\n",
      "\n",
      "Train set: Avg. loss: 0.000037186, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.63751 111.66236\n",
      "l2 norm: 903.1169840740426\n",
      "l1 norm: 757.3410244093027\n",
      "Rbeta: 903.2160739401835\n",
      "\n",
      "Train set: Avg. loss: 0.000037137, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.65169 111.678505\n",
      "l2 norm: 903.0861015610327\n",
      "l1 norm: 757.315542387796\n",
      "Rbeta: 903.185390226221\n",
      "\n",
      "Train set: Avg. loss: 0.000037088, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.66561 111.69462\n",
      "l2 norm: 903.0594794859835\n",
      "l1 norm: 757.2937070509228\n",
      "Rbeta: 903.1587614883355\n",
      "\n",
      "Train set: Avg. loss: 0.000037039, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.67954 111.71074\n",
      "l2 norm: 903.0328166052769\n",
      "l1 norm: 757.2718095008787\n",
      "Rbeta: 903.1322114857652\n",
      "\n",
      "Train set: Avg. loss: 0.000036990, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.693474 111.72691\n",
      "l2 norm: 903.0075478288785\n",
      "l1 norm: 757.2510498834022\n",
      "Rbeta: 903.1070083441244\n",
      "\n",
      "Train set: Avg. loss: 0.000036941, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.707405 111.7431\n",
      "l2 norm: 902.9765841335168\n",
      "l1 norm: 757.2255341804555\n",
      "Rbeta: 903.0760950944357\n",
      "\n",
      "Train set: Avg. loss: 0.000036892, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.72134 111.75932\n",
      "l2 norm: 902.9393866649434\n",
      "l1 norm: 757.1948509445481\n",
      "Rbeta: 903.0389792359466\n",
      "\n",
      "Train set: Avg. loss: 0.000036843, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.73527 111.77553\n",
      "l2 norm: 902.9070920714753\n",
      "l1 norm: 757.1682942918599\n",
      "Rbeta: 903.0068357863174\n",
      "\n",
      "Train set: Avg. loss: 0.000036795, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.7492 111.79169\n",
      "l2 norm: 902.8745122982709\n",
      "l1 norm: 757.1415103528379\n",
      "Rbeta: 902.9743433305758\n",
      "\n",
      "Train set: Avg. loss: 0.000036746, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.76313 111.80776\n",
      "l2 norm: 902.8442933061702\n",
      "l1 norm: 757.1166448159893\n",
      "Rbeta: 902.9441311225888\n",
      "\n",
      "Train set: Avg. loss: 0.000036698, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.77707 111.82383\n",
      "l2 norm: 902.8079943326323\n",
      "l1 norm: 757.086663642521\n",
      "Rbeta: 902.9079266938265\n",
      "\n",
      "Train set: Avg. loss: 0.000036650, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.79101 111.83984\n",
      "l2 norm: 902.7657389050606\n",
      "l1 norm: 757.0516661087628\n",
      "Rbeta: 902.8657742131618\n",
      "\n",
      "Train set: Avg. loss: 0.000036602, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.80495 111.855835\n",
      "l2 norm: 902.7237907589366\n",
      "l1 norm: 757.0169605707354\n",
      "Rbeta: 902.8238901669021\n",
      "\n",
      "Train set: Avg. loss: 0.000036554, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.81888 111.871826\n",
      "l2 norm: 902.6873122093173\n",
      "l1 norm: 756.9868265392679\n",
      "Rbeta: 902.7874693951545\n",
      "\n",
      "Train set: Avg. loss: 0.000036506, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.832825 111.8878\n",
      "l2 norm: 902.6527092870213\n",
      "l1 norm: 756.9582913495137\n",
      "Rbeta: 902.7529871069074\n",
      "\n",
      "Train set: Avg. loss: 0.000036458, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.84676 111.90376\n",
      "l2 norm: 902.616614831346\n",
      "l1 norm: 756.9284772310851\n",
      "Rbeta: 902.7169068000356\n",
      "\n",
      "Train set: Avg. loss: 0.000036411, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.85988 111.91968\n",
      "l2 norm: 902.5825251469279\n",
      "l1 norm: 756.900395995594\n",
      "Rbeta: 902.6829550420637\n",
      "\n",
      "Train set: Avg. loss: 0.000036366, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.87195 111.93556\n",
      "l2 norm: 902.546780648633\n",
      "l1 norm: 756.8709153765133\n",
      "Rbeta: 902.6473902299127\n",
      "\n",
      "Train set: Avg. loss: 0.000036322, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.88402 111.951416\n",
      "l2 norm: 902.5153390491494\n",
      "l1 norm: 756.8450655064223\n",
      "Rbeta: 902.6160554159405\n",
      "\n",
      "Train set: Avg. loss: 0.000036277, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.896095 111.96722\n",
      "l2 norm: 902.4897981914409\n",
      "l1 norm: 756.8241994214159\n",
      "Rbeta: 902.5907545179667\n",
      "\n",
      "Train set: Avg. loss: 0.000036233, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.90817 111.982956\n",
      "l2 norm: 902.4657966780048\n",
      "l1 norm: 756.8045776694125\n",
      "Rbeta: 902.5669156093771\n",
      "\n",
      "Train set: Avg. loss: 0.000036189, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.92024 111.998665\n",
      "l2 norm: 902.4398965249244\n",
      "l1 norm: 756.7833623742656\n",
      "Rbeta: 902.5412010706829\n",
      "\n",
      "Train set: Avg. loss: 0.000036145, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.93232 112.01435\n",
      "l2 norm: 902.4130834635671\n",
      "l1 norm: 756.761345846398\n",
      "Rbeta: 902.5144908627551\n",
      "\n",
      "Train set: Avg. loss: 0.000036101, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.9444 112.03003\n",
      "l2 norm: 902.388448040711\n",
      "l1 norm: 756.7411582692376\n",
      "Rbeta: 902.4900408274649\n",
      "\n",
      "Train set: Avg. loss: 0.000036057, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.95647 112.04567\n",
      "l2 norm: 902.3626565407911\n",
      "l1 norm: 756.7200253473477\n",
      "Rbeta: 902.464367960451\n",
      "\n",
      "Train set: Avg. loss: 0.000036013, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.968544 112.06131\n",
      "l2 norm: 902.3435533129863\n",
      "l1 norm: 756.7045114559385\n",
      "Rbeta: 902.4454562428942\n",
      "\n",
      "Train set: Avg. loss: 0.000035969, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.98063 112.0769\n",
      "l2 norm: 902.323342882475\n",
      "l1 norm: 756.6880850509168\n",
      "Rbeta: 902.4254373547884\n",
      "\n",
      "Train set: Avg. loss: 0.000035925, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.99271 112.09245\n",
      "l2 norm: 902.3025722333675\n",
      "l1 norm: 756.6711395080649\n",
      "Rbeta: 902.4048533175942\n",
      "\n",
      "Train set: Avg. loss: 0.000035882, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.00479 112.10797\n",
      "l2 norm: 902.2757396946779\n",
      "l1 norm: 756.649101513571\n",
      "Rbeta: 902.3781692810184\n",
      "\n",
      "Train set: Avg. loss: 0.000035839, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.01686 112.12347\n",
      "l2 norm: 902.2438551214747\n",
      "l1 norm: 756.622841295629\n",
      "Rbeta: 902.3463687803028\n",
      "\n",
      "Train set: Avg. loss: 0.000035795, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.028946 112.13887\n",
      "l2 norm: 902.2069694926037\n",
      "l1 norm: 756.5923670967175\n",
      "Rbeta: 902.3096668092061\n",
      "\n",
      "Train set: Avg. loss: 0.000035752, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.04103 112.15423\n",
      "l2 norm: 902.1747624210466\n",
      "l1 norm: 756.5658211554772\n",
      "Rbeta: 902.2776090196141\n",
      "\n",
      "Train set: Avg. loss: 0.000035709, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.053116 112.16957\n",
      "l2 norm: 902.14464542502\n",
      "l1 norm: 756.5410474080878\n",
      "Rbeta: 902.2476451074134\n",
      "\n",
      "Train set: Avg. loss: 0.000035666, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.06516 112.184944\n",
      "l2 norm: 902.1205415099375\n",
      "l1 norm: 756.5213623576489\n",
      "Rbeta: 902.2237137373395\n",
      "\n",
      "Train set: Avg. loss: 0.000035623, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.07707 112.2003\n",
      "l2 norm: 902.0968569335672\n",
      "l1 norm: 756.5021250285313\n",
      "Rbeta: 902.2001540760903\n",
      "\n",
      "Train set: Avg. loss: 0.000035581, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.08899 112.21569\n",
      "l2 norm: 902.0725408228063\n",
      "l1 norm: 756.4823946327755\n",
      "Rbeta: 902.1760756113324\n",
      "\n",
      "Train set: Avg. loss: 0.000035538, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.10091 112.23107\n",
      "l2 norm: 902.0496468191604\n",
      "l1 norm: 756.463799699191\n",
      "Rbeta: 902.1532498432823\n",
      "\n",
      "Train set: Avg. loss: 0.000035495, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.11282 112.246475\n",
      "l2 norm: 902.0287615917539\n",
      "l1 norm: 756.4468526735645\n",
      "Rbeta: 902.132589676139\n",
      "\n",
      "Train set: Avg. loss: 0.000035453, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.12473 112.26188\n",
      "l2 norm: 902.0100572869055\n",
      "l1 norm: 756.4316817396016\n",
      "Rbeta: 902.1140096440012\n",
      "\n",
      "Train set: Avg. loss: 0.000035410, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.13665 112.277306\n",
      "l2 norm: 901.9884548910586\n",
      "l1 norm: 756.4140943078819\n",
      "Rbeta: 902.0926018802269\n",
      "\n",
      "Train set: Avg. loss: 0.000035368, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.14857 112.29268\n",
      "l2 norm: 901.9654116925468\n",
      "l1 norm: 756.3953342139304\n",
      "Rbeta: 902.0697545236665\n",
      "\n",
      "Train set: Avg. loss: 0.000035325, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.160484 112.308014\n",
      "l2 norm: 901.9357255325979\n",
      "l1 norm: 756.3709901980055\n",
      "Rbeta: 902.0402125737822\n",
      "\n",
      "Train set: Avg. loss: 0.000035283, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.17241 112.32332\n",
      "l2 norm: 901.898706783539\n",
      "l1 norm: 756.3404903167559\n",
      "Rbeta: 902.0033303726381\n",
      "\n",
      "Train set: Avg. loss: 0.000035241, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.18433 112.3386\n",
      "l2 norm: 901.8623096803592\n",
      "l1 norm: 756.3105154311422\n",
      "Rbeta: 901.9671642664005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000035199, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.19625 112.353836\n",
      "l2 norm: 901.8254072247054\n",
      "l1 norm: 756.280132763517\n",
      "Rbeta: 901.9303784729202\n",
      "\n",
      "Train set: Avg. loss: 0.000035157, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.208176 112.36906\n",
      "l2 norm: 901.7884962122624\n",
      "l1 norm: 756.2497910821714\n",
      "Rbeta: 901.8936668003857\n",
      "\n",
      "Train set: Avg. loss: 0.000035115, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.22009 112.38431\n",
      "l2 norm: 901.7517306492965\n",
      "l1 norm: 756.2196050773827\n",
      "Rbeta: 901.856967114247\n",
      "\n",
      "Train set: Avg. loss: 0.000035073, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.23202 112.39953\n",
      "l2 norm: 901.7144246346493\n",
      "l1 norm: 756.188927099432\n",
      "Rbeta: 901.819834560715\n",
      "\n",
      "Train set: Avg. loss: 0.000035031, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.24394 112.41476\n",
      "l2 norm: 901.6762909774121\n",
      "l1 norm: 756.1575300396754\n",
      "Rbeta: 901.7818941280093\n",
      "\n",
      "Train set: Avg. loss: 0.000034990, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.25587 112.42999\n",
      "l2 norm: 901.6318943296363\n",
      "l1 norm: 756.1208898133323\n",
      "Rbeta: 901.7376333793625\n",
      "\n",
      "Train set: Avg. loss: 0.000034948, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.26779 112.44524\n",
      "l2 norm: 901.5934154524161\n",
      "l1 norm: 756.089187369158\n",
      "Rbeta: 901.6993515227746\n",
      "\n",
      "Train set: Avg. loss: 0.000034906, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.27972 112.46047\n",
      "l2 norm: 901.563472394311\n",
      "l1 norm: 756.0646693488814\n",
      "Rbeta: 901.6695432226265\n",
      "\n",
      "Train set: Avg. loss: 0.000034864, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.29164 112.47569\n",
      "l2 norm: 901.5272396642991\n",
      "l1 norm: 756.0348372694949\n",
      "Rbeta: 901.6334549905541\n",
      "\n",
      "Train set: Avg. loss: 0.000034823, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.303566 112.49089\n",
      "l2 norm: 901.498669405389\n",
      "l1 norm: 756.0113933035502\n",
      "Rbeta: 901.6050715402351\n",
      "\n",
      "Train set: Avg. loss: 0.000034781, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.3155 112.5061\n",
      "l2 norm: 901.474528823717\n",
      "l1 norm: 755.9916540411368\n",
      "Rbeta: 901.5810651320488\n",
      "\n",
      "Train set: Avg. loss: 0.000034740, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.32742 112.521255\n",
      "l2 norm: 901.4545811315268\n",
      "l1 norm: 755.9754125687052\n",
      "Rbeta: 901.5612474331747\n",
      "\n",
      "Train set: Avg. loss: 0.000034699, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.339355 112.536415\n",
      "l2 norm: 901.4333056772728\n",
      "l1 norm: 755.9580346210125\n",
      "Rbeta: 901.5401253802905\n",
      "\n",
      "Train set: Avg. loss: 0.000034658, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.35065 112.551544\n",
      "l2 norm: 901.40786263474\n",
      "l1 norm: 755.9371609206328\n",
      "Rbeta: 901.5149248517806\n",
      "\n",
      "Train set: Avg. loss: 0.000034620, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.3607 112.566605\n",
      "l2 norm: 901.381520018818\n",
      "l1 norm: 755.9155377906645\n",
      "Rbeta: 901.488821162694\n",
      "\n",
      "Train set: Avg. loss: 0.000034582, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.370766 112.58168\n",
      "l2 norm: 901.3472531062386\n",
      "l1 norm: 755.8873352825592\n",
      "Rbeta: 901.4548289336278\n",
      "\n",
      "Train set: Avg. loss: 0.000034546, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.3792 112.59679\n",
      "l2 norm: 901.3161097570166\n",
      "l1 norm: 755.8617947583048\n",
      "Rbeta: 901.4239880931274\n",
      "\n",
      "Train set: Avg. loss: 0.000034511, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.38739 112.6119\n",
      "l2 norm: 901.2811347599085\n",
      "l1 norm: 755.8330211356925\n",
      "Rbeta: 901.389507107657\n",
      "\n",
      "Train set: Avg. loss: 0.000034475, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.39558 112.62701\n",
      "l2 norm: 901.2444375159143\n",
      "l1 norm: 755.8028429833296\n",
      "Rbeta: 901.3531127076491\n",
      "\n",
      "Train set: Avg. loss: 0.000034440, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.40376 112.64212\n",
      "l2 norm: 901.2099083265465\n",
      "l1 norm: 755.7744694356993\n",
      "Rbeta: 901.3189715032839\n",
      "\n",
      "Train set: Avg. loss: 0.000034405, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.41195 112.65723\n",
      "l2 norm: 901.1718685827996\n",
      "l1 norm: 755.7431263998772\n",
      "Rbeta: 901.28135158481\n",
      "\n",
      "Train set: Avg. loss: 0.000034370, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.420135 112.67234\n",
      "l2 norm: 901.1390860004276\n",
      "l1 norm: 755.7162022607625\n",
      "Rbeta: 901.2489124996538\n",
      "\n",
      "Train set: Avg. loss: 0.000034334, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.42832 112.68742\n",
      "l2 norm: 901.1008254343948\n",
      "l1 norm: 755.6847343956476\n",
      "Rbeta: 901.2110882935525\n",
      "\n",
      "Train set: Avg. loss: 0.000034299, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.43651 112.70247\n",
      "l2 norm: 901.066924150217\n",
      "l1 norm: 755.65686389044\n",
      "Rbeta: 901.1775491818248\n",
      "\n",
      "Train set: Avg. loss: 0.000034264, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.444695 112.717514\n",
      "l2 norm: 901.0373634924409\n",
      "l1 norm: 755.6325434795021\n",
      "Rbeta: 901.1483225397194\n",
      "\n",
      "Train set: Avg. loss: 0.000034229, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.45288 112.73256\n",
      "l2 norm: 901.0087115721718\n",
      "l1 norm: 755.6089398771496\n",
      "Rbeta: 901.1200915901693\n",
      "\n",
      "Train set: Avg. loss: 0.000034194, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.46107 112.747604\n",
      "l2 norm: 900.9874447771388\n",
      "l1 norm: 755.5915355734253\n",
      "Rbeta: 901.0992191127514\n",
      "\n",
      "Train set: Avg. loss: 0.000034159, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.46926 112.76265\n",
      "l2 norm: 900.9672902898992\n",
      "l1 norm: 755.5750644001241\n",
      "Rbeta: 901.0794333936259\n",
      "\n",
      "Train set: Avg. loss: 0.000034124, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.47745 112.77768\n",
      "l2 norm: 900.9454655205149\n",
      "l1 norm: 755.5571656882812\n",
      "Rbeta: 901.0580196415362\n",
      "\n",
      "Train set: Avg. loss: 0.000034090, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.485634 112.792725\n",
      "l2 norm: 900.9215797827186\n",
      "l1 norm: 755.537508034693\n",
      "Rbeta: 901.0344887656759\n",
      "\n",
      "Train set: Avg. loss: 0.000034055, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.49382 112.80776\n",
      "l2 norm: 900.8999066775692\n",
      "l1 norm: 755.5196869265768\n",
      "Rbeta: 901.013195153553\n",
      "\n",
      "Train set: Avg. loss: 0.000034020, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.502014 112.82281\n",
      "l2 norm: 900.8743424861675\n",
      "l1 norm: 755.4986287093703\n",
      "Rbeta: 900.9881132107375\n",
      "\n",
      "Train set: Avg. loss: 0.000033985, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.51021 112.83783\n",
      "l2 norm: 900.8433473406192\n",
      "l1 norm: 755.4730000810872\n",
      "Rbeta: 900.9574668340232\n",
      "\n",
      "Train set: Avg. loss: 0.000033951, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.518394 112.85285\n",
      "l2 norm: 900.8171344611809\n",
      "l1 norm: 755.4513832816641\n",
      "Rbeta: 900.9316153000207\n",
      "\n",
      "Train set: Avg. loss: 0.000033916, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.52659 112.867874\n",
      "l2 norm: 900.7971459143733\n",
      "l1 norm: 755.435003592034\n",
      "Rbeta: 900.9120864268664\n",
      "\n",
      "Train set: Avg. loss: 0.000033881, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.534775 112.8829\n",
      "l2 norm: 900.7789439417826\n",
      "l1 norm: 755.4201434827592\n",
      "Rbeta: 900.8941788614118\n",
      "\n",
      "Train set: Avg. loss: 0.000033847, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.54297 112.89792\n",
      "l2 norm: 900.7533014653146\n",
      "l1 norm: 755.3990521912663\n",
      "Rbeta: 900.869005476333\n",
      "\n",
      "Train set: Avg. loss: 0.000033812, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.55116 112.91291\n",
      "l2 norm: 900.7287572067777\n",
      "l1 norm: 755.3788988776716\n",
      "Rbeta: 900.8448566839404\n",
      "\n",
      "Train set: Avg. loss: 0.000033778, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.55936 112.927925\n",
      "l2 norm: 900.7026259317119\n",
      "l1 norm: 755.3573837053634\n",
      "Rbeta: 900.8190837531431\n",
      "\n",
      "Train set: Avg. loss: 0.000033743, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.56755 112.942894\n",
      "l2 norm: 900.6799997939354\n",
      "l1 norm: 755.338831176885\n",
      "Rbeta: 900.7968349216476\n",
      "\n",
      "Train set: Avg. loss: 0.000033709, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.575745 112.95787\n",
      "l2 norm: 900.6552527500668\n",
      "l1 norm: 755.3185294320617\n",
      "Rbeta: 900.7725346367271\n",
      "\n",
      "Train set: Avg. loss: 0.000033675, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.58394 112.972824\n",
      "l2 norm: 900.6343634764022\n",
      "l1 norm: 755.3014401019122\n",
      "Rbeta: 900.7520285046058\n",
      "\n",
      "Train set: Avg. loss: 0.000033640, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.59213 112.98773\n",
      "l2 norm: 900.619951247297\n",
      "l1 norm: 755.2897632264778\n",
      "Rbeta: 900.7380231957011\n",
      "\n",
      "Train set: Avg. loss: 0.000033606, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.60033 113.00261\n",
      "l2 norm: 900.6026168745055\n",
      "l1 norm: 755.2756498963041\n",
      "Rbeta: 900.7210736807959\n",
      "\n",
      "Train set: Avg. loss: 0.000033572, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.60852 113.01744\n",
      "l2 norm: 900.5865836402454\n",
      "l1 norm: 755.2626756269156\n",
      "Rbeta: 900.7054092452286\n",
      "\n",
      "Train set: Avg. loss: 0.000033538, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.616714 113.032265\n",
      "l2 norm: 900.570528895871\n",
      "l1 norm: 755.2497289670292\n",
      "Rbeta: 900.6898028879879\n",
      "\n",
      "Train set: Avg. loss: 0.000033504, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.624916 113.04711\n",
      "l2 norm: 900.5479849417233\n",
      "l1 norm: 755.231350052103\n",
      "Rbeta: 900.6676391429206\n",
      "\n",
      "Train set: Avg. loss: 0.000033470, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.63311 113.061935\n",
      "l2 norm: 900.5280642324811\n",
      "l1 norm: 755.2151946807548\n",
      "Rbeta: 900.6480715190506\n",
      "\n",
      "Train set: Avg. loss: 0.000033437, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.64131 113.076744\n",
      "l2 norm: 900.5051145005168\n",
      "l1 norm: 755.1965078854905\n",
      "Rbeta: 900.6255239740202\n",
      "\n",
      "Train set: Avg. loss: 0.000033403, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.6495 113.09157\n",
      "l2 norm: 900.4749391004269\n",
      "l1 norm: 755.1717458743831\n",
      "Rbeta: 900.5957113208619\n",
      "\n",
      "Train set: Avg. loss: 0.000033369, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.6577 113.106384\n",
      "l2 norm: 900.4440417900872\n",
      "l1 norm: 755.1463565180784\n",
      "Rbeta: 900.5652632266325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000033335, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.6659 113.12117\n",
      "l2 norm: 900.4136027072227\n",
      "l1 norm: 755.1213110566719\n",
      "Rbeta: 900.5352425426905\n",
      "\n",
      "Train set: Avg. loss: 0.000033302, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.6741 113.13592\n",
      "l2 norm: 900.3881011770972\n",
      "l1 norm: 755.1004346930895\n",
      "Rbeta: 900.5101429081789\n",
      "\n",
      "Train set: Avg. loss: 0.000033268, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.6823 113.15065\n",
      "l2 norm: 900.3646712667781\n",
      "l1 norm: 755.081325103436\n",
      "Rbeta: 900.4870954939051\n",
      "\n",
      "Train set: Avg. loss: 0.000033235, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.6905 113.16532\n",
      "l2 norm: 900.3476808292622\n",
      "l1 norm: 755.0676216471636\n",
      "Rbeta: 900.4704834564538\n",
      "\n",
      "Train set: Avg. loss: 0.000033201, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.6987 113.18001\n",
      "l2 norm: 900.3322998372943\n",
      "l1 norm: 755.0552560097033\n",
      "Rbeta: 900.4555091635876\n",
      "\n",
      "Train set: Avg. loss: 0.000033168, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.7069 113.194725\n",
      "l2 norm: 900.3114326940907\n",
      "l1 norm: 755.0382832211817\n",
      "Rbeta: 900.435078929584\n",
      "\n",
      "Train set: Avg. loss: 0.000033134, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.7151 113.209404\n",
      "l2 norm: 900.2924530803463\n",
      "l1 norm: 755.0228833675692\n",
      "Rbeta: 900.4164198982495\n",
      "\n",
      "Train set: Avg. loss: 0.000033101, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.723305 113.22405\n",
      "l2 norm: 900.2678908542728\n",
      "l1 norm: 755.0028152460388\n",
      "Rbeta: 900.3922852837262\n",
      "\n",
      "Train set: Avg. loss: 0.000033068, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.7315 113.23868\n",
      "l2 norm: 900.2423606287599\n",
      "l1 norm: 754.9818819679269\n",
      "Rbeta: 900.3671668507321\n",
      "\n",
      "Train set: Avg. loss: 0.000033035, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.7397 113.2533\n",
      "l2 norm: 900.2146931648986\n",
      "l1 norm: 754.9591604153943\n",
      "Rbeta: 900.3398882409045\n",
      "\n",
      "Train set: Avg. loss: 0.000033002, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.7479 113.26791\n",
      "l2 norm: 900.1902785663784\n",
      "l1 norm: 754.9392090772714\n",
      "Rbeta: 900.3158515420704\n",
      "\n",
      "Train set: Avg. loss: 0.000032969, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.7561 113.282455\n",
      "l2 norm: 900.1715169055553\n",
      "l1 norm: 754.9239915191221\n",
      "Rbeta: 900.2974846117086\n",
      "\n",
      "Train set: Avg. loss: 0.000032936, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.76431 113.297005\n",
      "l2 norm: 900.1533325868434\n",
      "l1 norm: 754.9092691447477\n",
      "Rbeta: 900.2797177613529\n",
      "\n",
      "Train set: Avg. loss: 0.000032903, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.772514 113.31155\n",
      "l2 norm: 900.1313073777239\n",
      "l1 norm: 754.8912952434764\n",
      "Rbeta: 900.2581115679142\n",
      "\n",
      "Train set: Avg. loss: 0.000032871, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.77961 113.326065\n",
      "l2 norm: 900.1053156853526\n",
      "l1 norm: 754.8699645956019\n",
      "Rbeta: 900.2325500965989\n",
      "\n",
      "Train set: Avg. loss: 0.000032841, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.785904 113.34059\n",
      "l2 norm: 900.0736361359709\n",
      "l1 norm: 754.843862806332\n",
      "Rbeta: 900.2014051513801\n",
      "\n",
      "Train set: Avg. loss: 0.000032811, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.79206 113.355095\n",
      "l2 norm: 900.036628046216\n",
      "l1 norm: 754.8133070484197\n",
      "Rbeta: 900.164945089895\n",
      "\n",
      "Train set: Avg. loss: 0.000032782, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.79822 113.36959\n",
      "l2 norm: 899.9985420263696\n",
      "l1 norm: 754.7818252179335\n",
      "Rbeta: 900.1274127322218\n",
      "\n",
      "Train set: Avg. loss: 0.000032752, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.804375 113.384056\n",
      "l2 norm: 899.9620201307212\n",
      "l1 norm: 754.7516422761718\n",
      "Rbeta: 900.0913826468353\n",
      "\n",
      "Train set: Avg. loss: 0.000032722, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.81053 113.39849\n",
      "l2 norm: 899.9292268370771\n",
      "l1 norm: 754.7245577679414\n",
      "Rbeta: 900.0590788278114\n",
      "\n",
      "Train set: Avg. loss: 0.000032693, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.81668 113.41289\n",
      "l2 norm: 899.8959318806227\n",
      "l1 norm: 754.6970529656669\n",
      "Rbeta: 900.026335805309\n",
      "\n",
      "Train set: Avg. loss: 0.000032663, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.822845 113.42726\n",
      "l2 norm: 899.8611898332747\n",
      "l1 norm: 754.6683106063355\n",
      "Rbeta: 899.9921340215103\n",
      "\n",
      "Train set: Avg. loss: 0.000032634, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.82901 113.4416\n",
      "l2 norm: 899.8292091237364\n",
      "l1 norm: 754.6418636342128\n",
      "Rbeta: 899.9606755725122\n",
      "\n",
      "Train set: Avg. loss: 0.000032604, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.83517 113.45596\n",
      "l2 norm: 899.7971975062745\n",
      "l1 norm: 754.6154474281209\n",
      "Rbeta: 899.9292474498536\n",
      "\n",
      "Train set: Avg. loss: 0.000032575, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.841324 113.47031\n",
      "l2 norm: 899.7629733851967\n",
      "l1 norm: 754.5871986451551\n",
      "Rbeta: 899.8954813401054\n",
      "\n",
      "Train set: Avg. loss: 0.000032546, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.84748 113.48461\n",
      "l2 norm: 899.7292440546864\n",
      "l1 norm: 754.5594223892315\n",
      "Rbeta: 899.8623033640268\n",
      "\n",
      "Train set: Avg. loss: 0.000032516, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.85364 113.49887\n",
      "l2 norm: 899.6977199508711\n",
      "l1 norm: 754.5335040065424\n",
      "Rbeta: 899.8312723289152\n",
      "\n",
      "Train set: Avg. loss: 0.000032487, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.859795 113.51313\n",
      "l2 norm: 899.6684352984149\n",
      "l1 norm: 754.5094692173964\n",
      "Rbeta: 899.8025175713051\n",
      "\n",
      "Train set: Avg. loss: 0.000032458, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.86596 113.52739\n",
      "l2 norm: 899.6407795335765\n",
      "l1 norm: 754.4868154153421\n",
      "Rbeta: 899.7753899946902\n",
      "\n",
      "Train set: Avg. loss: 0.000032429, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.872116 113.54162\n",
      "l2 norm: 899.6093439527058\n",
      "l1 norm: 754.4609982871783\n",
      "Rbeta: 899.7444744751845\n",
      "\n",
      "Train set: Avg. loss: 0.000032400, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.87828 113.55581\n",
      "l2 norm: 899.5815962767716\n",
      "l1 norm: 754.4382694897721\n",
      "Rbeta: 899.717276838496\n",
      "\n",
      "Train set: Avg. loss: 0.000032371, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.884445 113.57001\n",
      "l2 norm: 899.5586493317039\n",
      "l1 norm: 754.4195377633577\n",
      "Rbeta: 899.6948973284851\n",
      "\n",
      "Train set: Avg. loss: 0.000032342, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.8906 113.5842\n",
      "l2 norm: 899.5358572383071\n",
      "l1 norm: 754.4008964195734\n",
      "Rbeta: 899.6726160367117\n",
      "\n",
      "Train set: Avg. loss: 0.000032313, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.89676 113.59832\n",
      "l2 norm: 899.5185455310697\n",
      "l1 norm: 754.3868700437829\n",
      "Rbeta: 899.6558757957351\n",
      "\n",
      "Train set: Avg. loss: 0.000032284, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.90292 113.61243\n",
      "l2 norm: 899.5041334239577\n",
      "l1 norm: 754.3752636549561\n",
      "Rbeta: 899.6419840408033\n",
      "\n",
      "Train set: Avg. loss: 0.000032255, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.90909 113.62652\n",
      "l2 norm: 899.4872763895909\n",
      "l1 norm: 754.3615583964454\n",
      "Rbeta: 899.6256674454193\n",
      "\n",
      "Train set: Avg. loss: 0.000032227, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.91525 113.640625\n",
      "l2 norm: 899.4673422987122\n",
      "l1 norm: 754.3452835223263\n",
      "Rbeta: 899.6062796106934\n",
      "\n",
      "Train set: Avg. loss: 0.000032198, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.92141 113.65471\n",
      "l2 norm: 899.4431337155158\n",
      "l1 norm: 754.3254538920875\n",
      "Rbeta: 899.5825123899949\n",
      "\n",
      "Train set: Avg. loss: 0.000032169, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.927574 113.66881\n",
      "l2 norm: 899.4231025545027\n",
      "l1 norm: 754.3091456480565\n",
      "Rbeta: 899.5630799083873\n",
      "\n",
      "Train set: Avg. loss: 0.000032141, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.93374 113.68287\n",
      "l2 norm: 899.401466518306\n",
      "l1 norm: 754.2914994687047\n",
      "Rbeta: 899.5420192306309\n",
      "\n",
      "Train set: Avg. loss: 0.000032112, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.9399 113.696915\n",
      "l2 norm: 899.3845426255737\n",
      "l1 norm: 754.2777455349658\n",
      "Rbeta: 899.5255850327887\n",
      "\n",
      "Train set: Avg. loss: 0.000032083, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.94606 113.710976\n",
      "l2 norm: 899.3645154566608\n",
      "l1 norm: 754.261357051163\n",
      "Rbeta: 899.5061505190446\n",
      "\n",
      "Train set: Avg. loss: 0.000032055, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.952225 113.72504\n",
      "l2 norm: 899.3425893027062\n",
      "l1 norm: 754.2433915199905\n",
      "Rbeta: 899.4847186085448\n",
      "\n",
      "Train set: Avg. loss: 0.000032026, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.9584 113.73908\n",
      "l2 norm: 899.3239458645638\n",
      "l1 norm: 754.2281800987271\n",
      "Rbeta: 899.466632921877\n",
      "\n",
      "Train set: Avg. loss: 0.000031998, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.964554 113.753136\n",
      "l2 norm: 899.3029910983973\n",
      "l1 norm: 754.2110506740703\n",
      "Rbeta: 899.4461728107053\n",
      "\n",
      "Train set: Avg. loss: 0.000031969, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.97072 113.767204\n",
      "l2 norm: 899.2787542290231\n",
      "l1 norm: 754.1912259449168\n",
      "Rbeta: 899.4224995836782\n",
      "\n",
      "Train set: Avg. loss: 0.000031941, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.97689 113.78128\n",
      "l2 norm: 899.2524025415186\n",
      "l1 norm: 754.1696626157961\n",
      "Rbeta: 899.3967997597481\n",
      "\n",
      "Train set: Avg. loss: 0.000031913, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.983055 113.79531\n",
      "l2 norm: 899.2301405752365\n",
      "l1 norm: 754.151511192626\n",
      "Rbeta: 899.3750006587726\n",
      "\n",
      "Train set: Avg. loss: 0.000031884, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.98921 113.80934\n",
      "l2 norm: 899.2132374260893\n",
      "l1 norm: 754.1378356730182\n",
      "Rbeta: 899.3586459020104\n",
      "\n",
      "Train set: Avg. loss: 0.000031856, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.995384 113.82335\n",
      "l2 norm: 899.1956095527516\n",
      "l1 norm: 754.1235911933347\n",
      "Rbeta: 899.3415333476275\n",
      "\n",
      "Train set: Avg. loss: 0.000031828, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.00155 113.837364\n",
      "l2 norm: 899.1751084599525\n",
      "l1 norm: 754.1069137647075\n",
      "Rbeta: 899.3216322424844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000031800, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.00771 113.85136\n",
      "l2 norm: 899.1557667237724\n",
      "l1 norm: 754.0912114718367\n",
      "Rbeta: 899.3027973581454\n",
      "\n",
      "Train set: Avg. loss: 0.000031771, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.013885 113.86533\n",
      "l2 norm: 899.1344307834067\n",
      "l1 norm: 754.0738415999183\n",
      "Rbeta: 899.2820214219548\n",
      "\n",
      "Train set: Avg. loss: 0.000031743, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.02005 113.87929\n",
      "l2 norm: 899.1112488144282\n",
      "l1 norm: 754.054923280193\n",
      "Rbeta: 899.2594128611984\n",
      "\n",
      "Train set: Avg. loss: 0.000031715, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.026215 113.89321\n",
      "l2 norm: 899.0850421040966\n",
      "l1 norm: 754.0334761539643\n",
      "Rbeta: 899.2337811256984\n",
      "\n",
      "Train set: Avg. loss: 0.000031687, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.03239 113.90712\n",
      "l2 norm: 899.0605591317872\n",
      "l1 norm: 754.0134535412275\n",
      "Rbeta: 899.2098696969804\n",
      "\n",
      "Train set: Avg. loss: 0.000031659, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.03855 113.92102\n",
      "l2 norm: 899.03676597964\n",
      "l1 norm: 753.9939903265808\n",
      "Rbeta: 899.1865406485759\n",
      "\n",
      "Train set: Avg. loss: 0.000031634, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.04284 113.93491\n",
      "l2 norm: 899.0143405120851\n",
      "l1 norm: 753.975659728572\n",
      "Rbeta: 899.1648339868193\n",
      "\n",
      "Train set: Avg. loss: 0.000031609, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.047134 113.94879\n",
      "l2 norm: 898.9909065887823\n",
      "l1 norm: 753.9564955928266\n",
      "Rbeta: 899.1421025711791\n",
      "\n",
      "Train set: Avg. loss: 0.000031583, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.05142 113.962654\n",
      "l2 norm: 898.963230403968\n",
      "l1 norm: 753.9338154845896\n",
      "Rbeta: 899.1150590844335\n",
      "\n",
      "Train set: Avg. loss: 0.000031558, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.05572 113.97651\n",
      "l2 norm: 898.9322611013678\n",
      "l1 norm: 753.9083583996612\n",
      "Rbeta: 899.0848030786641\n",
      "\n",
      "Train set: Avg. loss: 0.000031533, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.060005 113.99034\n",
      "l2 norm: 898.8932454103203\n",
      "l1 norm: 753.8761472722477\n",
      "Rbeta: 899.0465291363859\n",
      "\n",
      "Train set: Avg. loss: 0.000031508, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.0643 114.00416\n",
      "l2 norm: 898.8596523234891\n",
      "l1 norm: 753.8484661360235\n",
      "Rbeta: 899.0135708369805\n",
      "\n",
      "Train set: Avg. loss: 0.000031483, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.06859 114.017944\n",
      "l2 norm: 898.8289760399244\n",
      "l1 norm: 753.8232297016727\n",
      "Rbeta: 898.9836223040252\n",
      "\n",
      "Train set: Avg. loss: 0.000031458, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.072876 114.031715\n",
      "l2 norm: 898.7977261489161\n",
      "l1 norm: 753.7975791810256\n",
      "Rbeta: 898.9530208499017\n",
      "\n",
      "Train set: Avg. loss: 0.000031433, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.07715 114.04549\n",
      "l2 norm: 898.7664980501112\n",
      "l1 norm: 753.7719478793335\n",
      "Rbeta: 898.922533858528\n",
      "\n",
      "Train set: Avg. loss: 0.000031408, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.08127 114.05926\n",
      "l2 norm: 898.7340667827259\n",
      "l1 norm: 753.7453126256455\n",
      "Rbeta: 898.8908510177514\n",
      "\n",
      "Train set: Avg. loss: 0.000031383, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.08539 114.07301\n",
      "l2 norm: 898.7030185437704\n",
      "l1 norm: 753.7198618730775\n",
      "Rbeta: 898.8604495361161\n",
      "\n",
      "Train set: Avg. loss: 0.000031358, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.08951 114.086754\n",
      "l2 norm: 898.6692379318921\n",
      "l1 norm: 753.6921607804379\n",
      "Rbeta: 898.827423595532\n",
      "\n",
      "Train set: Avg. loss: 0.000031334, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.09363 114.100464\n",
      "l2 norm: 898.6376125874527\n",
      "l1 norm: 753.6662678646934\n",
      "Rbeta: 898.7964769334917\n",
      "\n",
      "Train set: Avg. loss: 0.000031309, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.09775 114.11417\n",
      "l2 norm: 898.603832179272\n",
      "l1 norm: 753.6385540517128\n",
      "Rbeta: 898.7633899204102\n",
      "\n",
      "Train set: Avg. loss: 0.000031285, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.101875 114.12785\n",
      "l2 norm: 898.5715104108712\n",
      "l1 norm: 753.6120473247557\n",
      "Rbeta: 898.7318135520355\n",
      "\n",
      "Train set: Avg. loss: 0.000031260, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.10593 114.141525\n",
      "l2 norm: 898.5399485370774\n",
      "l1 norm: 753.5861935372106\n",
      "Rbeta: 898.7009434065926\n",
      "\n",
      "Train set: Avg. loss: 0.000031236, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.10987 114.15518\n",
      "l2 norm: 898.5053602584029\n",
      "l1 norm: 753.5577507518549\n",
      "Rbeta: 898.667015381301\n",
      "\n",
      "Train set: Avg. loss: 0.000031212, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.113815 114.16883\n",
      "l2 norm: 898.4677469082357\n",
      "l1 norm: 753.5267450260353\n",
      "Rbeta: 898.6301518518475\n",
      "\n",
      "Train set: Avg. loss: 0.000031188, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.117775 114.18249\n",
      "l2 norm: 898.4348763199944\n",
      "l1 norm: 753.4997234172482\n",
      "Rbeta: 898.5980177819603\n",
      "\n",
      "Train set: Avg. loss: 0.000031163, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.12172 114.196175\n",
      "l2 norm: 898.3991360897068\n",
      "l1 norm: 753.4703273447354\n",
      "Rbeta: 898.5630096965912\n",
      "\n",
      "Train set: Avg. loss: 0.000031139, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.12567 114.20988\n",
      "l2 norm: 898.3627609769864\n",
      "l1 norm: 753.4403666888218\n",
      "Rbeta: 898.5273663173081\n",
      "\n",
      "Train set: Avg. loss: 0.000031115, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.129616 114.22355\n",
      "l2 norm: 898.3230769983352\n",
      "l1 norm: 753.407644624619\n",
      "Rbeta: 898.4883995343116\n",
      "\n",
      "Train set: Avg. loss: 0.000031091, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.133575 114.237206\n",
      "l2 norm: 898.2834621699698\n",
      "l1 norm: 753.3749522389642\n",
      "Rbeta: 898.4495494705287\n",
      "\n",
      "Train set: Avg. loss: 0.000031067, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.13752 114.25088\n",
      "l2 norm: 898.2430591409203\n",
      "l1 norm: 753.3415978096914\n",
      "Rbeta: 898.4098437667984\n",
      "\n",
      "Train set: Avg. loss: 0.000031043, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.141464 114.26455\n",
      "l2 norm: 898.2096552867492\n",
      "l1 norm: 753.3140995544038\n",
      "Rbeta: 898.3771147574156\n",
      "\n",
      "Train set: Avg. loss: 0.000031019, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.14536 114.27824\n",
      "l2 norm: 898.1807781067248\n",
      "l1 norm: 753.2904000126766\n",
      "Rbeta: 898.3490476284209\n",
      "\n",
      "Train set: Avg. loss: 0.000030995, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.14914 114.29193\n",
      "l2 norm: 898.1524633963774\n",
      "l1 norm: 753.2671807264694\n",
      "Rbeta: 898.3214624606588\n",
      "\n",
      "Train set: Avg. loss: 0.000030973, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.15147 114.30562\n",
      "l2 norm: 898.1229618969217\n",
      "l1 norm: 753.2429685184547\n",
      "Rbeta: 898.2928496879961\n",
      "\n",
      "Train set: Avg. loss: 0.000030952, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.153366 114.31926\n",
      "l2 norm: 898.0924939033899\n",
      "l1 norm: 753.2179368274983\n",
      "Rbeta: 898.2632530193043\n",
      "\n",
      "Train set: Avg. loss: 0.000030930, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.155266 114.33288\n",
      "l2 norm: 898.0601455151091\n",
      "l1 norm: 753.1913004883256\n",
      "Rbeta: 898.2318340090627\n",
      "\n",
      "Train set: Avg. loss: 0.000030909, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.157166 114.34645\n",
      "l2 norm: 898.0268548057284\n",
      "l1 norm: 753.1638496120896\n",
      "Rbeta: 898.1993816294759\n",
      "\n",
      "Train set: Avg. loss: 0.000030888, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.159065 114.36003\n",
      "l2 norm: 897.998126431912\n",
      "l1 norm: 753.1402361859036\n",
      "Rbeta: 898.1716015999106\n",
      "\n",
      "Train set: Avg. loss: 0.000030867, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.160965 114.37358\n",
      "l2 norm: 897.970665619094\n",
      "l1 norm: 753.1176645041735\n",
      "Rbeta: 898.1450358965751\n",
      "\n",
      "Train set: Avg. loss: 0.000030846, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.162865 114.387115\n",
      "l2 norm: 897.9417013788064\n",
      "l1 norm: 753.0938399719462\n",
      "Rbeta: 898.1169766596662\n",
      "\n",
      "Train set: Avg. loss: 0.000030825, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.164764 114.400665\n",
      "l2 norm: 897.9119077319477\n",
      "l1 norm: 753.0693548371866\n",
      "Rbeta: 898.088066715692\n",
      "\n",
      "Train set: Avg. loss: 0.000030804, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.166664 114.414185\n",
      "l2 norm: 897.8791256785979\n",
      "l1 norm: 753.0423505225141\n",
      "Rbeta: 898.0561508950432\n",
      "\n",
      "Train set: Avg. loss: 0.000030783, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.168564 114.427666\n",
      "l2 norm: 897.8481966909745\n",
      "l1 norm: 753.0168735691475\n",
      "Rbeta: 898.0261528923104\n",
      "\n",
      "Train set: Avg. loss: 0.000030762, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.17046 114.44112\n",
      "l2 norm: 897.8172122064639\n",
      "l1 norm: 752.9913504428707\n",
      "Rbeta: 897.9960389924047\n",
      "\n",
      "Train set: Avg. loss: 0.000030742, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.17236 114.454575\n",
      "l2 norm: 897.7856873263817\n",
      "l1 norm: 752.9653792384868\n",
      "Rbeta: 897.9654573194051\n",
      "\n",
      "Train set: Avg. loss: 0.000030721, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.17427 114.468\n",
      "l2 norm: 897.7545068623584\n",
      "l1 norm: 752.9397001150776\n",
      "Rbeta: 897.9351877949093\n",
      "\n",
      "Train set: Avg. loss: 0.000030700, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.17617 114.481445\n",
      "l2 norm: 897.7194871348084\n",
      "l1 norm: 752.9108110584746\n",
      "Rbeta: 897.9011351662847\n",
      "\n",
      "Train set: Avg. loss: 0.000030679, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.17807 114.49488\n",
      "l2 norm: 897.6869305857759\n",
      "l1 norm: 752.8839861083916\n",
      "Rbeta: 897.8694273459555\n",
      "\n",
      "Train set: Avg. loss: 0.000030659, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.17997 114.50832\n",
      "l2 norm: 897.6567546176842\n",
      "l1 norm: 752.8591671143588\n",
      "Rbeta: 897.8402003158695\n",
      "\n",
      "Train set: Avg. loss: 0.000030638, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.18187 114.521736\n",
      "l2 norm: 897.6323290467342\n",
      "l1 norm: 752.8391873419682\n",
      "Rbeta: 897.8167372197672\n",
      "\n",
      "Train set: Avg. loss: 0.000030617, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.18378 114.53514\n",
      "l2 norm: 897.6091991461428\n",
      "l1 norm: 752.8203293165868\n",
      "Rbeta: 897.7945003458865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000030597, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.18568 114.54855\n",
      "l2 norm: 897.5843598589639\n",
      "l1 norm: 752.800055992528\n",
      "Rbeta: 897.7705574589744\n",
      "\n",
      "Train set: Avg. loss: 0.000030576, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.18758 114.561966\n",
      "l2 norm: 897.5585687681482\n",
      "l1 norm: 752.7789518221829\n",
      "Rbeta: 897.7457182240039\n",
      "\n",
      "Train set: Avg. loss: 0.000030555, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.189476 114.57538\n",
      "l2 norm: 897.5356859773866\n",
      "l1 norm: 752.7603011312452\n",
      "Rbeta: 897.7237689066262\n",
      "\n",
      "Train set: Avg. loss: 0.000030535, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.191376 114.588806\n",
      "l2 norm: 897.5120757334458\n",
      "l1 norm: 752.7410450196967\n",
      "Rbeta: 897.7010757361988\n",
      "\n",
      "Train set: Avg. loss: 0.000030514, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.19328 114.60219\n",
      "l2 norm: 897.4867523955886\n",
      "l1 norm: 752.720338634879\n",
      "Rbeta: 897.6766624529287\n",
      "\n",
      "Train set: Avg. loss: 0.000030494, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.19518 114.615555\n",
      "l2 norm: 897.4619677174641\n",
      "l1 norm: 752.7000563050351\n",
      "Rbeta: 897.6527728790066\n",
      "\n",
      "Train set: Avg. loss: 0.000030473, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.19708 114.62891\n",
      "l2 norm: 897.4383850289753\n",
      "l1 norm: 752.6807815725113\n",
      "Rbeta: 897.6301694975756\n",
      "\n",
      "Train set: Avg. loss: 0.000030453, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.19899 114.64224\n",
      "l2 norm: 897.4194770184461\n",
      "l1 norm: 752.6654520152956\n",
      "Rbeta: 897.6122083696015\n",
      "\n",
      "Train set: Avg. loss: 0.000030432, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.20089 114.65556\n",
      "l2 norm: 897.4003734620433\n",
      "l1 norm: 752.6499486469751\n",
      "Rbeta: 897.5940217901438\n",
      "\n",
      "Train set: Avg. loss: 0.000030412, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.2028 114.66887\n",
      "l2 norm: 897.3752653413612\n",
      "l1 norm: 752.6293876969639\n",
      "Rbeta: 897.5699201167855\n",
      "\n",
      "Train set: Avg. loss: 0.000030392, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.2047 114.682205\n",
      "l2 norm: 897.349862155821\n",
      "l1 norm: 752.6086053629354\n",
      "Rbeta: 897.5453959709639\n",
      "\n",
      "Train set: Avg. loss: 0.000030371, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.206604 114.69559\n",
      "l2 norm: 897.3283226532799\n",
      "l1 norm: 752.5910444714629\n",
      "Rbeta: 897.5248561354935\n",
      "\n",
      "Train set: Avg. loss: 0.000030351, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.2085 114.70896\n",
      "l2 norm: 897.3037057863381\n",
      "l1 norm: 752.5709086083625\n",
      "Rbeta: 897.5011638889279\n",
      "\n",
      "Train set: Avg. loss: 0.000030330, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.2104 114.72233\n",
      "l2 norm: 897.2795999685055\n",
      "l1 norm: 752.5512143343869\n",
      "Rbeta: 897.4780097093036\n",
      "\n",
      "Train set: Avg. loss: 0.000030310, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.21231 114.73569\n",
      "l2 norm: 897.2650540194621\n",
      "l1 norm: 752.5395234883222\n",
      "Rbeta: 897.4644027297749\n",
      "\n",
      "Train set: Avg. loss: 0.000030290, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.21422 114.74904\n",
      "l2 norm: 897.2523951700194\n",
      "l1 norm: 752.5293908216149\n",
      "Rbeta: 897.452718592365\n",
      "\n",
      "Train set: Avg. loss: 0.000030269, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.21612 114.76236\n",
      "l2 norm: 897.2339397371775\n",
      "l1 norm: 752.5143766183744\n",
      "Rbeta: 897.4352601121024\n",
      "\n",
      "Train set: Avg. loss: 0.000030249, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.21802 114.775665\n",
      "l2 norm: 897.210739464077\n",
      "l1 norm: 752.4953727285772\n",
      "Rbeta: 897.4129892063694\n",
      "\n",
      "Train set: Avg. loss: 0.000030229, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.219925 114.78893\n",
      "l2 norm: 897.1886216898598\n",
      "l1 norm: 752.4773151898156\n",
      "Rbeta: 897.3917739964722\n",
      "\n",
      "Train set: Avg. loss: 0.000030209, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.22183 114.802216\n",
      "l2 norm: 897.1704228818547\n",
      "l1 norm: 752.4625363041616\n",
      "Rbeta: 897.3746304543746\n",
      "\n",
      "Train set: Avg. loss: 0.000030189, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.22373 114.81545\n",
      "l2 norm: 897.1513684805893\n",
      "l1 norm: 752.4470387429781\n",
      "Rbeta: 897.3565000175287\n",
      "\n",
      "Train set: Avg. loss: 0.000030168, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.22564 114.8287\n",
      "l2 norm: 897.1318046228741\n",
      "l1 norm: 752.4311312313934\n",
      "Rbeta: 897.3379004473398\n",
      "After training:\n",
      "\n",
      "Train set: Avg. loss: 0.000030168, Accuracy: 512/512 (100%)\n",
      "\n",
      "tensor(49.) tensor(1.0316e+22)\n",
      "110.22564 114.8287\n",
      "l2 norm: 897.1318046228741\n",
      "l1 norm: 752.4311312313934\n",
      "Rbeta: 897.3379004473398\n",
      "Time domain:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency domain:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3\n",
      "Before training:\n",
      "\n",
      "Train set: Avg. loss: 0.999979734, Accuracy: 289/512 (56%)\n",
      "\n",
      "9.5604446e-05 0.00944085\n",
      "l2 norm: 48.651484747323614\n",
      "l1 norm: 43.60491723920444\n",
      "Rbeta: -415.67340839268934\n",
      "Start training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ebd2c31e074fa6a08925e4c48d0caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.982893586, Accuracy: 296/512 (58%)\n",
      "\n",
      "0.014172583 0.023548238\n",
      "l2 norm: 3.946950424116289\n",
      "l1 norm: 2.0160057303628847\n",
      "Rbeta: -4.745568285285084\n",
      "\n",
      "Train set: Avg. loss: 0.951067030, Accuracy: 314/512 (61%)\n",
      "\n",
      "0.07330337 0.08329144\n",
      "l2 norm: 5.1884391498171425\n",
      "l1 norm: 2.640532276273096\n",
      "Rbeta: -5.403556411051095\n",
      "\n",
      "Train set: Avg. loss: 0.923890054, Accuracy: 341/512 (67%)\n",
      "\n",
      "0.16388588 0.17547804\n",
      "l2 norm: 12.197049070420235\n",
      "l1 norm: 7.563097607915668\n",
      "Rbeta: -12.425140852538483\n",
      "\n",
      "Train set: Avg. loss: 0.882523179, Accuracy: 366/512 (71%)\n",
      "\n",
      "0.35640714 0.371502\n",
      "l2 norm: 23.949820684160024\n",
      "l1 norm: 16.132535103474424\n",
      "Rbeta: -24.156934266412073\n",
      "\n",
      "Train set: Avg. loss: 0.830273747, Accuracy: 373/512 (73%)\n",
      "\n",
      "0.6667468 0.68829453\n",
      "l2 norm: 46.6935909260608\n",
      "l1 norm: 33.23028141094527\n",
      "Rbeta: -46.91076464865495\n",
      "Learning rate change\n",
      "\n",
      "Train set: Avg. loss: 0.320938438, Accuracy: 499/512 (97%)\n",
      "\n",
      "8.022483 10.247336\n",
      "l2 norm: 785.4239064720448\n",
      "l1 norm: 642.6331544106393\n",
      "Rbeta: -791.5521685798029\n",
      "\n",
      "Train set: Avg. loss: 0.119461805, Accuracy: 511/512 (100%)\n",
      "\n",
      "18.067343 21.650505\n",
      "l2 norm: 31786.58368593077\n",
      "l1 norm: 26304.136444349482\n",
      "Rbeta: -31921.29049488172\n",
      "\n",
      "Train set: Avg. loss: 0.059051201, Accuracy: 512/512 (100%)\n",
      "\n",
      "25.35545 29.332937\n",
      "l2 norm: 1987.7836843574942\n",
      "l1 norm: 1649.5990909794318\n",
      "Rbeta: 1993.2718366387066\n",
      "\n",
      "Train set: Avg. loss: 0.035680126, Accuracy: 512/512 (100%)\n",
      "\n",
      "30.567108 34.695446\n",
      "l2 norm: 1439.6963412714883\n",
      "l1 norm: 1196.8086788707094\n",
      "Rbeta: 1442.714061245155\n",
      "\n",
      "Train set: Avg. loss: 0.024688862, Accuracy: 512/512 (100%)\n",
      "\n",
      "34.52531 38.726036\n",
      "l2 norm: 1369.1202467452829\n",
      "l1 norm: 1139.422986665821\n",
      "Rbeta: 1371.486538571795\n",
      "\n",
      "Train set: Avg. loss: 0.018503753, Accuracy: 512/512 (100%)\n",
      "\n",
      "37.687458 41.929226\n",
      "l2 norm: 1318.6623927254143\n",
      "l1 norm: 1098.2810218202771\n",
      "Rbeta: 1320.6352651374807\n",
      "\n",
      "Train set: Avg. loss: 0.014602700, Accuracy: 512/512 (100%)\n",
      "\n",
      "40.30862 44.5764\n",
      "l2 norm: 1280.0476398911042\n",
      "l1 norm: 1066.727555917201\n",
      "Rbeta: 1281.7575702005988\n",
      "\n",
      "Train set: Avg. loss: 0.011949147, Accuracy: 512/512 (100%)\n",
      "\n",
      "42.540974 46.8266\n",
      "l2 norm: 1249.5628443281523\n",
      "l1 norm: 1041.7832772029046\n",
      "Rbeta: 1251.0847059437076\n",
      "\n",
      "Train set: Avg. loss: 0.010043613, Accuracy: 512/512 (100%)\n",
      "\n",
      "44.481705 48.780186\n",
      "l2 norm: 1224.8910052955505\n",
      "l1 norm: 1021.577579347157\n",
      "Rbeta: 1226.2717820403388\n",
      "\n",
      "Train set: Avg. loss: 0.008618096, Accuracy: 512/512 (100%)\n",
      "\n",
      "46.196075 50.504253\n",
      "l2 norm: 1204.4827424355358\n",
      "l1 norm: 1004.8534575833088\n",
      "Rbeta: 1205.7537862998213\n",
      "\n",
      "Train set: Avg. loss: 0.007516920, Accuracy: 512/512 (100%)\n",
      "\n",
      "47.730087 52.04576\n",
      "l2 norm: 1187.2875426179237\n",
      "l1 norm: 990.7562886441017\n",
      "Rbeta: 1188.4703687068595\n",
      "\n",
      "Train set: Avg. loss: 0.006644104, Accuracy: 512/512 (100%)\n",
      "\n",
      "49.117126 53.438736\n",
      "l2 norm: 1172.5655587142383\n",
      "l1 norm: 978.6828001918739\n",
      "Rbeta: 1173.6759195467375\n",
      "\n",
      "Train set: Avg. loss: 0.005937506, Accuracy: 512/512 (100%)\n",
      "\n",
      "50.382183 54.708645\n",
      "l2 norm: 1159.791148070488\n",
      "l1 norm: 968.2039052743813\n",
      "Rbeta: 1160.840852229333\n",
      "\n",
      "Train set: Avg. loss: 0.005355258, Accuracy: 512/512 (100%)\n",
      "\n",
      "51.544502 55.874973\n",
      "l2 norm: 1148.5736568009438\n",
      "l1 norm: 959.0003171861099\n",
      "Rbeta: 1149.5717103397278\n",
      "\n",
      "Train set: Avg. loss: 0.004868219, Accuracy: 512/512 (100%)\n",
      "\n",
      "52.61921 56.952988\n",
      "l2 norm: 1138.63016884028\n",
      "l1 norm: 950.8406970283224\n",
      "Rbeta: 1139.5836762899607\n",
      "\n",
      "Train set: Avg. loss: 0.004455560, Accuracy: 512/512 (100%)\n",
      "\n",
      "53.618217 57.95482\n",
      "l2 norm: 1129.7345803585774\n",
      "l1 norm: 943.5400419048733\n",
      "Rbeta: 1130.6492371687848\n",
      "\n",
      "Train set: Avg. loss: 0.004101980, Accuracy: 512/512 (100%)\n",
      "\n",
      "54.551292 58.89033\n",
      "l2 norm: 1121.7196033014066\n",
      "l1 norm: 936.9613601591527\n",
      "Rbeta: 1122.5999111704095\n",
      "\n",
      "Train set: Avg. loss: 0.003796061, Accuracy: 512/512 (100%)\n",
      "\n",
      "55.42637 59.767563\n",
      "l2 norm: 1114.4472445899435\n",
      "l1 norm: 930.9916905682869\n",
      "Rbeta: 1115.2971539380328\n",
      "\n",
      "Train set: Avg. loss: 0.003529069, Accuracy: 512/512 (100%)\n",
      "\n",
      "56.250168 60.593212\n",
      "l2 norm: 1107.808387639606\n",
      "l1 norm: 925.5415685702737\n",
      "Rbeta: 1108.6310749027518\n",
      "\n",
      "Train set: Avg. loss: 0.003294263, Accuracy: 512/512 (100%)\n",
      "\n",
      "57.028187 61.37288\n",
      "l2 norm: 1101.718692957769\n",
      "l1 norm: 920.5419656937695\n",
      "Rbeta: 1102.516635619006\n",
      "\n",
      "Train set: Avg. loss: 0.003086315, Accuracy: 512/512 (100%)\n",
      "\n",
      "57.76535 62.11132\n",
      "l2 norm: 1096.1034252504032\n",
      "l1 norm: 915.9315820259767\n",
      "Rbeta: 1096.8790273363936\n",
      "\n",
      "Train set: Avg. loss: 0.002901061, Accuracy: 512/512 (100%)\n",
      "\n",
      "58.465252 62.81261\n",
      "l2 norm: 1090.9049589720091\n",
      "l1 norm: 911.6631434772954\n",
      "Rbeta: 1091.6602609285474\n",
      "\n",
      "Train set: Avg. loss: 0.002735057, Accuracy: 512/512 (100%)\n",
      "\n",
      "59.131744 63.480232\n",
      "l2 norm: 1086.072324534294\n",
      "l1 norm: 907.694871746151\n",
      "Rbeta: 1086.8089968313761\n",
      "\n",
      "Train set: Avg. loss: 0.002585571, Accuracy: 512/512 (100%)\n",
      "\n",
      "59.767593 64.1172\n",
      "l2 norm: 1081.5690650833428\n",
      "l1 norm: 903.996998274491\n",
      "Rbeta: 1082.2885686555458\n",
      "\n",
      "Train set: Avg. loss: 0.002450309, Accuracy: 512/512 (100%)\n",
      "\n",
      "60.375664 64.72616\n",
      "l2 norm: 1077.352130695832\n",
      "l1 norm: 900.5340528126329\n",
      "Rbeta: 1078.0556201242055\n",
      "\n",
      "Train set: Avg. loss: 0.002327415, Accuracy: 512/512 (100%)\n",
      "\n",
      "60.958076 65.30943\n",
      "l2 norm: 1073.3968666376732\n",
      "l1 norm: 897.2859068994132\n",
      "Rbeta: 1074.0856663032582\n",
      "\n",
      "Train set: Avg. loss: 0.002215318, Accuracy: 512/512 (100%)\n",
      "\n",
      "61.51686 65.86908\n",
      "l2 norm: 1069.6758920010527\n",
      "l1 norm: 894.2300673045166\n",
      "Rbeta: 1070.3509790820367\n",
      "\n",
      "Train set: Avg. loss: 0.002112680, Accuracy: 512/512 (100%)\n",
      "\n",
      "62.054073 66.40687\n",
      "l2 norm: 1066.1628981955466\n",
      "l1 norm: 891.3448996643051\n",
      "Rbeta: 1066.825113109825\n",
      "\n",
      "Train set: Avg. loss: 0.002018417, Accuracy: 512/512 (100%)\n",
      "\n",
      "62.57099 66.92442\n",
      "l2 norm: 1062.8462620260232\n",
      "l1 norm: 888.6210283074156\n",
      "Rbeta: 1063.4966008713895\n",
      "\n",
      "Train set: Avg. loss: 0.001931578, Accuracy: 512/512 (100%)\n",
      "\n",
      "63.06903 67.42318\n",
      "l2 norm: 1059.7015928870005\n",
      "l1 norm: 886.0382707177906\n",
      "Rbeta: 1060.3406751831567\n",
      "\n",
      "Train set: Avg. loss: 0.001851326, Accuracy: 512/512 (100%)\n",
      "\n",
      "63.549717 67.90447\n",
      "l2 norm: 1056.7197683360548\n",
      "l1 norm: 883.5892606929029\n",
      "Rbeta: 1057.3482695243154\n",
      "\n",
      "Train set: Avg. loss: 0.001776965, Accuracy: 512/512 (100%)\n",
      "\n",
      "64.01424 68.369415\n",
      "l2 norm: 1053.8790964552154\n",
      "l1 norm: 881.2560445632323\n",
      "Rbeta: 1054.4975415953706\n",
      "\n",
      "Train set: Avg. loss: 0.001707902, Accuracy: 512/512 (100%)\n",
      "\n",
      "64.46341 68.81908\n",
      "l2 norm: 1051.1755321383662\n",
      "l1 norm: 879.0354501070738\n",
      "Rbeta: 1051.7844803034034\n",
      "\n",
      "Train set: Avg. loss: 0.001643600, Accuracy: 512/512 (100%)\n",
      "\n",
      "64.898315 69.25444\n",
      "l2 norm: 1048.5969343511335\n",
      "l1 norm: 876.9175090558332\n",
      "Rbeta: 1049.197074269284\n",
      "\n",
      "Train set: Avg. loss: 0.001583604, Accuracy: 512/512 (100%)\n",
      "\n",
      "65.319725 69.676346\n",
      "l2 norm: 1046.1317756553287\n",
      "l1 norm: 874.8926486736136\n",
      "Rbeta: 1046.7233482456465\n",
      "\n",
      "Train set: Avg. loss: 0.001527501, Accuracy: 512/512 (100%)\n",
      "\n",
      "65.72858 70.085594\n",
      "l2 norm: 1043.7715672727504\n",
      "l1 norm: 872.9539939751126\n",
      "Rbeta: 1044.355187329048\n",
      "\n",
      "Train set: Avg. loss: 0.001474943, Accuracy: 512/512 (100%)\n",
      "\n",
      "66.12549 70.4829\n",
      "l2 norm: 1041.514186114743\n",
      "l1 norm: 871.099827785347\n",
      "Rbeta: 1042.0902144190661\n",
      "\n",
      "Train set: Avg. loss: 0.001425599, Accuracy: 512/512 (100%)\n",
      "\n",
      "66.51133 70.86895\n",
      "l2 norm: 1039.343738545581\n",
      "l1 norm: 869.3170096434301\n",
      "Rbeta: 1039.9123550519153\n",
      "\n",
      "Train set: Avg. loss: 0.001379213, Accuracy: 512/512 (100%)\n",
      "\n",
      "66.88637 71.244354\n",
      "l2 norm: 1037.2556619165584\n",
      "l1 norm: 867.6017249508452\n",
      "Rbeta: 1037.8173336329298\n",
      "\n",
      "Train set: Avg. loss: 0.001335547, Accuracy: 512/512 (100%)\n",
      "\n",
      "67.25096 71.60966\n",
      "l2 norm: 1035.2527570447967\n",
      "l1 norm: 865.9564928288403\n",
      "Rbeta: 1035.8078431376907\n",
      "\n",
      "Train set: Avg. loss: 0.001294301, Accuracy: 512/512 (100%)\n",
      "\n",
      "67.606995 71.96541\n",
      "l2 norm: 1033.3227284178047\n",
      "l1 norm: 864.3711138978754\n",
      "Rbeta: 1033.871322194311\n",
      "\n",
      "Train set: Avg. loss: 0.001255370, Accuracy: 512/512 (100%)\n",
      "\n",
      "67.95336 72.31203\n",
      "l2 norm: 1031.4612943796\n",
      "l1 norm: 862.8420499161913\n",
      "Rbeta: 1032.003715325058\n",
      "\n",
      "Train set: Avg. loss: 0.001218554, Accuracy: 512/512 (100%)\n",
      "\n",
      "68.29079 72.65004\n",
      "l2 norm: 1029.6663184472313\n",
      "l1 norm: 861.3674903976153\n",
      "Rbeta: 1030.2030579837976\n",
      "\n",
      "Train set: Avg. loss: 0.001183672, Accuracy: 512/512 (100%)\n",
      "\n",
      "68.620094 72.97983\n",
      "l2 norm: 1027.933712060396\n",
      "l1 norm: 859.9442457268798\n",
      "Rbeta: 1028.4647860525654\n",
      "\n",
      "Train set: Avg. loss: 0.001150547, Accuracy: 512/512 (100%)\n",
      "\n",
      "68.94237 73.30175\n",
      "l2 norm: 1026.2630874458189\n",
      "l1 norm: 858.5719426955134\n",
      "Rbeta: 1026.7886643048485\n",
      "\n",
      "Train set: Avg. loss: 0.001119131, Accuracy: 512/512 (100%)\n",
      "\n",
      "69.25623 73.6162\n",
      "l2 norm: 1024.6420327106052\n",
      "l1 norm: 857.2402328070089\n",
      "Rbeta: 1025.1625377565979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.001089252, Accuracy: 512/512 (100%)\n",
      "\n",
      "69.56315 73.923515\n",
      "l2 norm: 1023.0782013554388\n",
      "l1 norm: 855.955557905123\n",
      "Rbeta: 1023.5937645698424\n",
      "\n",
      "Train set: Avg. loss: 0.001060791, Accuracy: 512/512 (100%)\n",
      "\n",
      "69.86363 74.224\n",
      "l2 norm: 1021.5651096057024\n",
      "l1 norm: 854.7127020564832\n",
      "Rbeta: 1022.0757594703566\n",
      "\n",
      "Train set: Avg. loss: 0.001033694, Accuracy: 512/512 (100%)\n",
      "\n",
      "70.15708 74.517914\n",
      "l2 norm: 1020.0901036388578\n",
      "l1 norm: 853.5008877495404\n",
      "Rbeta: 1020.5961542059838\n",
      "\n",
      "Train set: Avg. loss: 0.001007835, Accuracy: 512/512 (100%)\n",
      "\n",
      "70.44453 74.80558\n",
      "l2 norm: 1018.6613245803778\n",
      "l1 norm: 852.3271239930186\n",
      "Rbeta: 1019.1629137134069\n",
      "\n",
      "Train set: Avg. loss: 0.000983127, Accuracy: 512/512 (100%)\n",
      "\n",
      "70.726395 75.087204\n",
      "l2 norm: 1017.2759120039345\n",
      "l1 norm: 851.1890041424916\n",
      "Rbeta: 1017.7730358689256\n",
      "\n",
      "Train set: Avg. loss: 0.000959562, Accuracy: 512/512 (100%)\n",
      "\n",
      "71.00137 75.36301\n",
      "l2 norm: 1015.9342500484548\n",
      "l1 norm: 850.0869611601634\n",
      "Rbeta: 1016.4273421913159\n",
      "\n",
      "Train set: Avg. loss: 0.000936945, Accuracy: 512/512 (100%)\n",
      "\n",
      "71.27258 75.63334\n",
      "l2 norm: 1014.6296597392762\n",
      "l1 norm: 849.0153050170525\n",
      "Rbeta: 1015.1184231322916\n",
      "\n",
      "Train set: Avg. loss: 0.000915374, Accuracy: 512/512 (100%)\n",
      "\n",
      "71.5364 75.89834\n",
      "l2 norm: 1013.3608811459944\n",
      "l1 norm: 847.9730490050938\n",
      "Rbeta: 1013.8459928595538\n",
      "\n",
      "Train set: Avg. loss: 0.000894648, Accuracy: 512/512 (100%)\n",
      "\n",
      "71.79651 76.15823\n",
      "l2 norm: 1012.123694262276\n",
      "l1 norm: 846.9566577261128\n",
      "Rbeta: 1012.6048610786746\n",
      "\n",
      "Train set: Avg. loss: 0.000874762, Accuracy: 512/512 (100%)\n",
      "\n",
      "72.05195 76.41321\n",
      "l2 norm: 1010.9224357366361\n",
      "l1 norm: 845.9698855881962\n",
      "Rbeta: 1011.3997735058716\n",
      "\n",
      "Train set: Avg. loss: 0.000855739, Accuracy: 512/512 (100%)\n",
      "\n",
      "72.30102 76.66338\n",
      "l2 norm: 1009.747711246259\n",
      "l1 norm: 845.0047840148766\n",
      "Rbeta: 1010.221706217086\n",
      "\n",
      "Train set: Avg. loss: 0.000837419, Accuracy: 512/512 (100%)\n",
      "\n",
      "72.54678 76.90902\n",
      "l2 norm: 1008.6061199682227\n",
      "l1 norm: 844.0669556218743\n",
      "Rbeta: 1009.0765963029469\n",
      "\n",
      "Train set: Avg. loss: 0.000819826, Accuracy: 512/512 (100%)\n",
      "\n",
      "72.78777 77.15021\n",
      "l2 norm: 1007.4942178159171\n",
      "l1 norm: 843.1535245184832\n",
      "Rbeta: 1007.9613859668314\n",
      "\n",
      "Train set: Avg. loss: 0.000802881, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.02515 77.38716\n",
      "l2 norm: 1006.4071214808196\n",
      "l1 norm: 842.260408537112\n",
      "Rbeta: 1006.8708521934605\n",
      "\n",
      "Train set: Avg. loss: 0.000786590, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.257835 77.62001\n",
      "l2 norm: 1005.3494969159339\n",
      "l1 norm: 841.3916443144083\n",
      "Rbeta: 1005.8099488484519\n",
      "\n",
      "Train set: Avg. loss: 0.000770946, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.48519 77.84884\n",
      "l2 norm: 1004.3134514432219\n",
      "l1 norm: 840.5404601822564\n",
      "Rbeta: 1004.7711330945039\n",
      "\n",
      "Train set: Avg. loss: 0.000755800, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.7109 78.0738\n",
      "l2 norm: 1003.3078893635856\n",
      "l1 norm: 839.7144252403571\n",
      "Rbeta: 1003.7624158965415\n",
      "\n",
      "Train set: Avg. loss: 0.000741174, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.933464 78.29506\n",
      "l2 norm: 1002.3147156651339\n",
      "l1 norm: 838.898416471373\n",
      "Rbeta: 1002.7659570792398\n",
      "\n",
      "Train set: Avg. loss: 0.000727070, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.15219 78.51272\n",
      "l2 norm: 1001.348529486894\n",
      "l1 norm: 838.1046141014721\n",
      "Rbeta: 1001.7967656334918\n",
      "\n",
      "Train set: Avg. loss: 0.000713525, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.3652 78.72687\n",
      "l2 norm: 1000.4027442162426\n",
      "l1 norm: 837.3275449967342\n",
      "Rbeta: 1000.8484073161889\n",
      "\n",
      "Train set: Avg. loss: 0.000700489, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.57321 78.93763\n",
      "l2 norm: 999.4858187117378\n",
      "l1 norm: 836.5743066470684\n",
      "Rbeta: 999.9291119543016\n",
      "\n",
      "Train set: Avg. loss: 0.000687850, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.77923 79.14521\n",
      "l2 norm: 998.5886854453299\n",
      "l1 norm: 835.8373262687818\n",
      "Rbeta: 999.0296850197302\n",
      "\n",
      "Train set: Avg. loss: 0.000675576, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.983826 79.3496\n",
      "l2 norm: 997.7041040288409\n",
      "l1 norm: 835.1106855364851\n",
      "Rbeta: 998.1424707721818\n",
      "\n",
      "Train set: Avg. loss: 0.000663702, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.18526 79.55097\n",
      "l2 norm: 996.8382007059016\n",
      "l1 norm: 834.3992463534396\n",
      "Rbeta: 997.2739701425913\n",
      "\n",
      "Train set: Avg. loss: 0.000652186, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.38462 79.74926\n",
      "l2 norm: 995.9903304408659\n",
      "l1 norm: 833.7027665036404\n",
      "Rbeta: 996.4233644212205\n",
      "\n",
      "Train set: Avg. loss: 0.000641022, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.58145 79.94469\n",
      "l2 norm: 995.1613554346017\n",
      "l1 norm: 833.0217413688513\n",
      "Rbeta: 995.5917399285498\n",
      "\n",
      "Train set: Avg. loss: 0.000630227, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.77476 80.13722\n",
      "l2 norm: 994.3436147614256\n",
      "l1 norm: 832.3498972710985\n",
      "Rbeta: 994.7715077005572\n",
      "\n",
      "Train set: Avg. loss: 0.000619813, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.9635 80.32699\n",
      "l2 norm: 993.5445311543436\n",
      "l1 norm: 831.6934569699497\n",
      "Rbeta: 993.9702732270266\n",
      "\n",
      "Train set: Avg. loss: 0.000609671, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.151184 80.51411\n",
      "l2 norm: 992.759857917124\n",
      "l1 norm: 831.0488604053947\n",
      "Rbeta: 993.1831437065024\n",
      "\n",
      "Train set: Avg. loss: 0.000599860, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.335236 80.69866\n",
      "l2 norm: 991.9875916274963\n",
      "l1 norm: 830.4143587602159\n",
      "Rbeta: 992.40877756786\n",
      "\n",
      "Train set: Avg. loss: 0.000590364, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.51576 80.88064\n",
      "l2 norm: 991.2341290349074\n",
      "l1 norm: 829.7954264556511\n",
      "Rbeta: 991.6533763290729\n",
      "\n",
      "Train set: Avg. loss: 0.000581116, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.69487 81.06026\n",
      "l2 norm: 990.498288693897\n",
      "l1 norm: 829.1910080845271\n",
      "Rbeta: 990.9154595849333\n",
      "\n",
      "Train set: Avg. loss: 0.000572120, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.87212 81.23749\n",
      "l2 norm: 989.7725593920577\n",
      "l1 norm: 828.5947943573148\n",
      "Rbeta: 990.1877569943621\n",
      "\n",
      "Train set: Avg. loss: 0.000563389, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.04677 81.41229\n",
      "l2 norm: 989.0610320692617\n",
      "l1 norm: 828.0101768578645\n",
      "Rbeta: 989.4741714299539\n",
      "\n",
      "Train set: Avg. loss: 0.000554918, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.21839 81.58485\n",
      "l2 norm: 988.3691289503347\n",
      "l1 norm: 827.4419085271138\n",
      "Rbeta: 988.7804075512872\n",
      "\n",
      "Train set: Avg. loss: 0.000546646, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.38932 81.75524\n",
      "l2 norm: 987.6733128939011\n",
      "l1 norm: 826.8701347672175\n",
      "Rbeta: 988.0825843773569\n",
      "\n",
      "Train set: Avg. loss: 0.000538579, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.55884 81.92351\n",
      "l2 norm: 986.9992591763516\n",
      "l1 norm: 826.3164784005929\n",
      "Rbeta: 987.4063435033486\n",
      "\n",
      "Train set: Avg. loss: 0.000530766, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.724785 82.08964\n",
      "l2 norm: 986.3276240090014\n",
      "l1 norm: 825.7646252419275\n",
      "Rbeta: 986.7328703294373\n",
      "\n",
      "Train set: Avg. loss: 0.000523170, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.88822 82.25371\n",
      "l2 norm: 985.6759994733528\n",
      "l1 norm: 825.22949323826\n",
      "Rbeta: 986.0795235523493\n",
      "\n",
      "Train set: Avg. loss: 0.000515747, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.050896 82.415825\n",
      "l2 norm: 985.0366807509715\n",
      "l1 norm: 824.7043412036871\n",
      "Rbeta: 985.4382692031053\n",
      "\n",
      "Train set: Avg. loss: 0.000508510, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.211945 82.575905\n",
      "l2 norm: 984.4030615394043\n",
      "l1 norm: 824.1837139443122\n",
      "Rbeta: 984.802631132217\n",
      "\n",
      "Train set: Avg. loss: 0.000501487, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.36976 82.73412\n",
      "l2 norm: 983.7873321380015\n",
      "l1 norm: 823.677995139773\n",
      "Rbeta: 984.185260441136\n",
      "\n",
      "Train set: Avg. loss: 0.000494610, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.52722 82.890434\n",
      "l2 norm: 983.1751483702758\n",
      "l1 norm: 823.1751306187576\n",
      "Rbeta: 983.5712474170323\n",
      "\n",
      "Train set: Avg. loss: 0.000487904, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.683075 83.044846\n",
      "l2 norm: 982.5771170294812\n",
      "l1 norm: 822.6839041800002\n",
      "Rbeta: 982.97127084813\n",
      "\n",
      "Train set: Avg. loss: 0.000481398, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.83544 83.197525\n",
      "l2 norm: 981.9851533604776\n",
      "l1 norm: 822.197491257192\n",
      "Rbeta: 982.3776480931995\n",
      "\n",
      "Train set: Avg. loss: 0.000475120, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.98279 83.3485\n",
      "l2 norm: 981.3965472304878\n",
      "l1 norm: 821.7138577913731\n",
      "Rbeta: 981.7881300262159\n",
      "\n",
      "Train set: Avg. loss: 0.000468979, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.12908 83.49783\n",
      "l2 norm: 980.8327107236395\n",
      "l1 norm: 821.2508278049447\n",
      "Rbeta: 981.2232003548825\n",
      "\n",
      "Train set: Avg. loss: 0.000462954, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.275375 83.64541\n",
      "l2 norm: 980.2699093526695\n",
      "l1 norm: 820.7885364811477\n",
      "Rbeta: 980.6589328627886\n",
      "\n",
      "Train set: Avg. loss: 0.000457040, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.42164 83.79135\n",
      "l2 norm: 979.7145576300521\n",
      "l1 norm: 820.3322724778994\n",
      "Rbeta: 980.1020047056903\n",
      "\n",
      "Train set: Avg. loss: 0.000451236, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.56769 83.93574\n",
      "l2 norm: 979.1619920552201\n",
      "l1 norm: 819.8782944030906\n",
      "Rbeta: 979.5475854449384\n",
      "\n",
      "Train set: Avg. loss: 0.000445580, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.71159 84.078545\n",
      "l2 norm: 978.6248479389197\n",
      "l1 norm: 819.4370396938596\n",
      "Rbeta: 979.0088165190181\n",
      "\n",
      "Train set: Avg. loss: 0.000440051, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.85413 84.21979\n",
      "l2 norm: 978.1004831004225\n",
      "l1 norm: 819.0064659492934\n",
      "Rbeta: 978.4827859300134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000434650, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.99495 84.35967\n",
      "l2 norm: 977.567700109492\n",
      "l1 norm: 818.5686069576079\n",
      "Rbeta: 977.9484242252662\n",
      "\n",
      "Train set: Avg. loss: 0.000429361, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.13481 84.49809\n",
      "l2 norm: 977.0455127303798\n",
      "l1 norm: 818.1396301011948\n",
      "Rbeta: 977.4246338393356\n",
      "\n",
      "Train set: Avg. loss: 0.000424240, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.27068 84.634926\n",
      "l2 norm: 976.5310133103211\n",
      "l1 norm: 817.7168414845426\n",
      "Rbeta: 976.9088051863446\n",
      "\n",
      "Train set: Avg. loss: 0.000419239, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.40473 84.77037\n",
      "l2 norm: 976.0296997432984\n",
      "l1 norm: 817.3050969078814\n",
      "Rbeta: 976.4063815727234\n",
      "\n",
      "Train set: Avg. loss: 0.000414322, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.53881 84.90449\n",
      "l2 norm: 975.5355111083979\n",
      "l1 norm: 816.899140632886\n",
      "Rbeta: 975.9108920916732\n",
      "\n",
      "Train set: Avg. loss: 0.000409498, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.67224 85.037254\n",
      "l2 norm: 975.0462850496752\n",
      "l1 norm: 816.497291627644\n",
      "Rbeta: 975.420203799626\n",
      "\n",
      "Train set: Avg. loss: 0.000404794, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.8033 85.168785\n",
      "l2 norm: 974.5639118802056\n",
      "l1 norm: 816.1011761661214\n",
      "Rbeta: 974.9365316893395\n",
      "\n",
      "Train set: Avg. loss: 0.000400183, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.93355 85.29897\n",
      "l2 norm: 974.081377247963\n",
      "l1 norm: 815.7046580767462\n",
      "Rbeta: 974.4527266603216\n",
      "\n",
      "Train set: Avg. loss: 0.000395665, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.06276 85.427864\n",
      "l2 norm: 973.6178605261766\n",
      "l1 norm: 815.3238873057292\n",
      "Rbeta: 973.9879100984432\n",
      "\n",
      "Train set: Avg. loss: 0.000391223, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.19177 85.55549\n",
      "l2 norm: 973.1649687647649\n",
      "l1 norm: 814.9518914136798\n",
      "Rbeta: 973.5335210879421\n",
      "\n",
      "Train set: Avg. loss: 0.000386893, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.31834 85.68195\n",
      "l2 norm: 972.7008454265574\n",
      "l1 norm: 814.5705777026668\n",
      "Rbeta: 973.0681675630508\n",
      "\n",
      "Train set: Avg. loss: 0.000382665, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.442924 85.80717\n",
      "l2 norm: 972.2457649179313\n",
      "l1 norm: 814.1965726352481\n",
      "Rbeta: 972.6119801158092\n",
      "\n",
      "Train set: Avg. loss: 0.000378511, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.56697 85.93121\n",
      "l2 norm: 971.8066737534775\n",
      "l1 norm: 813.8360054650404\n",
      "Rbeta: 972.1717066062731\n",
      "\n",
      "Train set: Avg. loss: 0.000374424, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.69099 86.05405\n",
      "l2 norm: 971.3616551353574\n",
      "l1 norm: 813.4703434139051\n",
      "Rbeta: 971.7252967332091\n",
      "\n",
      "Train set: Avg. loss: 0.000370440, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.81231 86.17585\n",
      "l2 norm: 970.9292118214477\n",
      "l1 norm: 813.1151852147647\n",
      "Rbeta: 971.2918043385623\n",
      "\n",
      "Train set: Avg. loss: 0.000366535, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.93268 86.296425\n",
      "l2 norm: 970.4929872400273\n",
      "l1 norm: 812.7567530164349\n",
      "Rbeta: 970.8543659564456\n",
      "\n",
      "Train set: Avg. loss: 0.000362717, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.0512 86.41592\n",
      "l2 norm: 970.0698135372239\n",
      "l1 norm: 812.4091967155232\n",
      "Rbeta: 970.430270294026\n",
      "\n",
      "Train set: Avg. loss: 0.000358994, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.167145 86.5345\n",
      "l2 norm: 969.6596519686966\n",
      "l1 norm: 812.0725327874748\n",
      "Rbeta: 970.0193966595632\n",
      "\n",
      "Train set: Avg. loss: 0.000355335, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.28256 86.651985\n",
      "l2 norm: 969.243707909738\n",
      "l1 norm: 811.730920686286\n",
      "Rbeta: 969.6026295317378\n",
      "\n",
      "Train set: Avg. loss: 0.000351732, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.397766 86.768425\n",
      "l2 norm: 968.8408058916694\n",
      "l1 norm: 811.400171218583\n",
      "Rbeta: 969.1988195335641\n",
      "\n",
      "Train set: Avg. loss: 0.000348182, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.51299 86.88385\n",
      "l2 norm: 968.437874825161\n",
      "l1 norm: 811.0692234956664\n",
      "Rbeta: 968.7948998981353\n",
      "\n",
      "Train set: Avg. loss: 0.000344684, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.62819 86.998184\n",
      "l2 norm: 968.0416723902912\n",
      "l1 norm: 810.7438230322387\n",
      "Rbeta: 968.3975010289311\n",
      "\n",
      "Train set: Avg. loss: 0.000341257, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.74196 87.111626\n",
      "l2 norm: 967.6402404201665\n",
      "l1 norm: 810.4138812574452\n",
      "Rbeta: 967.9949413862171\n",
      "\n",
      "Train set: Avg. loss: 0.000337879, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.85569 87.22406\n",
      "l2 norm: 967.2470751008493\n",
      "l1 norm: 810.0909977067747\n",
      "Rbeta: 967.6005196318359\n",
      "\n",
      "Train set: Avg. loss: 0.000334553, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.969154 87.335495\n",
      "l2 norm: 966.8542052563378\n",
      "l1 norm: 809.7680847714323\n",
      "Rbeta: 967.2064165431666\n",
      "\n",
      "Train set: Avg. loss: 0.000331297, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.0811 87.44595\n",
      "l2 norm: 966.4760159819617\n",
      "l1 norm: 809.4573595603429\n",
      "Rbeta: 966.8269245440239\n",
      "\n",
      "Train set: Avg. loss: 0.000328120, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.19062 87.55549\n",
      "l2 norm: 966.0835608525468\n",
      "l1 norm: 809.1347733234038\n",
      "Rbeta: 966.4334715605984\n",
      "\n",
      "Train set: Avg. loss: 0.000325001, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.299034 87.66423\n",
      "l2 norm: 965.7140676099808\n",
      "l1 norm: 808.8313810935413\n",
      "Rbeta: 966.0630235684973\n",
      "\n",
      "Train set: Avg. loss: 0.000321965, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.40466 87.77195\n",
      "l2 norm: 965.3400546168434\n",
      "l1 norm: 808.5240736106772\n",
      "Rbeta: 965.6883563756965\n",
      "\n",
      "Train set: Avg. loss: 0.000318974, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.509865 87.87895\n",
      "l2 norm: 964.9747792820236\n",
      "l1 norm: 808.224066909045\n",
      "Rbeta: 965.322486623458\n",
      "\n",
      "Train set: Avg. loss: 0.000316023, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.615036 87.985085\n",
      "l2 norm: 964.602374200238\n",
      "l1 norm: 807.9179730866414\n",
      "Rbeta: 964.949253707331\n",
      "\n",
      "Train set: Avg. loss: 0.000313119, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.71976 88.09025\n",
      "l2 norm: 964.2462033320562\n",
      "l1 norm: 807.6253548475966\n",
      "Rbeta: 964.5921719276937\n",
      "\n",
      "Train set: Avg. loss: 0.000310271, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.82312 88.19467\n",
      "l2 norm: 963.8996884499803\n",
      "l1 norm: 807.3408779338973\n",
      "Rbeta: 964.2448885221808\n",
      "\n",
      "Train set: Avg. loss: 0.000307460, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.926544 88.298195\n",
      "l2 norm: 963.5592059861451\n",
      "l1 norm: 807.061294032887\n",
      "Rbeta: 963.9034759319488\n",
      "\n",
      "Train set: Avg. loss: 0.000304685, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.02994 88.400955\n",
      "l2 norm: 963.2063274221213\n",
      "l1 norm: 806.7711741544379\n",
      "Rbeta: 963.5496283407614\n",
      "\n",
      "Train set: Avg. loss: 0.000301950, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.13308 88.502815\n",
      "l2 norm: 962.8700922509864\n",
      "l1 norm: 806.4949552357587\n",
      "Rbeta: 963.2123563628357\n",
      "\n",
      "Train set: Avg. loss: 0.000299257, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.2356 88.604004\n",
      "l2 norm: 962.5336469774747\n",
      "l1 norm: 806.2185078806867\n",
      "Rbeta: 962.8748090905997\n",
      "\n",
      "Train set: Avg. loss: 0.000296619, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.33658 88.70437\n",
      "l2 norm: 962.2048621447803\n",
      "l1 norm: 805.9485953229237\n",
      "Rbeta: 962.5451668181586\n",
      "\n",
      "Train set: Avg. loss: 0.000294035, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.435936 88.803955\n",
      "l2 norm: 961.8844333020422\n",
      "l1 norm: 805.6855013332829\n",
      "Rbeta: 962.2238308118333\n",
      "\n",
      "Train set: Avg. loss: 0.000291498, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.53404 88.90292\n",
      "l2 norm: 961.5665164620907\n",
      "l1 norm: 805.4245160778404\n",
      "Rbeta: 961.9051198708111\n",
      "\n",
      "Train set: Avg. loss: 0.000288996, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.63189 89.001114\n",
      "l2 norm: 961.2513747252071\n",
      "l1 norm: 805.1657750264253\n",
      "Rbeta: 961.5892143815947\n",
      "\n",
      "Train set: Avg. loss: 0.000286526, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.72978 89.09842\n",
      "l2 norm: 960.9187991725366\n",
      "l1 norm: 804.8924630243914\n",
      "Rbeta: 961.2557304551452\n",
      "\n",
      "Train set: Avg. loss: 0.000284085, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.82773 89.19503\n",
      "l2 norm: 960.6072760134293\n",
      "l1 norm: 804.6366697532821\n",
      "Rbeta: 960.9432485387306\n",
      "\n",
      "Train set: Avg. loss: 0.000281673, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.92573 89.290955\n",
      "l2 norm: 960.3048175460513\n",
      "l1 norm: 804.3884782552811\n",
      "Rbeta: 960.6395730009892\n",
      "\n",
      "Train set: Avg. loss: 0.000279289, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.023735 89.38621\n",
      "l2 norm: 960.0014660877446\n",
      "l1 norm: 804.1394007947133\n",
      "Rbeta: 960.3351262614577\n",
      "\n",
      "Train set: Avg. loss: 0.000276936, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.12158 89.48082\n",
      "l2 norm: 959.6854481246851\n",
      "l1 norm: 803.8798200315007\n",
      "Rbeta: 960.0178177368872\n",
      "\n",
      "Train set: Avg. loss: 0.000274642, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.21679 89.574745\n",
      "l2 norm: 959.3745820546463\n",
      "l1 norm: 803.624387766627\n",
      "Rbeta: 959.7060019437831\n",
      "\n",
      "Train set: Avg. loss: 0.000272390, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.310844 89.66796\n",
      "l2 norm: 959.0740488762707\n",
      "l1 norm: 803.3777339511635\n",
      "Rbeta: 959.4046081636042\n",
      "\n",
      "Train set: Avg. loss: 0.000270178, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.40374 89.76051\n",
      "l2 norm: 958.7768182261516\n",
      "l1 norm: 803.1336692308475\n",
      "Rbeta: 959.1065422406074\n",
      "\n",
      "Train set: Avg. loss: 0.000267993, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.496506 89.85232\n",
      "l2 norm: 958.4903692699909\n",
      "l1 norm: 802.8986097601453\n",
      "Rbeta: 958.8192228286367\n",
      "\n",
      "Train set: Avg. loss: 0.000265835, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.589226 89.94355\n",
      "l2 norm: 958.1814107394455\n",
      "l1 norm: 802.6444498648286\n",
      "Rbeta: 958.5092236882629\n",
      "\n",
      "Train set: Avg. loss: 0.000263700, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.681854 90.03423\n",
      "l2 norm: 957.8878996239835\n",
      "l1 norm: 802.4032992472701\n",
      "Rbeta: 958.214798400498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000261605, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.77322 90.12428\n",
      "l2 norm: 957.5966049640075\n",
      "l1 norm: 802.1638818404817\n",
      "Rbeta: 957.9225125342931\n",
      "\n",
      "Train set: Avg. loss: 0.000259568, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.86168 90.21363\n",
      "l2 norm: 957.3004527924555\n",
      "l1 norm: 801.920527419046\n",
      "Rbeta: 957.625867488252\n",
      "\n",
      "Train set: Avg. loss: 0.000257563, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.949265 90.30239\n",
      "l2 norm: 957.011728379673\n",
      "l1 norm: 801.6832447128332\n",
      "Rbeta: 957.3365263846804\n",
      "\n",
      "Train set: Avg. loss: 0.000255585, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.03646 90.39046\n",
      "l2 norm: 956.7353547185542\n",
      "l1 norm: 801.4562497923041\n",
      "Rbeta: 957.0595501893255\n",
      "\n",
      "Train set: Avg. loss: 0.000253643, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.122375 90.47803\n",
      "l2 norm: 956.4708798598069\n",
      "l1 norm: 801.2391773000643\n",
      "Rbeta: 956.7946267186185\n",
      "\n",
      "Train set: Avg. loss: 0.000251735, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.20709 90.565\n",
      "l2 norm: 956.1955060788881\n",
      "l1 norm: 801.0129529956332\n",
      "Rbeta: 956.5188528070498\n",
      "\n",
      "Train set: Avg. loss: 0.000249855, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.29117 90.65138\n",
      "l2 norm: 955.9087105759323\n",
      "l1 norm: 800.7770529268566\n",
      "Rbeta: 956.2317028005887\n",
      "\n",
      "Train set: Avg. loss: 0.000247996, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.37518 90.73718\n",
      "l2 norm: 955.631340696237\n",
      "l1 norm: 800.5488692905383\n",
      "Rbeta: 955.9538892447836\n",
      "\n",
      "Train set: Avg. loss: 0.000246171, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.457794 90.822525\n",
      "l2 norm: 955.3440408004802\n",
      "l1 norm: 800.3125247510484\n",
      "Rbeta: 955.6662730336941\n",
      "\n",
      "Train set: Avg. loss: 0.000244370, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.53998 90.90734\n",
      "l2 norm: 955.0947067796587\n",
      "l1 norm: 800.1079085170245\n",
      "Rbeta: 955.4166682311757\n",
      "\n",
      "Train set: Avg. loss: 0.000242590, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.62196 90.9916\n",
      "l2 norm: 954.8172753368094\n",
      "l1 norm: 799.8794800282491\n",
      "Rbeta: 955.138755441019\n",
      "\n",
      "Train set: Avg. loss: 0.000240828, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.703995 91.07531\n",
      "l2 norm: 954.547477996028\n",
      "l1 norm: 799.6576557551101\n",
      "Rbeta: 954.8685302125903\n",
      "\n",
      "Train set: Avg. loss: 0.000239085, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.786064 91.15846\n",
      "l2 norm: 954.301087075875\n",
      "l1 norm: 799.4553967223503\n",
      "Rbeta: 954.621589189553\n",
      "\n",
      "Train set: Avg. loss: 0.000237360, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.86817 91.241005\n",
      "l2 norm: 954.0394983370808\n",
      "l1 norm: 799.240376941338\n",
      "Rbeta: 954.3594828015027\n",
      "\n",
      "Train set: Avg. loss: 0.000235652, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.95032 91.323105\n",
      "l2 norm: 953.7951759452629\n",
      "l1 norm: 799.0400120207738\n",
      "Rbeta: 954.1145172508146\n",
      "\n",
      "Train set: Avg. loss: 0.000233961, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.0325 91.40474\n",
      "l2 norm: 953.5409070740541\n",
      "l1 norm: 798.8311405642667\n",
      "Rbeta: 953.8595621495231\n",
      "\n",
      "Train set: Avg. loss: 0.000232302, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.11329 91.48584\n",
      "l2 norm: 953.3022467198599\n",
      "l1 norm: 798.6352781849832\n",
      "Rbeta: 953.6202807843222\n",
      "\n",
      "Train set: Avg. loss: 0.000230663, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.19383 91.5663\n",
      "l2 norm: 953.0664375378342\n",
      "l1 norm: 798.441861483453\n",
      "Rbeta: 953.3838844570569\n",
      "\n",
      "Train set: Avg. loss: 0.000229049, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.27344 91.6464\n",
      "l2 norm: 952.8058299647524\n",
      "l1 norm: 798.2277248888868\n",
      "Rbeta: 953.1226543742163\n",
      "\n",
      "Train set: Avg. loss: 0.000227460, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.35216 91.72598\n",
      "l2 norm: 952.5606159572795\n",
      "l1 norm: 798.026443770346\n",
      "Rbeta: 952.8769322098257\n",
      "\n",
      "Train set: Avg. loss: 0.000225894, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.430115 91.805115\n",
      "l2 norm: 952.3270657568809\n",
      "l1 norm: 797.8348439995484\n",
      "Rbeta: 952.6430128054242\n",
      "\n",
      "Train set: Avg. loss: 0.000224355, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.50689 91.883865\n",
      "l2 norm: 952.0837504860181\n",
      "l1 norm: 797.6349593955747\n",
      "Rbeta: 952.3992392014114\n",
      "\n",
      "Train set: Avg. loss: 0.000222832, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.58365 91.96206\n",
      "l2 norm: 951.8489577981236\n",
      "l1 norm: 797.4420871013126\n",
      "Rbeta: 952.1641598663552\n",
      "\n",
      "Train set: Avg. loss: 0.000221325, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.66032 92.03975\n",
      "l2 norm: 951.6198098971556\n",
      "l1 norm: 797.253933303133\n",
      "Rbeta: 951.9344838092952\n",
      "\n",
      "Train set: Avg. loss: 0.000219831, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.73703 92.11706\n",
      "l2 norm: 951.3726695434581\n",
      "l1 norm: 797.0509706363513\n",
      "Rbeta: 951.6868771603032\n",
      "\n",
      "Train set: Avg. loss: 0.000218360, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.81284 92.193985\n",
      "l2 norm: 951.1345664446633\n",
      "l1 norm: 796.8553027992323\n",
      "Rbeta: 951.4483350910369\n",
      "\n",
      "Train set: Avg. loss: 0.000216910, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.88795 92.27035\n",
      "l2 norm: 950.911293808163\n",
      "l1 norm: 796.672133950792\n",
      "Rbeta: 951.2245788842031\n",
      "\n",
      "Train set: Avg. loss: 0.000215475, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.96309 92.346245\n",
      "l2 norm: 950.6666431005142\n",
      "l1 norm: 796.4711050857811\n",
      "Rbeta: 950.9794682467029\n",
      "\n",
      "Train set: Avg. loss: 0.000214053, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.03826 92.421646\n",
      "l2 norm: 950.4472414125639\n",
      "l1 norm: 796.2911499658129\n",
      "Rbeta: 950.7595016607436\n",
      "\n",
      "Train set: Avg. loss: 0.000212645, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.11342 92.49658\n",
      "l2 norm: 950.2461727114203\n",
      "l1 norm: 796.1263727366797\n",
      "Rbeta: 950.5578649838872\n",
      "\n",
      "Train set: Avg. loss: 0.000211251, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.18849 92.57114\n",
      "l2 norm: 950.0224974751973\n",
      "l1 norm: 795.9425855292295\n",
      "Rbeta: 950.3336125965193\n",
      "\n",
      "Train set: Avg. loss: 0.000209869, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.263596 92.64527\n",
      "l2 norm: 949.810298017992\n",
      "l1 norm: 795.7684301081011\n",
      "Rbeta: 950.1207710818557\n",
      "\n",
      "Train set: Avg. loss: 0.000208499, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.33872 92.71912\n",
      "l2 norm: 949.5716503266489\n",
      "l1 norm: 795.5720664972785\n",
      "Rbeta: 949.8813091751456\n",
      "\n",
      "Train set: Avg. loss: 0.000207142, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.413895 92.79255\n",
      "l2 norm: 949.3449034275525\n",
      "l1 norm: 795.3856369231104\n",
      "Rbeta: 949.6538767602735\n",
      "\n",
      "Train set: Avg. loss: 0.000205811, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.48756 92.865524\n",
      "l2 norm: 949.1336165698661\n",
      "l1 norm: 795.2122128153461\n",
      "Rbeta: 949.4419457163281\n",
      "\n",
      "Train set: Avg. loss: 0.000204495, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.560875 92.93812\n",
      "l2 norm: 948.9050669538117\n",
      "l1 norm: 795.0244117016615\n",
      "Rbeta: 949.2127138266793\n",
      "\n",
      "Train set: Avg. loss: 0.000203204, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.63279 93.01025\n",
      "l2 norm: 948.692389593923\n",
      "l1 norm: 794.8496641022282\n",
      "Rbeta: 948.9995016721348\n",
      "\n",
      "Train set: Avg. loss: 0.000201929, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.70423 93.08202\n",
      "l2 norm: 948.474364509218\n",
      "l1 norm: 794.6704772569163\n",
      "Rbeta: 948.781094231751\n",
      "\n",
      "Train set: Avg. loss: 0.000200668, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.77539 93.15337\n",
      "l2 norm: 948.2662141927733\n",
      "l1 norm: 794.4995565616143\n",
      "Rbeta: 948.5724420604377\n",
      "\n",
      "Train set: Avg. loss: 0.000199417, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.84657 93.224335\n",
      "l2 norm: 948.0561779048401\n",
      "l1 norm: 794.3271464136088\n",
      "Rbeta: 948.361779440218\n",
      "\n",
      "Train set: Avg. loss: 0.000198178, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.917786 93.29493\n",
      "l2 norm: 947.8408197922008\n",
      "l1 norm: 794.150176572921\n",
      "Rbeta: 948.1458220434257\n",
      "\n",
      "Train set: Avg. loss: 0.000196950, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.98902 93.36509\n",
      "l2 norm: 947.6273367289623\n",
      "l1 norm: 793.9748247747029\n",
      "Rbeta: 947.9316873805701\n",
      "\n",
      "Train set: Avg. loss: 0.000195733, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.060295 93.43486\n",
      "l2 norm: 947.4137377887901\n",
      "l1 norm: 793.7993176959876\n",
      "Rbeta: 947.7173993337137\n",
      "\n",
      "Train set: Avg. loss: 0.000194525, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.13158 93.50435\n",
      "l2 norm: 947.2117470165842\n",
      "l1 norm: 793.6333608692912\n",
      "Rbeta: 947.5147186698711\n",
      "\n",
      "Train set: Avg. loss: 0.000193329, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.20273 93.5735\n",
      "l2 norm: 947.0019004928448\n",
      "l1 norm: 793.4607356993893\n",
      "Rbeta: 947.3041321577485\n",
      "\n",
      "Train set: Avg. loss: 0.000192165, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.27149 93.64214\n",
      "l2 norm: 946.7982640750482\n",
      "l1 norm: 793.2931944640724\n",
      "Rbeta: 947.0999228362616\n",
      "\n",
      "Train set: Avg. loss: 0.000191017, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.33935 93.710556\n",
      "l2 norm: 946.6016106768229\n",
      "l1 norm: 793.1317120014752\n",
      "Rbeta: 946.9028151007168\n",
      "\n",
      "Train set: Avg. loss: 0.000189879, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.40722 93.778534\n",
      "l2 norm: 946.4060496443866\n",
      "l1 norm: 792.9712925909298\n",
      "Rbeta: 946.7068754484837\n",
      "\n",
      "Train set: Avg. loss: 0.000188753, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.474976 93.84615\n",
      "l2 norm: 946.212736376215\n",
      "l1 norm: 792.8127765999545\n",
      "Rbeta: 946.5130825280421\n",
      "\n",
      "Train set: Avg. loss: 0.000187636, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.5427 93.913414\n",
      "l2 norm: 946.0274881715469\n",
      "l1 norm: 792.6609601624634\n",
      "Rbeta: 946.3272098488186\n",
      "\n",
      "Train set: Avg. loss: 0.000186529, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.61026 93.98045\n",
      "l2 norm: 945.8510814231653\n",
      "l1 norm: 792.5166215233637\n",
      "Rbeta: 946.1503254969001\n",
      "\n",
      "Train set: Avg. loss: 0.000185431, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.67774 94.04724\n",
      "l2 norm: 945.6596679023817\n",
      "l1 norm: 792.3595243320634\n",
      "Rbeta: 945.9583206964642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000184343, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.74525 94.11364\n",
      "l2 norm: 945.4736274517091\n",
      "l1 norm: 792.2067905526496\n",
      "Rbeta: 945.7716832521784\n",
      "\n",
      "Train set: Avg. loss: 0.000183265, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.81263 94.179695\n",
      "l2 norm: 945.2835764517858\n",
      "l1 norm: 792.0508002361456\n",
      "Rbeta: 945.5810048088064\n",
      "\n",
      "Train set: Avg. loss: 0.000182208, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.8785 94.24536\n",
      "l2 norm: 945.0881225000566\n",
      "l1 norm: 791.8902031625857\n",
      "Rbeta: 945.3850874166136\n",
      "\n",
      "Train set: Avg. loss: 0.000181160, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.94441 94.3107\n",
      "l2 norm: 944.8999737032519\n",
      "l1 norm: 791.7357782327233\n",
      "Rbeta: 945.1964185864086\n",
      "\n",
      "Train set: Avg. loss: 0.000180121, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.01033 94.375656\n",
      "l2 norm: 944.7187626250177\n",
      "l1 norm: 791.587078781669\n",
      "Rbeta: 945.0145884635817\n",
      "\n",
      "Train set: Avg. loss: 0.000179090, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.07628 94.44037\n",
      "l2 norm: 944.5183445965334\n",
      "l1 norm: 791.4224023748523\n",
      "Rbeta: 944.8135367265694\n",
      "\n",
      "Train set: Avg. loss: 0.000178067, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.14225 94.50473\n",
      "l2 norm: 944.3341381161586\n",
      "l1 norm: 791.2710404373655\n",
      "Rbeta: 944.6287079584546\n",
      "\n",
      "Train set: Avg. loss: 0.000177053, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.208244 94.568634\n",
      "l2 norm: 944.15653186718\n",
      "l1 norm: 791.125349156831\n",
      "Rbeta: 944.4504504929927\n",
      "\n",
      "Train set: Avg. loss: 0.000176051, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.273605 94.63235\n",
      "l2 norm: 943.9696098229464\n",
      "l1 norm: 790.9716304905237\n",
      "Rbeta: 944.2628296804437\n",
      "\n",
      "Train set: Avg. loss: 0.000175066, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.33797 94.695656\n",
      "l2 norm: 943.7838978120909\n",
      "l1 norm: 790.8189676704412\n",
      "Rbeta: 944.0766489413709\n",
      "\n",
      "Train set: Avg. loss: 0.000174088, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.40234 94.7587\n",
      "l2 norm: 943.5873888600975\n",
      "l1 norm: 790.6572881253787\n",
      "Rbeta: 943.879503766115\n",
      "\n",
      "Train set: Avg. loss: 0.000173129, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.46529 94.82148\n",
      "l2 norm: 943.3887328995481\n",
      "l1 norm: 790.4940334306633\n",
      "Rbeta: 943.6803941326056\n",
      "\n",
      "Train set: Avg. loss: 0.000172189, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.52666 94.88398\n",
      "l2 norm: 943.20575017028\n",
      "l1 norm: 790.3437548083216\n",
      "Rbeta: 943.4971334597477\n",
      "\n",
      "Train set: Avg. loss: 0.000171260, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.58755 94.94613\n",
      "l2 norm: 943.0227214558203\n",
      "l1 norm: 790.1932251385633\n",
      "Rbeta: 943.3137808338633\n",
      "\n",
      "Train set: Avg. loss: 0.000170340, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.64822 95.00793\n",
      "l2 norm: 942.8484670798068\n",
      "l1 norm: 790.0501211054195\n",
      "Rbeta: 943.1392258265922\n",
      "\n",
      "Train set: Avg. loss: 0.000169428, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.70883 95.06946\n",
      "l2 norm: 942.6651849708062\n",
      "l1 norm: 789.8995130563679\n",
      "Rbeta: 942.9556364149024\n",
      "\n",
      "Train set: Avg. loss: 0.000168523, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.769455 95.13065\n",
      "l2 norm: 942.4906965824457\n",
      "l1 norm: 789.7562267092982\n",
      "Rbeta: 942.7808513902293\n",
      "\n",
      "Train set: Avg. loss: 0.000167625, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.8301 95.19154\n",
      "l2 norm: 942.3235930376479\n",
      "l1 norm: 789.6191722394351\n",
      "Rbeta: 942.613283252719\n",
      "\n",
      "Train set: Avg. loss: 0.000166734, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.89078 95.25216\n",
      "l2 norm: 942.1628104443704\n",
      "l1 norm: 789.4873368803261\n",
      "Rbeta: 942.4522353912204\n",
      "\n",
      "Train set: Avg. loss: 0.000165850, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.95146 95.31233\n",
      "l2 norm: 941.9945882366809\n",
      "l1 norm: 789.3493058358974\n",
      "Rbeta: 942.2834538304951\n",
      "\n",
      "Train set: Avg. loss: 0.000164974, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.01217 95.372215\n",
      "l2 norm: 941.8349883142793\n",
      "l1 norm: 789.2185914607326\n",
      "Rbeta: 942.1233619855925\n",
      "\n",
      "Train set: Avg. loss: 0.000164102, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.0729 95.43195\n",
      "l2 norm: 941.6710484922867\n",
      "l1 norm: 789.0839569005124\n",
      "Rbeta: 941.958944980711\n",
      "\n",
      "Train set: Avg. loss: 0.000163238, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.13364 95.49144\n",
      "l2 norm: 941.4892029454433\n",
      "l1 norm: 788.9343846513639\n",
      "Rbeta: 941.7764979323579\n",
      "\n",
      "Train set: Avg. loss: 0.000162379, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.19432 95.55079\n",
      "l2 norm: 941.3170388852236\n",
      "l1 norm: 788.7930206085159\n",
      "Rbeta: 941.6038327582418\n",
      "\n",
      "Train set: Avg. loss: 0.000161527, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.25489 95.6098\n",
      "l2 norm: 941.1395153796813\n",
      "l1 norm: 788.6470125307571\n",
      "Rbeta: 941.4256064216121\n",
      "\n",
      "Train set: Avg. loss: 0.000160683, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.31536 95.66851\n",
      "l2 norm: 940.9610061570513\n",
      "l1 norm: 788.5000942351589\n",
      "Rbeta: 941.246531723618\n",
      "\n",
      "Train set: Avg. loss: 0.000159846, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.37586 95.72694\n",
      "l2 norm: 940.789021834459\n",
      "l1 norm: 788.3585913177499\n",
      "Rbeta: 941.073924809758\n",
      "\n",
      "Train set: Avg. loss: 0.000159015, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.43621 95.78515\n",
      "l2 norm: 940.6349592812811\n",
      "l1 norm: 788.2322215403855\n",
      "Rbeta: 940.9192076821562\n",
      "\n",
      "Train set: Avg. loss: 0.000158197, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.4955 95.84321\n",
      "l2 norm: 940.4787485591465\n",
      "l1 norm: 788.1039379279675\n",
      "Rbeta: 940.7625555786447\n",
      "\n",
      "Train set: Avg. loss: 0.000157390, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.55402 95.90105\n",
      "l2 norm: 940.3122080686433\n",
      "l1 norm: 787.967047146856\n",
      "Rbeta: 940.5955486887034\n",
      "\n",
      "Train set: Avg. loss: 0.000156590, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.612526 95.95856\n",
      "l2 norm: 940.1627895096938\n",
      "l1 norm: 787.8444830533608\n",
      "Rbeta: 940.4455209552144\n",
      "\n",
      "Train set: Avg. loss: 0.000155797, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.67092 96.01566\n",
      "l2 norm: 939.998588924416\n",
      "l1 norm: 787.709325802343\n",
      "Rbeta: 940.2807854280679\n",
      "\n",
      "Train set: Avg. loss: 0.000155010, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.72934 96.072556\n",
      "l2 norm: 939.836887137886\n",
      "l1 norm: 787.5762822951351\n",
      "Rbeta: 940.1186131101009\n",
      "\n",
      "Train set: Avg. loss: 0.000154227, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.787704 96.12928\n",
      "l2 norm: 939.6928888654815\n",
      "l1 norm: 787.4580625546464\n",
      "Rbeta: 939.9740531924908\n",
      "\n",
      "Train set: Avg. loss: 0.000153452, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.84599 96.185684\n",
      "l2 norm: 939.549586234035\n",
      "l1 norm: 787.3407607203153\n",
      "Rbeta: 939.8301753403355\n",
      "\n",
      "Train set: Avg. loss: 0.000152683, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.9043 96.24175\n",
      "l2 norm: 939.3897229255067\n",
      "l1 norm: 787.2097398145941\n",
      "Rbeta: 939.6696773263996\n",
      "\n",
      "Train set: Avg. loss: 0.000151919, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.96252 96.297646\n",
      "l2 norm: 939.2452809363134\n",
      "l1 norm: 787.0915842580182\n",
      "Rbeta: 939.5245469150782\n",
      "\n",
      "Train set: Avg. loss: 0.000151174, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.01873 96.35329\n",
      "l2 norm: 939.0937682904262\n",
      "l1 norm: 786.9672316905946\n",
      "Rbeta: 939.3725636482882\n",
      "\n",
      "Train set: Avg. loss: 0.000150443, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.07368 96.40867\n",
      "l2 norm: 938.9294858493021\n",
      "l1 norm: 786.8322389648283\n",
      "Rbeta: 939.2080519440982\n",
      "\n",
      "Train set: Avg. loss: 0.000149717, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.12864 96.46372\n",
      "l2 norm: 938.7768695940258\n",
      "l1 norm: 786.7069116976653\n",
      "Rbeta: 939.0550147904539\n",
      "\n",
      "Train set: Avg. loss: 0.000148996, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.183624 96.51854\n",
      "l2 norm: 938.6435710126487\n",
      "l1 norm: 786.5976891115224\n",
      "Rbeta: 938.9214004382468\n",
      "\n",
      "Train set: Avg. loss: 0.000148286, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.23774 96.57314\n",
      "l2 norm: 938.4986794400656\n",
      "l1 norm: 786.4788259175143\n",
      "Rbeta: 938.776263434696\n",
      "\n",
      "Train set: Avg. loss: 0.000147586, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.29104 96.62755\n",
      "l2 norm: 938.3427071549016\n",
      "l1 norm: 786.3506498817376\n",
      "Rbeta: 938.6200046760308\n",
      "\n",
      "Train set: Avg. loss: 0.000146889, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.34436 96.6818\n",
      "l2 norm: 938.2000348953266\n",
      "l1 norm: 786.2336304004473\n",
      "Rbeta: 938.4771167360401\n",
      "\n",
      "Train set: Avg. loss: 0.000146197, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.3977 96.73595\n",
      "l2 norm: 938.0484087771981\n",
      "l1 norm: 786.1088816172794\n",
      "Rbeta: 938.3252401195078\n",
      "\n",
      "Train set: Avg. loss: 0.000145510, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.45104 96.789856\n",
      "l2 norm: 937.9012623660865\n",
      "l1 norm: 785.9880854523445\n",
      "Rbeta: 938.1778626308342\n",
      "\n",
      "Train set: Avg. loss: 0.000144827, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.50441 96.843475\n",
      "l2 norm: 937.755344667486\n",
      "l1 norm: 785.868370440429\n",
      "Rbeta: 938.031616445087\n",
      "\n",
      "Train set: Avg. loss: 0.000144149, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.55775 96.89695\n",
      "l2 norm: 937.6262132654314\n",
      "l1 norm: 785.7627439190412\n",
      "Rbeta: 937.9021850296698\n",
      "\n",
      "Train set: Avg. loss: 0.000143476, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.61097 96.950195\n",
      "l2 norm: 937.4946572068179\n",
      "l1 norm: 785.6550246803774\n",
      "Rbeta: 937.7702311305684\n",
      "\n",
      "Train set: Avg. loss: 0.000142808, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.664215 97.003136\n",
      "l2 norm: 937.337735134984\n",
      "l1 norm: 785.5259055838262\n",
      "Rbeta: 937.6130748232762\n",
      "\n",
      "Train set: Avg. loss: 0.000142145, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.71732 97.05576\n",
      "l2 norm: 937.1734619444189\n",
      "l1 norm: 785.3905520920014\n",
      "Rbeta: 937.4482898935286\n",
      "\n",
      "Train set: Avg. loss: 0.000141487, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.77043 97.10824\n",
      "l2 norm: 937.0215182256132\n",
      "l1 norm: 785.2654679286353\n",
      "Rbeta: 937.2960233474037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000140834, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.8235 97.16052\n",
      "l2 norm: 936.8648054990073\n",
      "l1 norm: 785.1365739943499\n",
      "Rbeta: 937.1387886711982\n",
      "\n",
      "Train set: Avg. loss: 0.000140184, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.876465 97.21271\n",
      "l2 norm: 936.7230770852016\n",
      "l1 norm: 785.0202840720935\n",
      "Rbeta: 936.9966948284693\n",
      "\n",
      "Train set: Avg. loss: 0.000139539, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.92945 97.264626\n",
      "l2 norm: 936.5944189712652\n",
      "l1 norm: 784.914855980313\n",
      "Rbeta: 936.8676267699525\n",
      "\n",
      "Train set: Avg. loss: 0.000138899, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.982445 97.316345\n",
      "l2 norm: 936.4443237135474\n",
      "l1 norm: 784.7913913181937\n",
      "Rbeta: 936.7170852042145\n",
      "\n",
      "Train set: Avg. loss: 0.000138262, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.035446 97.3679\n",
      "l2 norm: 936.3120878063202\n",
      "l1 norm: 784.6829814479092\n",
      "Rbeta: 936.584329698141\n",
      "\n",
      "Train set: Avg. loss: 0.000137630, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.08848 97.41919\n",
      "l2 norm: 936.1716403524163\n",
      "l1 norm: 784.5676445436457\n",
      "Rbeta: 936.4432866228191\n",
      "\n",
      "Train set: Avg. loss: 0.000137002, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.14152 97.470245\n",
      "l2 norm: 936.0405250235958\n",
      "l1 norm: 784.4600479042219\n",
      "Rbeta: 936.3116710943286\n",
      "\n",
      "Train set: Avg. loss: 0.000136377, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.194565 97.52113\n",
      "l2 norm: 935.9048959827197\n",
      "l1 norm: 784.3487263260133\n",
      "Rbeta: 936.1755154383541\n",
      "\n",
      "Train set: Avg. loss: 0.000135758, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.247505 97.57178\n",
      "l2 norm: 935.7685958906636\n",
      "l1 norm: 784.2368120725762\n",
      "Rbeta: 936.0385722896384\n",
      "\n",
      "Train set: Avg. loss: 0.000135148, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.29935 97.622375\n",
      "l2 norm: 935.634415551776\n",
      "l1 norm: 784.1266390776507\n",
      "Rbeta: 935.9039393379818\n",
      "\n",
      "Train set: Avg. loss: 0.000134546, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.350555 97.67283\n",
      "l2 norm: 935.4983837749253\n",
      "l1 norm: 784.0148375625433\n",
      "Rbeta: 935.7675151624513\n",
      "\n",
      "Train set: Avg. loss: 0.000133948, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.40178 97.72311\n",
      "l2 norm: 935.366397806966\n",
      "l1 norm: 783.9064554944791\n",
      "Rbeta: 935.635169952701\n",
      "\n",
      "Train set: Avg. loss: 0.000133354, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.45287 97.77309\n",
      "l2 norm: 935.2292594177388\n",
      "l1 norm: 783.7939207386842\n",
      "Rbeta: 935.4975708545409\n",
      "\n",
      "Train set: Avg. loss: 0.000132774, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.5024 97.82273\n",
      "l2 norm: 935.0971601031044\n",
      "l1 norm: 783.6856757341282\n",
      "Rbeta: 935.365216942802\n",
      "\n",
      "Train set: Avg. loss: 0.000132198, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.55195 97.87222\n",
      "l2 norm: 934.9809685082885\n",
      "l1 norm: 783.5906435922475\n",
      "Rbeta: 935.2486331746248\n",
      "\n",
      "Train set: Avg. loss: 0.000131624, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.6015 97.92158\n",
      "l2 norm: 934.8392878945473\n",
      "l1 norm: 783.4741651180602\n",
      "Rbeta: 935.106702120446\n",
      "\n",
      "Train set: Avg. loss: 0.000131059, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.65037 97.97067\n",
      "l2 norm: 934.7168815324502\n",
      "l1 norm: 783.3739213648705\n",
      "Rbeta: 934.9840012988525\n",
      "\n",
      "Train set: Avg. loss: 0.000130503, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.69813 98.01969\n",
      "l2 norm: 934.5985186672436\n",
      "l1 norm: 783.2771244757996\n",
      "Rbeta: 934.8655784480128\n",
      "\n",
      "Train set: Avg. loss: 0.000129951, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.74567 98.068504\n",
      "l2 norm: 934.4593658120231\n",
      "l1 norm: 783.1627535717234\n",
      "Rbeta: 934.7262855649901\n",
      "\n",
      "Train set: Avg. loss: 0.000129402, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.79322 98.11723\n",
      "l2 norm: 934.3122389827324\n",
      "l1 norm: 783.0416671778082\n",
      "Rbeta: 934.579003168408\n",
      "\n",
      "Train set: Avg. loss: 0.000128857, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.840775 98.16571\n",
      "l2 norm: 934.1823208549279\n",
      "l1 norm: 782.9350770933343\n",
      "Rbeta: 934.4488134877358\n",
      "\n",
      "Train set: Avg. loss: 0.000128316, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.88835 98.21391\n",
      "l2 norm: 934.0445108627587\n",
      "l1 norm: 782.8217811512902\n",
      "Rbeta: 934.3107637216715\n",
      "\n",
      "Train set: Avg. loss: 0.000127779, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.93594 98.26183\n",
      "l2 norm: 933.927021845292\n",
      "l1 norm: 782.7255045116458\n",
      "Rbeta: 934.1931474986807\n",
      "\n",
      "Train set: Avg. loss: 0.000127249, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.982765 98.30959\n",
      "l2 norm: 933.8225338111489\n",
      "l1 norm: 782.6400409001674\n",
      "Rbeta: 934.0884585963065\n",
      "\n",
      "Train set: Avg. loss: 0.000126727, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.028656 98.357155\n",
      "l2 norm: 933.696576356646\n",
      "l1 norm: 782.5367750044667\n",
      "Rbeta: 933.9623341866519\n",
      "\n",
      "Train set: Avg. loss: 0.000126208, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.074554 98.4047\n",
      "l2 norm: 933.5652298843485\n",
      "l1 norm: 782.4288858108077\n",
      "Rbeta: 933.8308303339609\n",
      "\n",
      "Train set: Avg. loss: 0.000125692, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.12047 98.45208\n",
      "l2 norm: 933.4303767956203\n",
      "l1 norm: 782.317795957122\n",
      "Rbeta: 933.6959179807416\n",
      "\n",
      "Train set: Avg. loss: 0.000125178, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.16639 98.49935\n",
      "l2 norm: 933.3110086705215\n",
      "l1 norm: 782.2197170998608\n",
      "Rbeta: 933.5764375252761\n",
      "\n",
      "Train set: Avg. loss: 0.000124667, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.212326 98.546486\n",
      "l2 norm: 933.196240781668\n",
      "l1 norm: 782.1255548810952\n",
      "Rbeta: 933.461563934357\n",
      "\n",
      "Train set: Avg. loss: 0.000124159, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.25826 98.59342\n",
      "l2 norm: 933.0880753377246\n",
      "l1 norm: 782.03693219543\n",
      "Rbeta: 933.3531324669001\n",
      "\n",
      "Train set: Avg. loss: 0.000123654, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.30422 98.640175\n",
      "l2 norm: 932.9752252383186\n",
      "l1 norm: 781.9444010470586\n",
      "Rbeta: 933.2401239622901\n",
      "\n",
      "Train set: Avg. loss: 0.000123153, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.3502 98.68667\n",
      "l2 norm: 932.8694282621079\n",
      "l1 norm: 781.8578101391677\n",
      "Rbeta: 933.1341369386854\n",
      "\n",
      "Train set: Avg. loss: 0.000122654, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.39617 98.73304\n",
      "l2 norm: 932.7577550691907\n",
      "l1 norm: 781.7663592570368\n",
      "Rbeta: 933.022245146614\n",
      "\n",
      "Train set: Avg. loss: 0.000122158, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.44216 98.779205\n",
      "l2 norm: 932.6439042357488\n",
      "l1 norm: 781.6731377804751\n",
      "Rbeta: 932.90819353896\n",
      "\n",
      "Train set: Avg. loss: 0.000121665, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.48816 98.82516\n",
      "l2 norm: 932.5444488209813\n",
      "l1 norm: 781.5918935470916\n",
      "Rbeta: 932.8084058211736\n",
      "\n",
      "Train set: Avg. loss: 0.000121175, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.53417 98.871\n",
      "l2 norm: 932.4252206578941\n",
      "l1 norm: 781.4939644069959\n",
      "Rbeta: 932.688922653633\n",
      "\n",
      "Train set: Avg. loss: 0.000120688, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.5802 98.91667\n",
      "l2 norm: 932.3026562971546\n",
      "l1 norm: 781.393400869703\n",
      "Rbeta: 932.5660286083465\n",
      "\n",
      "Train set: Avg. loss: 0.000120204, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.62611 98.96221\n",
      "l2 norm: 932.1697360400826\n",
      "l1 norm: 781.284080104864\n",
      "Rbeta: 932.4328049747579\n",
      "\n",
      "Train set: Avg. loss: 0.000119727, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.67115 99.00759\n",
      "l2 norm: 932.0329952754649\n",
      "l1 norm: 781.1714728916502\n",
      "Rbeta: 932.2958270854799\n",
      "\n",
      "Train set: Avg. loss: 0.000119258, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.715256 99.05281\n",
      "l2 norm: 931.8997609075876\n",
      "l1 norm: 781.0619623308214\n",
      "Rbeta: 932.1624188765481\n",
      "\n",
      "Train set: Avg. loss: 0.000118792, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.759254 99.09786\n",
      "l2 norm: 931.7871585657575\n",
      "l1 norm: 780.9696573923549\n",
      "Rbeta: 932.0496862726555\n",
      "\n",
      "Train set: Avg. loss: 0.000118329, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.80327 99.14277\n",
      "l2 norm: 931.6842093817163\n",
      "l1 norm: 780.8852806310745\n",
      "Rbeta: 931.946573976323\n",
      "\n",
      "Train set: Avg. loss: 0.000117871, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.84689 99.187386\n",
      "l2 norm: 931.5961721955557\n",
      "l1 norm: 780.8134094680748\n",
      "Rbeta: 931.8583466369879\n",
      "\n",
      "Train set: Avg. loss: 0.000117422, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.88919 99.2318\n",
      "l2 norm: 931.4949845960234\n",
      "l1 norm: 780.7305003001197\n",
      "Rbeta: 931.7571673785546\n",
      "\n",
      "Train set: Avg. loss: 0.000116976, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.9315 99.27606\n",
      "l2 norm: 931.3714858126812\n",
      "l1 norm: 780.6288994205097\n",
      "Rbeta: 931.6335978134392\n",
      "\n",
      "Train set: Avg. loss: 0.000116532, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.97383 99.32025\n",
      "l2 norm: 931.2563895130698\n",
      "l1 norm: 780.5342846782011\n",
      "Rbeta: 931.5185298089598\n",
      "\n",
      "Train set: Avg. loss: 0.000116091, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.01615 99.36434\n",
      "l2 norm: 931.1643402628487\n",
      "l1 norm: 780.458953867856\n",
      "Rbeta: 931.426357530116\n",
      "\n",
      "Train set: Avg. loss: 0.000115651, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.05849 99.40834\n",
      "l2 norm: 931.0625769593071\n",
      "l1 norm: 780.375699167591\n",
      "Rbeta: 931.3245486870487\n",
      "\n",
      "Train set: Avg. loss: 0.000115214, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.10084 99.45218\n",
      "l2 norm: 930.9633541289021\n",
      "l1 norm: 780.2944915967984\n",
      "Rbeta: 931.2251773262101\n",
      "\n",
      "Train set: Avg. loss: 0.000114779, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.1432 99.49596\n",
      "l2 norm: 930.8687497444115\n",
      "l1 norm: 780.2171035226349\n",
      "Rbeta: 931.130548194548\n",
      "\n",
      "Train set: Avg. loss: 0.000114346, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.18557 99.53959\n",
      "l2 norm: 930.7602250727722\n",
      "l1 norm: 780.1279403553117\n",
      "Rbeta: 931.0218933399487\n",
      "\n",
      "Train set: Avg. loss: 0.000113915, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.22794 99.58302\n",
      "l2 norm: 930.6335648816481\n",
      "l1 norm: 780.0236129765026\n",
      "Rbeta: 930.8951180485182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000113488, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.270226 99.626305\n",
      "l2 norm: 930.5178884374232\n",
      "l1 norm: 779.9285172311072\n",
      "Rbeta: 930.7793052818494\n",
      "\n",
      "Train set: Avg. loss: 0.000113063, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.312454 99.669464\n",
      "l2 norm: 930.3953568617133\n",
      "l1 norm: 779.8276164964594\n",
      "Rbeta: 930.656599702617\n",
      "\n",
      "Train set: Avg. loss: 0.000112640, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.35469 99.71253\n",
      "l2 norm: 930.2779279858042\n",
      "l1 norm: 779.7311337777229\n",
      "Rbeta: 930.5390938668218\n",
      "\n",
      "Train set: Avg. loss: 0.000112219, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.396935 99.75544\n",
      "l2 norm: 930.1762035063842\n",
      "l1 norm: 779.6479266791266\n",
      "Rbeta: 930.4371150828246\n",
      "\n",
      "Train set: Avg. loss: 0.000111808, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.43771 99.79816\n",
      "l2 norm: 930.0776158722178\n",
      "l1 norm: 779.5671689614686\n",
      "Rbeta: 930.3384928951763\n",
      "\n",
      "Train set: Avg. loss: 0.000111409, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.47651 99.84069\n",
      "l2 norm: 929.9701700394753\n",
      "l1 norm: 779.478787563914\n",
      "Rbeta: 930.2311748257704\n",
      "\n",
      "Train set: Avg. loss: 0.000111012, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.51531 99.88305\n",
      "l2 norm: 929.8661224522681\n",
      "l1 norm: 779.3933308005097\n",
      "Rbeta: 930.127262319629\n",
      "\n",
      "Train set: Avg. loss: 0.000110617, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.55412 99.92537\n",
      "l2 norm: 929.7481967560042\n",
      "l1 norm: 779.2962039565416\n",
      "Rbeta: 930.0094682492927\n",
      "\n",
      "Train set: Avg. loss: 0.000110224, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.59294 99.96754\n",
      "l2 norm: 929.6248485392545\n",
      "l1 norm: 779.1946693968971\n",
      "Rbeta: 929.8863389450992\n",
      "\n",
      "Train set: Avg. loss: 0.000109834, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.6316 100.00958\n",
      "l2 norm: 929.5073179380446\n",
      "l1 norm: 779.0980862829523\n",
      "Rbeta: 929.7688731455797\n",
      "\n",
      "Train set: Avg. loss: 0.000109446, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.67023 100.0514\n",
      "l2 norm: 929.4038287397121\n",
      "l1 norm: 779.0132265160094\n",
      "Rbeta: 929.6654424629936\n",
      "\n",
      "Train set: Avg. loss: 0.000109062, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.70873 100.09302\n",
      "l2 norm: 929.3081733662912\n",
      "l1 norm: 778.9349593557121\n",
      "Rbeta: 929.5700088938542\n",
      "\n",
      "Train set: Avg. loss: 0.000108679, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.747246 100.134445\n",
      "l2 norm: 929.2158374680578\n",
      "l1 norm: 778.859349178301\n",
      "Rbeta: 929.4776682857025\n",
      "\n",
      "Train set: Avg. loss: 0.000108298, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.78577 100.1758\n",
      "l2 norm: 929.1095428560459\n",
      "l1 norm: 778.7720832647352\n",
      "Rbeta: 929.3714770635504\n",
      "\n",
      "Train set: Avg. loss: 0.000107919, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.82429 100.21706\n",
      "l2 norm: 929.0042816482627\n",
      "l1 norm: 778.6855943963648\n",
      "Rbeta: 929.26627538835\n",
      "\n",
      "Train set: Avg. loss: 0.000107543, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.86265 100.258125\n",
      "l2 norm: 928.9135110602058\n",
      "l1 norm: 778.6113192858375\n",
      "Rbeta: 929.1755193882008\n",
      "\n",
      "Train set: Avg. loss: 0.000107169, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.901024 100.29917\n",
      "l2 norm: 928.8072159247472\n",
      "l1 norm: 778.5239044991619\n",
      "Rbeta: 929.0693173766134\n",
      "\n",
      "Train set: Avg. loss: 0.000106795, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.93941 100.34011\n",
      "l2 norm: 928.6949078052356\n",
      "l1 norm: 778.4314489474839\n",
      "Rbeta: 928.9570202500873\n",
      "\n",
      "Train set: Avg. loss: 0.000106424, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.97779 100.38097\n",
      "l2 norm: 928.5919499941964\n",
      "l1 norm: 778.3466600094084\n",
      "Rbeta: 928.8541112540145\n",
      "\n",
      "Train set: Avg. loss: 0.000106060, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.01504 100.421715\n",
      "l2 norm: 928.4799160437769\n",
      "l1 norm: 778.2542987587833\n",
      "Rbeta: 928.7421794723394\n",
      "\n",
      "Train set: Avg. loss: 0.000105701, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.051704 100.46232\n",
      "l2 norm: 928.3794452071897\n",
      "l1 norm: 778.171528304016\n",
      "Rbeta: 928.6419489954478\n",
      "\n",
      "Train set: Avg. loss: 0.000105343, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.08837 100.50269\n",
      "l2 norm: 928.2849290274818\n",
      "l1 norm: 778.0938043708618\n",
      "Rbeta: 928.5475998601936\n",
      "\n",
      "Train set: Avg. loss: 0.000104987, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.12505 100.54297\n",
      "l2 norm: 928.1852228863753\n",
      "l1 norm: 778.0117385697138\n",
      "Rbeta: 928.4480850914156\n",
      "\n",
      "Train set: Avg. loss: 0.000104633, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.161736 100.583176\n",
      "l2 norm: 928.0991048585635\n",
      "l1 norm: 777.9410194869246\n",
      "Rbeta: 928.3620104519879\n",
      "\n",
      "Train set: Avg. loss: 0.000104281, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.19843 100.62321\n",
      "l2 norm: 928.0036340774941\n",
      "l1 norm: 777.8625054030058\n",
      "Rbeta: 928.2667599585134\n",
      "\n",
      "Train set: Avg. loss: 0.000103931, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.23512 100.662994\n",
      "l2 norm: 927.9147754376968\n",
      "l1 norm: 777.7896905326999\n",
      "Rbeta: 928.1779434539286\n",
      "\n",
      "Train set: Avg. loss: 0.000103583, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.27183 100.7027\n",
      "l2 norm: 927.8116788193882\n",
      "l1 norm: 777.7049755220792\n",
      "Rbeta: 928.0749091249158\n",
      "\n",
      "Train set: Avg. loss: 0.000103236, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.30854 100.74228\n",
      "l2 norm: 927.6994733912978\n",
      "l1 norm: 777.6125751627776\n",
      "Rbeta: 927.9628137171575\n",
      "\n",
      "Train set: Avg. loss: 0.000102890, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.34526 100.78191\n",
      "l2 norm: 927.59221314937\n",
      "l1 norm: 777.5242804767096\n",
      "Rbeta: 927.8556748464537\n",
      "\n",
      "Train set: Avg. loss: 0.000102546, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.38198 100.82146\n",
      "l2 norm: 927.4960254965016\n",
      "l1 norm: 777.4452928673759\n",
      "Rbeta: 927.7595296545051\n",
      "\n",
      "Train set: Avg. loss: 0.000102206, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.418175 100.860855\n",
      "l2 norm: 927.3891141172661\n",
      "l1 norm: 777.3574303985575\n",
      "Rbeta: 927.6527816471324\n",
      "\n",
      "Train set: Avg. loss: 0.000101873, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.45317 100.9001\n",
      "l2 norm: 927.2856048042103\n",
      "l1 norm: 777.2723740810256\n",
      "Rbeta: 927.5495730408089\n",
      "\n",
      "Train set: Avg. loss: 0.000101541, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.48817 100.93928\n",
      "l2 norm: 927.1965124855559\n",
      "l1 norm: 777.1993879768469\n",
      "Rbeta: 927.4606492902116\n",
      "\n",
      "Train set: Avg. loss: 0.000101211, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.52318 100.97844\n",
      "l2 norm: 927.1006249956438\n",
      "l1 norm: 777.1207356331104\n",
      "Rbeta: 927.364996282944\n",
      "\n",
      "Train set: Avg. loss: 0.000100882, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.55819 101.017494\n",
      "l2 norm: 926.9883871302767\n",
      "l1 norm: 777.0284046681547\n",
      "Rbeta: 927.2529811224484\n",
      "\n",
      "Train set: Avg. loss: 0.000100555, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.59321 101.056335\n",
      "l2 norm: 926.8788680838258\n",
      "l1 norm: 776.9383632738211\n",
      "Rbeta: 927.1436392134846\n",
      "\n",
      "Train set: Avg. loss: 0.000100230, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.6281 101.09504\n",
      "l2 norm: 926.7892800610113\n",
      "l1 norm: 776.8649896645458\n",
      "Rbeta: 927.0543098043876\n",
      "\n",
      "Train set: Avg. loss: 0.000099914, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.66138 101.13365\n",
      "l2 norm: 926.6983915825315\n",
      "l1 norm: 776.790536479314\n",
      "Rbeta: 926.9637157434112\n",
      "\n",
      "Train set: Avg. loss: 0.000099599, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.694626 101.17219\n",
      "l2 norm: 926.6030929577727\n",
      "l1 norm: 776.7123648537752\n",
      "Rbeta: 926.8688297998132\n",
      "\n",
      "Train set: Avg. loss: 0.000099287, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.72775 101.210625\n",
      "l2 norm: 926.5078933366708\n",
      "l1 norm: 776.6341731170671\n",
      "Rbeta: 926.7739949986625\n",
      "\n",
      "Train set: Avg. loss: 0.000098975, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.76088 101.248985\n",
      "l2 norm: 926.4025922570065\n",
      "l1 norm: 776.5474633918011\n",
      "Rbeta: 926.6689969696855\n",
      "\n",
      "Train set: Avg. loss: 0.000098665, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.794014 101.28726\n",
      "l2 norm: 926.288648709007\n",
      "l1 norm: 776.453460434452\n",
      "Rbeta: 926.5553524968151\n",
      "\n",
      "Train set: Avg. loss: 0.000098356, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.827156 101.32547\n",
      "l2 norm: 926.1761922396859\n",
      "l1 norm: 776.3607126724945\n",
      "Rbeta: 926.4432105613629\n",
      "\n",
      "Train set: Avg. loss: 0.000098049, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.8603 101.36353\n",
      "l2 norm: 926.0847816027682\n",
      "l1 norm: 776.2854888338798\n",
      "Rbeta: 926.352144528631\n",
      "\n",
      "Train set: Avg. loss: 0.000097743, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.893456 101.40144\n",
      "l2 norm: 926.003262051891\n",
      "l1 norm: 776.2185456194608\n",
      "Rbeta: 926.2708936991268\n",
      "\n",
      "Train set: Avg. loss: 0.000097439, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.926605 101.43922\n",
      "l2 norm: 925.9286962120477\n",
      "l1 norm: 776.1574767797766\n",
      "Rbeta: 926.1966136062864\n",
      "\n",
      "Train set: Avg. loss: 0.000097136, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.95978 101.47689\n",
      "l2 norm: 925.8386070903996\n",
      "l1 norm: 776.0834494501072\n",
      "Rbeta: 926.1069137943049\n",
      "\n",
      "Train set: Avg. loss: 0.000096834, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.99294 101.51444\n",
      "l2 norm: 925.7622610438755\n",
      "l1 norm: 776.0209905823233\n",
      "Rbeta: 926.0308094758518\n",
      "\n",
      "Train set: Avg. loss: 0.000096534, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.026115 101.551865\n",
      "l2 norm: 925.696965077602\n",
      "l1 norm: 775.9678673768867\n",
      "Rbeta: 925.9657193028462\n",
      "\n",
      "Train set: Avg. loss: 0.000096236, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.059296 101.58918\n",
      "l2 norm: 925.6285449059711\n",
      "l1 norm: 775.9121780734301\n",
      "Rbeta: 925.8974925755318\n",
      "\n",
      "Train set: Avg. loss: 0.000095938, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.092476 101.626434\n",
      "l2 norm: 925.5384371856642\n",
      "l1 norm: 775.8382101291111\n",
      "Rbeta: 925.8076368335904\n",
      "\n",
      "Train set: Avg. loss: 0.000095642, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.12566 101.66365\n",
      "l2 norm: 925.447558734589\n",
      "l1 norm: 775.7635461929333\n",
      "Rbeta: 925.7169492316956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000095347, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.15886 101.7008\n",
      "l2 norm: 925.3571250654222\n",
      "l1 norm: 775.6891214842663\n",
      "Rbeta: 925.6267657072577\n",
      "\n",
      "Train set: Avg. loss: 0.000095053, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.19206 101.7379\n",
      "l2 norm: 925.2669118658192\n",
      "l1 norm: 775.6149243950649\n",
      "Rbeta: 925.5368423592255\n",
      "\n",
      "Train set: Avg. loss: 0.000094760, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.225266 101.774864\n",
      "l2 norm: 925.1809180002543\n",
      "l1 norm: 775.5441400272937\n",
      "Rbeta: 925.4509720003149\n",
      "\n",
      "Train set: Avg. loss: 0.000094469, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.25847 101.81165\n",
      "l2 norm: 925.1076059285754\n",
      "l1 norm: 775.4840531479258\n",
      "Rbeta: 925.3779294163804\n",
      "\n",
      "Train set: Avg. loss: 0.000094179, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.29169 101.84831\n",
      "l2 norm: 925.0354936697435\n",
      "l1 norm: 775.4250092565185\n",
      "Rbeta: 925.3059773366123\n",
      "\n",
      "Train set: Avg. loss: 0.000093892, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.324814 101.884796\n",
      "l2 norm: 924.9451029950723\n",
      "l1 norm: 775.3508198844798\n",
      "Rbeta: 925.2157212744801\n",
      "\n",
      "Train set: Avg. loss: 0.000093606, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.357864 101.921104\n",
      "l2 norm: 924.8479212040181\n",
      "l1 norm: 775.2710216421593\n",
      "Rbeta: 925.1185819453149\n",
      "\n",
      "Train set: Avg. loss: 0.000093321, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.39092 101.957375\n",
      "l2 norm: 924.7601679569509\n",
      "l1 norm: 775.199215059505\n",
      "Rbeta: 925.031053933625\n",
      "\n",
      "Train set: Avg. loss: 0.000093038, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.42398 101.99359\n",
      "l2 norm: 924.6710409220907\n",
      "l1 norm: 775.126370874842\n",
      "Rbeta: 924.9420416906149\n",
      "\n",
      "Train set: Avg. loss: 0.000092755, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.45703 102.02975\n",
      "l2 norm: 924.5828661776972\n",
      "l1 norm: 775.0542644240891\n",
      "Rbeta: 924.8541032674514\n",
      "\n",
      "Train set: Avg. loss: 0.000092474, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.48993 102.06581\n",
      "l2 norm: 924.4850840355083\n",
      "l1 norm: 774.9740179969328\n",
      "Rbeta: 924.7563557859045\n",
      "\n",
      "Train set: Avg. loss: 0.000092195, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.52271 102.10188\n",
      "l2 norm: 924.3892006084853\n",
      "l1 norm: 774.8953363853938\n",
      "Rbeta: 924.660697631657\n",
      "\n",
      "Train set: Avg. loss: 0.000091917, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.55545 102.13782\n",
      "l2 norm: 924.3028676273616\n",
      "l1 norm: 774.824731022846\n",
      "Rbeta: 924.5744128093255\n",
      "\n",
      "Train set: Avg. loss: 0.000091641, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.5882 102.17366\n",
      "l2 norm: 924.2102376368387\n",
      "l1 norm: 774.74882499789\n",
      "Rbeta: 924.4819918033878\n",
      "\n",
      "Train set: Avg. loss: 0.000091365, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.620964 102.209404\n",
      "l2 norm: 924.1297001351342\n",
      "l1 norm: 774.6830226902854\n",
      "Rbeta: 924.4015202482866\n",
      "\n",
      "Train set: Avg. loss: 0.000091091, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.65372 102.24498\n",
      "l2 norm: 924.0644824706477\n",
      "l1 norm: 774.6299508908337\n",
      "Rbeta: 924.3363912860166\n",
      "\n",
      "Train set: Avg. loss: 0.000090819, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.686485 102.28041\n",
      "l2 norm: 923.9866123888816\n",
      "l1 norm: 774.5660603114761\n",
      "Rbeta: 924.2585861813894\n",
      "\n",
      "Train set: Avg. loss: 0.000090547, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.71925 102.31574\n",
      "l2 norm: 923.9159801670029\n",
      "l1 norm: 774.5081961407327\n",
      "Rbeta: 924.1880661581567\n",
      "\n",
      "Train set: Avg. loss: 0.000090277, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.75204 102.35096\n",
      "l2 norm: 923.8571510854349\n",
      "l1 norm: 774.4602396190087\n",
      "Rbeta: 924.1293150760348\n",
      "\n",
      "Train set: Avg. loss: 0.000090008, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.78481 102.38608\n",
      "l2 norm: 923.7805273228074\n",
      "l1 norm: 774.3972907829132\n",
      "Rbeta: 924.0527717989091\n",
      "\n",
      "Train set: Avg. loss: 0.000089740, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.817604 102.42119\n",
      "l2 norm: 923.7045178248327\n",
      "l1 norm: 774.3348584058217\n",
      "Rbeta: 923.9768013572033\n",
      "\n",
      "Train set: Avg. loss: 0.000089473, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.8504 102.45623\n",
      "l2 norm: 923.6259878822082\n",
      "l1 norm: 774.2703048696676\n",
      "Rbeta: 923.8983273766977\n",
      "\n",
      "Train set: Avg. loss: 0.000089208, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.883194 102.4911\n",
      "l2 norm: 923.5541174032076\n",
      "l1 norm: 774.2113584510467\n",
      "Rbeta: 923.8264379453555\n",
      "\n",
      "Train set: Avg. loss: 0.000088943, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.916 102.52588\n",
      "l2 norm: 923.4719209768575\n",
      "l1 norm: 774.143788943252\n",
      "Rbeta: 923.744233631821\n",
      "\n",
      "Train set: Avg. loss: 0.000088680, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.948814 102.56062\n",
      "l2 norm: 923.4027149057353\n",
      "l1 norm: 774.0871380496601\n",
      "Rbeta: 923.6750834199968\n",
      "\n",
      "Train set: Avg. loss: 0.000088417, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.98163 102.59532\n",
      "l2 norm: 923.3295556579735\n",
      "l1 norm: 774.0271097475459\n",
      "Rbeta: 923.6019204180622\n",
      "\n",
      "Train set: Avg. loss: 0.000088156, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.01445 102.62989\n",
      "l2 norm: 923.2486104946312\n",
      "l1 norm: 773.9606451830684\n",
      "Rbeta: 923.5209720656635\n",
      "\n",
      "Train set: Avg. loss: 0.000087896, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.04727 102.66434\n",
      "l2 norm: 923.1688628550068\n",
      "l1 norm: 773.8952438003333\n",
      "Rbeta: 923.4412330751355\n",
      "\n",
      "Train set: Avg. loss: 0.000087637, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.0801 102.69867\n",
      "l2 norm: 923.0747273489833\n",
      "l1 norm: 773.81779319537\n",
      "Rbeta: 923.3470296798337\n",
      "\n",
      "Train set: Avg. loss: 0.000087379, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.11294 102.732895\n",
      "l2 norm: 922.980123079439\n",
      "l1 norm: 773.7399960862838\n",
      "Rbeta: 923.2523473570604\n",
      "\n",
      "Train set: Avg. loss: 0.000087122, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.145775 102.767044\n",
      "l2 norm: 922.8988771647995\n",
      "l1 norm: 773.6733675249661\n",
      "Rbeta: 923.1709936400421\n",
      "\n",
      "Train set: Avg. loss: 0.000086866, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.17862 102.801125\n",
      "l2 norm: 922.8280921004047\n",
      "l1 norm: 773.6154599540475\n",
      "Rbeta: 923.1001682691683\n",
      "\n",
      "Train set: Avg. loss: 0.000086611, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.21147 102.83513\n",
      "l2 norm: 922.7505991300575\n",
      "l1 norm: 773.5518327491475\n",
      "Rbeta: 923.022616570352\n",
      "\n",
      "Train set: Avg. loss: 0.000086357, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.24432 102.86914\n",
      "l2 norm: 922.6655968551908\n",
      "l1 norm: 773.4819170758385\n",
      "Rbeta: 922.9375557496431\n",
      "\n",
      "Train set: Avg. loss: 0.000086105, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.27719 102.90294\n",
      "l2 norm: 922.5810163447352\n",
      "l1 norm: 773.4123772411132\n",
      "Rbeta: 922.8529188184968\n",
      "\n",
      "Train set: Avg. loss: 0.000085853, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.31005 102.936714\n",
      "l2 norm: 922.49657068604\n",
      "l1 norm: 773.3428510491832\n",
      "Rbeta: 922.7683396343265\n",
      "\n",
      "Train set: Avg. loss: 0.000085603, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.342926 102.97037\n",
      "l2 norm: 922.4110446768115\n",
      "l1 norm: 773.2723437996578\n",
      "Rbeta: 922.682712214225\n",
      "\n",
      "Train set: Avg. loss: 0.000085353, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.3758 103.00388\n",
      "l2 norm: 922.3393751446386\n",
      "l1 norm: 773.2132635145516\n",
      "Rbeta: 922.6109515780123\n",
      "\n",
      "Train set: Avg. loss: 0.000085105, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.40868 103.03732\n",
      "l2 norm: 922.2751899539227\n",
      "l1 norm: 773.1605239929061\n",
      "Rbeta: 922.5466504624501\n",
      "\n",
      "Train set: Avg. loss: 0.000084858, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.44157 103.070656\n",
      "l2 norm: 922.204692595474\n",
      "l1 norm: 773.1025625635153\n",
      "Rbeta: 922.4759896800339\n",
      "\n",
      "Train set: Avg. loss: 0.000084611, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.47446 103.10389\n",
      "l2 norm: 922.1275699129562\n",
      "l1 norm: 773.0390835501756\n",
      "Rbeta: 922.3986776834258\n",
      "\n",
      "Train set: Avg. loss: 0.000084367, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.507355 103.136955\n",
      "l2 norm: 922.0574804818095\n",
      "l1 norm: 772.9815959461175\n",
      "Rbeta: 922.3284705910579\n",
      "\n",
      "Train set: Avg. loss: 0.000084123, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.54026 103.169876\n",
      "l2 norm: 921.9849614334627\n",
      "l1 norm: 772.9220948146782\n",
      "Rbeta: 922.2558002512666\n",
      "\n",
      "Train set: Avg. loss: 0.000083880, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.573166 103.202736\n",
      "l2 norm: 921.9130386405188\n",
      "l1 norm: 772.8630901921658\n",
      "Rbeta: 922.183663459591\n",
      "\n",
      "Train set: Avg. loss: 0.000083638, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.605965 103.23559\n",
      "l2 norm: 921.8443448475198\n",
      "l1 norm: 772.8068569977427\n",
      "Rbeta: 922.1147309201318\n",
      "\n",
      "Train set: Avg. loss: 0.000083398, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.6387 103.26847\n",
      "l2 norm: 921.7709386208121\n",
      "l1 norm: 772.7465740035211\n",
      "Rbeta: 922.0411338325872\n",
      "\n",
      "Train set: Avg. loss: 0.000083158, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.671455 103.30129\n",
      "l2 norm: 921.6969271914234\n",
      "l1 norm: 772.6856349054129\n",
      "Rbeta: 921.9669552863604\n",
      "\n",
      "Train set: Avg. loss: 0.000082919, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.704216 103.33408\n",
      "l2 norm: 921.6254985513624\n",
      "l1 norm: 772.6267588909569\n",
      "Rbeta: 921.89539293027\n",
      "\n",
      "Train set: Avg. loss: 0.000082681, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.73683 103.36677\n",
      "l2 norm: 921.5499853529063\n",
      "l1 norm: 772.5644555270143\n",
      "Rbeta: 921.8197045542963\n",
      "\n",
      "Train set: Avg. loss: 0.000082445, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.769424 103.399315\n",
      "l2 norm: 921.4790536042783\n",
      "l1 norm: 772.5060273082355\n",
      "Rbeta: 921.7485029321754\n",
      "\n",
      "Train set: Avg. loss: 0.000082210, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.802025 103.43177\n",
      "l2 norm: 921.4071653772364\n",
      "l1 norm: 772.4468246617541\n",
      "Rbeta: 921.676454061713\n",
      "\n",
      "Train set: Avg. loss: 0.000081975, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.834625 103.46415\n",
      "l2 norm: 921.328148570953\n",
      "l1 norm: 772.3816189543436\n",
      "Rbeta: 921.5972264187793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000081742, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.86717 103.49639\n",
      "l2 norm: 921.2521365291265\n",
      "l1 norm: 772.319045661776\n",
      "Rbeta: 921.5209813062804\n",
      "\n",
      "Train set: Avg. loss: 0.000081511, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.89961 103.52855\n",
      "l2 norm: 921.1797174590017\n",
      "l1 norm: 772.2594068951339\n",
      "Rbeta: 921.4484017073019\n",
      "\n",
      "Train set: Avg. loss: 0.000081280, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.93206 103.56058\n",
      "l2 norm: 921.1188325850293\n",
      "l1 norm: 772.2094593148702\n",
      "Rbeta: 921.3873191439274\n",
      "\n",
      "Train set: Avg. loss: 0.000081051, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.964516 103.592514\n",
      "l2 norm: 921.0655297027687\n",
      "l1 norm: 772.1660147009475\n",
      "Rbeta: 921.3337977492906\n",
      "\n",
      "Train set: Avg. loss: 0.000080822, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.99697 103.6244\n",
      "l2 norm: 921.0065021749741\n",
      "l1 norm: 772.117825158246\n",
      "Rbeta: 921.2744837936887\n",
      "\n",
      "Train set: Avg. loss: 0.000080594, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.029434 103.65624\n",
      "l2 norm: 920.9367701590731\n",
      "l1 norm: 772.0607280512613\n",
      "Rbeta: 921.2044701359396\n",
      "\n",
      "Train set: Avg. loss: 0.000080367, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.0619 103.68805\n",
      "l2 norm: 920.8500514980653\n",
      "l1 norm: 771.9894230455246\n",
      "Rbeta: 921.1175360065878\n",
      "\n",
      "Train set: Avg. loss: 0.000080140, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.09437 103.71983\n",
      "l2 norm: 920.7506803175314\n",
      "l1 norm: 771.9075788969133\n",
      "Rbeta: 921.017896692975\n",
      "\n",
      "Train set: Avg. loss: 0.000079915, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.12685 103.75153\n",
      "l2 norm: 920.666331406176\n",
      "l1 norm: 771.8382011450485\n",
      "Rbeta: 920.9333126260498\n",
      "\n",
      "Train set: Avg. loss: 0.000079690, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.159325 103.78319\n",
      "l2 norm: 920.5933433652957\n",
      "l1 norm: 771.7783001415889\n",
      "Rbeta: 920.8599769920927\n",
      "\n",
      "Train set: Avg. loss: 0.000079465, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.19181 103.81491\n",
      "l2 norm: 920.5248643149644\n",
      "l1 norm: 771.7220823082216\n",
      "Rbeta: 920.7912220771178\n",
      "\n",
      "Train set: Avg. loss: 0.000079242, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.2243 103.846565\n",
      "l2 norm: 920.4552830875742\n",
      "l1 norm: 771.6649874617913\n",
      "Rbeta: 920.7214354921625\n",
      "\n",
      "Train set: Avg. loss: 0.000079020, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.25663 103.87807\n",
      "l2 norm: 920.3914602612894\n",
      "l1 norm: 771.6127326166022\n",
      "Rbeta: 920.657346411955\n",
      "\n",
      "Train set: Avg. loss: 0.000078799, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.28896 103.909485\n",
      "l2 norm: 920.3286322906173\n",
      "l1 norm: 771.5613191697005\n",
      "Rbeta: 920.5943211682144\n",
      "\n",
      "Train set: Avg. loss: 0.000078579, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.32129 103.94079\n",
      "l2 norm: 920.26142365406\n",
      "l1 norm: 771.5062492625591\n",
      "Rbeta: 920.5267533834739\n",
      "\n",
      "Train set: Avg. loss: 0.000078360, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.35363 103.97191\n",
      "l2 norm: 920.1847147228978\n",
      "l1 norm: 771.4431909171325\n",
      "Rbeta: 920.4496539682332\n",
      "\n",
      "Train set: Avg. loss: 0.000078142, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.38597 104.002975\n",
      "l2 norm: 920.1108964120516\n",
      "l1 norm: 771.3825138902689\n",
      "Rbeta: 920.3755805280015\n",
      "\n",
      "Train set: Avg. loss: 0.000077925, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.41832 104.03398\n",
      "l2 norm: 920.0359569181513\n",
      "l1 norm: 771.3209428189434\n",
      "Rbeta: 920.3003586039399\n",
      "\n",
      "Train set: Avg. loss: 0.000077709, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.45066 104.06493\n",
      "l2 norm: 919.9641904061137\n",
      "l1 norm: 771.2620831934339\n",
      "Rbeta: 920.228267973548\n",
      "\n",
      "Train set: Avg. loss: 0.000077493, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.48302 104.09577\n",
      "l2 norm: 919.8816467180756\n",
      "l1 norm: 771.1941932211063\n",
      "Rbeta: 920.145408110003\n",
      "\n",
      "Train set: Avg. loss: 0.000077279, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.51537 104.12652\n",
      "l2 norm: 919.8018190745512\n",
      "l1 norm: 771.1286650181348\n",
      "Rbeta: 920.0652291320704\n",
      "\n",
      "Train set: Avg. loss: 0.000077065, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.54774 104.15711\n",
      "l2 norm: 919.732339989433\n",
      "l1 norm: 771.0717881279259\n",
      "Rbeta: 919.9953903324309\n",
      "\n",
      "Train set: Avg. loss: 0.000076853, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.5801 104.1876\n",
      "l2 norm: 919.6712468400343\n",
      "l1 norm: 771.021872935903\n",
      "Rbeta: 919.9339109132027\n",
      "\n",
      "Train set: Avg. loss: 0.000076641, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.61248 104.21803\n",
      "l2 norm: 919.6080971180179\n",
      "l1 norm: 770.9701682256659\n",
      "Rbeta: 919.8703533928121\n",
      "\n",
      "Train set: Avg. loss: 0.000076430, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.64487 104.24845\n",
      "l2 norm: 919.5492509249071\n",
      "l1 norm: 770.9219739265818\n",
      "Rbeta: 919.8111720327843\n",
      "\n",
      "Train set: Avg. loss: 0.000076220, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.677246 104.27881\n",
      "l2 norm: 919.4859626113075\n",
      "l1 norm: 770.869962182165\n",
      "Rbeta: 919.7474949501194\n",
      "\n",
      "Train set: Avg. loss: 0.000076010, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.70963 104.309204\n",
      "l2 norm: 919.4168337670872\n",
      "l1 norm: 770.8130488439202\n",
      "Rbeta: 919.6779492996891\n",
      "\n",
      "Train set: Avg. loss: 0.000075801, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.74203 104.339554\n",
      "l2 norm: 919.3331381654436\n",
      "l1 norm: 770.7439477056089\n",
      "Rbeta: 919.5938706466166\n",
      "\n",
      "Train set: Avg. loss: 0.000075593, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.77429 104.369804\n",
      "l2 norm: 919.2461471762973\n",
      "l1 norm: 770.6721995020753\n",
      "Rbeta: 919.5065516427633\n",
      "\n",
      "Train set: Avg. loss: 0.000075387, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.80635 104.399956\n",
      "l2 norm: 919.1646999741353\n",
      "l1 norm: 770.6051799763309\n",
      "Rbeta: 919.4246999838309\n",
      "\n",
      "Train set: Avg. loss: 0.000075182, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.83841 104.42997\n",
      "l2 norm: 919.0841253818323\n",
      "l1 norm: 770.5388973355217\n",
      "Rbeta: 919.3437197578089\n",
      "\n",
      "Train set: Avg. loss: 0.000074977, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.870476 104.45986\n",
      "l2 norm: 918.9955980459309\n",
      "l1 norm: 770.4658714429277\n",
      "Rbeta: 919.2548041006004\n",
      "\n",
      "Train set: Avg. loss: 0.000074774, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.90255 104.48963\n",
      "l2 norm: 918.923578621391\n",
      "l1 norm: 770.4067070599267\n",
      "Rbeta: 919.1824110254762\n",
      "\n",
      "Train set: Avg. loss: 0.000074572, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.93463 104.51928\n",
      "l2 norm: 918.864546691617\n",
      "l1 norm: 770.3585052777672\n",
      "Rbeta: 919.1230151228543\n",
      "\n",
      "Train set: Avg. loss: 0.000074370, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.966705 104.548836\n",
      "l2 norm: 918.8134894734171\n",
      "l1 norm: 770.317124694363\n",
      "Rbeta: 919.0714226737905\n",
      "\n",
      "Train set: Avg. loss: 0.000074169, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.998795 104.578384\n",
      "l2 norm: 918.7638098243158\n",
      "l1 norm: 770.2769839743366\n",
      "Rbeta: 919.0213576136109\n",
      "\n",
      "Train set: Avg. loss: 0.000073968, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.03089 104.60793\n",
      "l2 norm: 918.7020003643939\n",
      "l1 norm: 770.2264851523704\n",
      "Rbeta: 918.9591524238186\n",
      "\n",
      "Train set: Avg. loss: 0.000073769, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.06299 104.63739\n",
      "l2 norm: 918.6212239337002\n",
      "l1 norm: 770.1600001433762\n",
      "Rbeta: 918.8778968592876\n",
      "\n",
      "Train set: Avg. loss: 0.000073570, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.09509 104.66681\n",
      "l2 norm: 918.5400637511646\n",
      "l1 norm: 770.0931679409771\n",
      "Rbeta: 918.7963218295505\n",
      "\n",
      "Train set: Avg. loss: 0.000073371, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.127205 104.69617\n",
      "l2 norm: 918.4634879821476\n",
      "l1 norm: 770.0300155189209\n",
      "Rbeta: 918.7192646964562\n",
      "\n",
      "Train set: Avg. loss: 0.000073174, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.15932 104.725464\n",
      "l2 norm: 918.3912157911361\n",
      "l1 norm: 769.9704650665783\n",
      "Rbeta: 918.6465419763991\n",
      "\n",
      "Train set: Avg. loss: 0.000072977, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.19144 104.75475\n",
      "l2 norm: 918.3114019692749\n",
      "l1 norm: 769.9045516995268\n",
      "Rbeta: 918.5663898107882\n",
      "\n",
      "Train set: Avg. loss: 0.000072780, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.22356 104.78393\n",
      "l2 norm: 918.2360781296778\n",
      "l1 norm: 769.8423821134604\n",
      "Rbeta: 918.4904672813338\n",
      "\n",
      "Train set: Avg. loss: 0.000072585, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.25569 104.813034\n",
      "l2 norm: 918.169464234109\n",
      "l1 norm: 769.7875856387661\n",
      "Rbeta: 918.4234758843526\n",
      "\n",
      "Train set: Avg. loss: 0.000072390, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.28783 104.84205\n",
      "l2 norm: 918.1078815010304\n",
      "l1 norm: 769.736994543298\n",
      "Rbeta: 918.3614544766076\n",
      "\n",
      "Train set: Avg. loss: 0.000072196, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.31996 104.87106\n",
      "l2 norm: 918.0419645983419\n",
      "l1 norm: 769.6827590983046\n",
      "Rbeta: 918.2950009102834\n",
      "\n",
      "Train set: Avg. loss: 0.000072002, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.35211 104.900055\n",
      "l2 norm: 917.9631349571131\n",
      "l1 norm: 769.6177658766651\n",
      "Rbeta: 918.2157422795792\n",
      "\n",
      "Train set: Avg. loss: 0.000071809, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.384254 104.92903\n",
      "l2 norm: 917.8873101201727\n",
      "l1 norm: 769.5553397051087\n",
      "Rbeta: 918.1394129917568\n",
      "\n",
      "Train set: Avg. loss: 0.000071617, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.41641 104.95796\n",
      "l2 norm: 917.822351863036\n",
      "l1 norm: 769.5021057482486\n",
      "Rbeta: 918.073944886551\n",
      "\n",
      "Train set: Avg. loss: 0.000071425, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.44858 104.98684\n",
      "l2 norm: 917.7562324981945\n",
      "l1 norm: 769.4479679746323\n",
      "Rbeta: 918.0074106262836\n",
      "\n",
      "Train set: Avg. loss: 0.000071234, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.480736 105.0156\n",
      "l2 norm: 917.6910998059549\n",
      "l1 norm: 769.394681669831\n",
      "Rbeta: 917.9417174063273\n",
      "\n",
      "Train set: Avg. loss: 0.000071044, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.51291 105.04424\n",
      "l2 norm: 917.6290659576119\n",
      "l1 norm: 769.3440027125404\n",
      "Rbeta: 917.8792059986857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000070855, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.54508 105.07277\n",
      "l2 norm: 917.579632885093\n",
      "l1 norm: 769.3038943590429\n",
      "Rbeta: 917.8292596294782\n",
      "\n",
      "Train set: Avg. loss: 0.000070667, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.57726 105.10118\n",
      "l2 norm: 917.5319387240559\n",
      "l1 norm: 769.2650393563132\n",
      "Rbeta: 917.7810003718241\n",
      "\n",
      "Train set: Avg. loss: 0.000070479, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.60945 105.129555\n",
      "l2 norm: 917.481523280205\n",
      "l1 norm: 769.2238068813223\n",
      "Rbeta: 917.7300905731693\n",
      "\n",
      "Train set: Avg. loss: 0.000070292, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.64164 105.1579\n",
      "l2 norm: 917.417445892905\n",
      "l1 norm: 769.1712433285751\n",
      "Rbeta: 917.6654078061049\n",
      "\n",
      "Train set: Avg. loss: 0.000070106, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.673836 105.18609\n",
      "l2 norm: 917.343865727853\n",
      "l1 norm: 769.1105865178998\n",
      "Rbeta: 917.5913298241536\n",
      "\n",
      "Train set: Avg. loss: 0.000069921, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.70592 105.2141\n",
      "l2 norm: 917.2855316978774\n",
      "l1 norm: 769.0627316199732\n",
      "Rbeta: 917.5323957487335\n",
      "\n",
      "Train set: Avg. loss: 0.000069737, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.737946 105.24206\n",
      "l2 norm: 917.2332630934148\n",
      "l1 norm: 769.0199431250844\n",
      "Rbeta: 917.479672536828\n",
      "\n",
      "Train set: Avg. loss: 0.000069554, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.76999 105.26992\n",
      "l2 norm: 917.1761569890962\n",
      "l1 norm: 768.9731037146737\n",
      "Rbeta: 917.4220131179671\n",
      "\n",
      "Train set: Avg. loss: 0.000069371, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.801956 105.29771\n",
      "l2 norm: 917.1111422155124\n",
      "l1 norm: 768.9195847406919\n",
      "Rbeta: 917.3564139203022\n",
      "\n",
      "Train set: Avg. loss: 0.000069190, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.83383 105.3255\n",
      "l2 norm: 917.0380984964562\n",
      "l1 norm: 768.859281832745\n",
      "Rbeta: 917.2827917830996\n",
      "\n",
      "Train set: Avg. loss: 0.000069009, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.865715 105.353264\n",
      "l2 norm: 916.9712028060595\n",
      "l1 norm: 768.8040695223772\n",
      "Rbeta: 917.2153567504973\n",
      "\n",
      "Train set: Avg. loss: 0.000068829, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.8976 105.38092\n",
      "l2 norm: 916.9047023364287\n",
      "l1 norm: 768.7491371111932\n",
      "Rbeta: 917.1482856501528\n",
      "\n",
      "Train set: Avg. loss: 0.000068649, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.92949 105.40854\n",
      "l2 norm: 916.8386951513246\n",
      "l1 norm: 768.694707217977\n",
      "Rbeta: 917.0817347945593\n",
      "\n",
      "Train set: Avg. loss: 0.000068470, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.96138 105.43613\n",
      "l2 norm: 916.7680041359106\n",
      "l1 norm: 768.6363218198661\n",
      "Rbeta: 917.0104823501777\n",
      "\n",
      "Train set: Avg. loss: 0.000068291, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.99327 105.46371\n",
      "l2 norm: 916.6994393375134\n",
      "l1 norm: 768.5797180667369\n",
      "Rbeta: 916.9412775014362\n",
      "\n",
      "Train set: Avg. loss: 0.000068113, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.02518 105.49126\n",
      "l2 norm: 916.6409775060414\n",
      "l1 norm: 768.5315874431551\n",
      "Rbeta: 916.882313480173\n",
      "\n",
      "Train set: Avg. loss: 0.000067936, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.05708 105.518776\n",
      "l2 norm: 916.5921048789861\n",
      "l1 norm: 768.4915636435056\n",
      "Rbeta: 916.8328963469659\n",
      "\n",
      "Train set: Avg. loss: 0.000067759, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.089 105.546196\n",
      "l2 norm: 916.52860927292\n",
      "l1 norm: 768.4392684137883\n",
      "Rbeta: 916.7687351786757\n",
      "\n",
      "Train set: Avg. loss: 0.000067583, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.12091 105.5736\n",
      "l2 norm: 916.4653009777143\n",
      "l1 norm: 768.3871163513435\n",
      "Rbeta: 916.7049227282646\n",
      "\n",
      "Train set: Avg. loss: 0.000067407, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.15283 105.600914\n",
      "l2 norm: 916.3981431622287\n",
      "l1 norm: 768.3317881845669\n",
      "Rbeta: 916.6371770210482\n",
      "\n",
      "Train set: Avg. loss: 0.000067233, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.18476 105.628105\n",
      "l2 norm: 916.327893604561\n",
      "l1 norm: 768.2737487244824\n",
      "Rbeta: 916.5662757984885\n",
      "\n",
      "Train set: Avg. loss: 0.000067058, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.21669 105.65526\n",
      "l2 norm: 916.2504515891168\n",
      "l1 norm: 768.209714118506\n",
      "Rbeta: 916.488212022517\n",
      "\n",
      "Train set: Avg. loss: 0.000066885, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.248634 105.682396\n",
      "l2 norm: 916.1613095216728\n",
      "l1 norm: 768.1359111069808\n",
      "Rbeta: 916.3984741502883\n",
      "\n",
      "Train set: Avg. loss: 0.000066712, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.28058 105.70949\n",
      "l2 norm: 916.0717869433655\n",
      "l1 norm: 768.0618209467987\n",
      "Rbeta: 916.308360642761\n",
      "\n",
      "Train set: Avg. loss: 0.000066539, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.31252 105.736465\n",
      "l2 norm: 915.9969929810095\n",
      "l1 norm: 768.0000805628575\n",
      "Rbeta: 916.2329404570198\n",
      "\n",
      "Train set: Avg. loss: 0.000066368, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.34448 105.76332\n",
      "l2 norm: 915.9198907491658\n",
      "l1 norm: 767.9363418793423\n",
      "Rbeta: 916.1552381232368\n",
      "\n",
      "Train set: Avg. loss: 0.000066197, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.376434 105.790146\n",
      "l2 norm: 915.8569763635109\n",
      "l1 norm: 767.8844041111727\n",
      "Rbeta: 916.0916363520392\n",
      "\n",
      "Train set: Avg. loss: 0.000066027, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.40833 105.81691\n",
      "l2 norm: 915.7994998241859\n",
      "l1 norm: 767.8369356585396\n",
      "Rbeta: 916.033580880048\n",
      "\n",
      "Train set: Avg. loss: 0.000065857, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.44013 105.84368\n",
      "l2 norm: 915.7348999838687\n",
      "l1 norm: 767.7835069340958\n",
      "Rbeta: 915.9683987645018\n",
      "\n",
      "Train set: Avg. loss: 0.000065688, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.47193 105.87043\n",
      "l2 norm: 915.6654817509623\n",
      "l1 norm: 767.7260802219179\n",
      "Rbeta: 915.8983723159498\n",
      "\n",
      "Train set: Avg. loss: 0.000065519, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.50374 105.89717\n",
      "l2 norm: 915.5989658971719\n",
      "l1 norm: 767.6710699478178\n",
      "Rbeta: 915.8312283851604\n",
      "\n",
      "Train set: Avg. loss: 0.000065351, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.53554 105.923805\n",
      "l2 norm: 915.5360437233136\n",
      "l1 norm: 767.619033215535\n",
      "Rbeta: 915.7676965113327\n",
      "\n",
      "Train set: Avg. loss: 0.000065184, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.56735 105.950386\n",
      "l2 norm: 915.4833334045064\n",
      "l1 norm: 767.5756846882937\n",
      "Rbeta: 915.7142962913525\n",
      "\n",
      "Train set: Avg. loss: 0.000065017, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.59918 105.97692\n",
      "l2 norm: 915.4268367950098\n",
      "l1 norm: 767.5292455955666\n",
      "Rbeta: 915.65732104674\n",
      "\n",
      "Train set: Avg. loss: 0.000064851, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.631 106.003395\n",
      "l2 norm: 915.3832811007094\n",
      "l1 norm: 767.4936531786443\n",
      "Rbeta: 915.6130368506732\n",
      "\n",
      "Train set: Avg. loss: 0.000064686, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.662834 106.029816\n",
      "l2 norm: 915.3373905544998\n",
      "l1 norm: 767.4561647107456\n",
      "Rbeta: 915.5666024233777\n",
      "\n",
      "Train set: Avg. loss: 0.000064521, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.694664 106.05609\n",
      "l2 norm: 915.2905413293253\n",
      "l1 norm: 767.4179274790579\n",
      "Rbeta: 915.5190453644926\n",
      "\n",
      "Train set: Avg. loss: 0.000064357, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.7265 106.08226\n",
      "l2 norm: 915.2520181388991\n",
      "l1 norm: 767.3867301917895\n",
      "Rbeta: 915.479883489713\n",
      "\n",
      "Train set: Avg. loss: 0.000064194, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.75835 106.108246\n",
      "l2 norm: 915.2117692704907\n",
      "l1 norm: 767.3541341814991\n",
      "Rbeta: 915.4389262706869\n",
      "\n",
      "Train set: Avg. loss: 0.000064031, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.7902 106.13417\n",
      "l2 norm: 915.1627913603833\n",
      "l1 norm: 767.3140899964972\n",
      "Rbeta: 915.3893265743491\n",
      "\n",
      "Train set: Avg. loss: 0.000063869, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.82205 106.16005\n",
      "l2 norm: 915.1288939920285\n",
      "l1 norm: 767.2867124290483\n",
      "Rbeta: 915.3547387206453\n",
      "\n",
      "Train set: Avg. loss: 0.000063707, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.85391 106.18586\n",
      "l2 norm: 915.0908069526774\n",
      "l1 norm: 767.2558708416773\n",
      "Rbeta: 915.3159435279543\n",
      "\n",
      "Train set: Avg. loss: 0.000063547, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.88573 106.21165\n",
      "l2 norm: 915.0465489821344\n",
      "l1 norm: 767.2198178650001\n",
      "Rbeta: 915.2710930746357\n",
      "\n",
      "Train set: Avg. loss: 0.000063386, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.91742 106.23738\n",
      "l2 norm: 914.9872844788334\n",
      "l1 norm: 767.1710647031485\n",
      "Rbeta: 915.2110685184169\n",
      "\n",
      "Train set: Avg. loss: 0.000063227, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.94912 106.263054\n",
      "l2 norm: 914.9307833626812\n",
      "l1 norm: 767.1245705839069\n",
      "Rbeta: 915.1538806243664\n",
      "\n",
      "Train set: Avg. loss: 0.000063068, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.98083 106.288704\n",
      "l2 norm: 914.8743510603773\n",
      "l1 norm: 767.0781248814294\n",
      "Rbeta: 915.0967841623213\n",
      "\n",
      "Train set: Avg. loss: 0.000062910, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.012535 106.3143\n",
      "l2 norm: 914.811176997273\n",
      "l1 norm: 767.0260180865826\n",
      "Rbeta: 915.0329972264127\n",
      "\n",
      "Train set: Avg. loss: 0.000062752, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.044235 106.3398\n",
      "l2 norm: 914.7470404392236\n",
      "l1 norm: 766.9731261605272\n",
      "Rbeta: 914.9681196348062\n",
      "\n",
      "Train set: Avg. loss: 0.000062595, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.075775 106.36529\n",
      "l2 norm: 914.6898801534153\n",
      "l1 norm: 766.9260811996149\n",
      "Rbeta: 914.910336423184\n",
      "\n",
      "Train set: Avg. loss: 0.000062439, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.10733 106.39074\n",
      "l2 norm: 914.633990759317\n",
      "l1 norm: 766.8802660793118\n",
      "Rbeta: 914.8537741749013\n",
      "\n",
      "Train set: Avg. loss: 0.000062283, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.138885 106.41616\n",
      "l2 norm: 914.5810848378983\n",
      "l1 norm: 766.8368946697722\n",
      "Rbeta: 914.8002252114757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000062127, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.17045 106.44157\n",
      "l2 norm: 914.5218763319249\n",
      "l1 norm: 766.7882103002771\n",
      "Rbeta: 914.740309289135\n",
      "\n",
      "Train set: Avg. loss: 0.000061972, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.20201 106.46695\n",
      "l2 norm: 914.4579373992464\n",
      "l1 norm: 766.7355448847646\n",
      "Rbeta: 914.6757069539208\n",
      "\n",
      "Train set: Avg. loss: 0.000061818, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.23358 106.49228\n",
      "l2 norm: 914.3931790205651\n",
      "l1 norm: 766.6820889466475\n",
      "Rbeta: 914.6102914847221\n",
      "\n",
      "Train set: Avg. loss: 0.000061664, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.26516 106.517555\n",
      "l2 norm: 914.3255832853888\n",
      "l1 norm: 766.6262388548739\n",
      "Rbeta: 914.5419745563178\n",
      "\n",
      "Train set: Avg. loss: 0.000061510, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.29674 106.54276\n",
      "l2 norm: 914.2600953269855\n",
      "l1 norm: 766.5721304739177\n",
      "Rbeta: 914.4758245300378\n",
      "\n",
      "Train set: Avg. loss: 0.000061357, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.32832 106.56787\n",
      "l2 norm: 914.2002159006906\n",
      "l1 norm: 766.5227300855254\n",
      "Rbeta: 914.4152169778165\n",
      "\n",
      "Train set: Avg. loss: 0.000061205, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.35992 106.592896\n",
      "l2 norm: 914.1415640821443\n",
      "l1 norm: 766.4744183558373\n",
      "Rbeta: 914.355933408042\n",
      "\n",
      "Train set: Avg. loss: 0.000061054, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.39151 106.617775\n",
      "l2 norm: 914.0860065626565\n",
      "l1 norm: 766.4287276539953\n",
      "Rbeta: 914.2995908706766\n",
      "\n",
      "Train set: Avg. loss: 0.000060903, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.42311 106.64259\n",
      "l2 norm: 914.0221325690876\n",
      "l1 norm: 766.3760766727873\n",
      "Rbeta: 914.2350501813462\n",
      "\n",
      "Train set: Avg. loss: 0.000060752, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.45471 106.66739\n",
      "l2 norm: 913.9540459696636\n",
      "l1 norm: 766.3198832091025\n",
      "Rbeta: 914.1661920592697\n",
      "\n",
      "Train set: Avg. loss: 0.000060602, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.48633 106.69218\n",
      "l2 norm: 913.8846227845513\n",
      "l1 norm: 766.26253098657\n",
      "Rbeta: 914.0960719163448\n",
      "\n",
      "Train set: Avg. loss: 0.000060453, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.51794 106.71682\n",
      "l2 norm: 913.8256341662681\n",
      "l1 norm: 766.2140164224015\n",
      "Rbeta: 914.0363481839188\n",
      "\n",
      "Train set: Avg. loss: 0.000060304, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.54956 106.7414\n",
      "l2 norm: 913.7687982323708\n",
      "l1 norm: 766.1673276172453\n",
      "Rbeta: 913.9787768505638\n",
      "\n",
      "Train set: Avg. loss: 0.000060155, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.581184 106.76591\n",
      "l2 norm: 913.716328493351\n",
      "l1 norm: 766.1243804916425\n",
      "Rbeta: 913.92567399442\n",
      "\n",
      "Train set: Avg. loss: 0.000060008, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.61281 106.790306\n",
      "l2 norm: 913.6667459108655\n",
      "l1 norm: 766.083920363799\n",
      "Rbeta: 913.8752822855823\n",
      "\n",
      "Train set: Avg. loss: 0.000059860, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.64445 106.81467\n",
      "l2 norm: 913.6140211129471\n",
      "l1 norm: 766.0407920020364\n",
      "Rbeta: 913.8218266993539\n",
      "\n",
      "Train set: Avg. loss: 0.000059713, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.67608 106.839005\n",
      "l2 norm: 913.5565271242358\n",
      "l1 norm: 765.9936847949383\n",
      "Rbeta: 913.7635814457167\n",
      "\n",
      "Train set: Avg. loss: 0.000059567, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.70772 106.86329\n",
      "l2 norm: 913.4936786546793\n",
      "l1 norm: 765.9421009005857\n",
      "Rbeta: 913.7000772911947\n",
      "\n",
      "Train set: Avg. loss: 0.000059421, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.739365 106.887596\n",
      "l2 norm: 913.4374046869688\n",
      "l1 norm: 765.8960698579052\n",
      "Rbeta: 913.642964868995\n",
      "\n",
      "Train set: Avg. loss: 0.000059275, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.77102 106.91187\n",
      "l2 norm: 913.3778018331026\n",
      "l1 norm: 765.8472546546525\n",
      "Rbeta: 913.5826871181457\n",
      "\n",
      "Train set: Avg. loss: 0.000059130, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.80268 106.93605\n",
      "l2 norm: 913.3151571736499\n",
      "l1 norm: 765.7958907049623\n",
      "Rbeta: 913.5193826765071\n",
      "\n",
      "Train set: Avg. loss: 0.000058986, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.834335 106.96014\n",
      "l2 norm: 913.2563759696942\n",
      "l1 norm: 765.7478164947291\n",
      "Rbeta: 913.4597377976443\n",
      "\n",
      "Train set: Avg. loss: 0.000058842, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.866005 106.98411\n",
      "l2 norm: 913.2021165894531\n",
      "l1 norm: 765.703460712664\n",
      "Rbeta: 913.4047766881151\n",
      "\n",
      "Train set: Avg. loss: 0.000058699, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.897675 107.008026\n",
      "l2 norm: 913.1461097775554\n",
      "l1 norm: 765.657550051627\n",
      "Rbeta: 913.3479868072052\n",
      "\n",
      "Train set: Avg. loss: 0.000058555, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.92935 107.03202\n",
      "l2 norm: 913.0856656972533\n",
      "l1 norm: 765.6078508695092\n",
      "Rbeta: 913.2867709854909\n",
      "\n",
      "Train set: Avg. loss: 0.000058413, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.96104 107.05597\n",
      "l2 norm: 913.032294768635\n",
      "l1 norm: 765.5640834470113\n",
      "Rbeta: 913.2326230694811\n",
      "\n",
      "Train set: Avg. loss: 0.000058270, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.99272 107.0799\n",
      "l2 norm: 912.9767012983732\n",
      "l1 norm: 765.5184968580264\n",
      "Rbeta: 913.1763808181431\n",
      "\n",
      "Train set: Avg. loss: 0.000058128, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.02441 107.10375\n",
      "l2 norm: 912.9257753954894\n",
      "l1 norm: 765.4769200290818\n",
      "Rbeta: 913.124618745243\n",
      "\n",
      "Train set: Avg. loss: 0.000057987, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.05611 107.12752\n",
      "l2 norm: 912.8691704533434\n",
      "l1 norm: 765.4306307224265\n",
      "Rbeta: 913.0673052249942\n",
      "\n",
      "Train set: Avg. loss: 0.000057846, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.0878 107.151215\n",
      "l2 norm: 912.815368037049\n",
      "l1 norm: 765.3867052424935\n",
      "Rbeta: 913.0127110195884\n",
      "\n",
      "Train set: Avg. loss: 0.000057706, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.11951 107.17489\n",
      "l2 norm: 912.7720452705801\n",
      "l1 norm: 765.3515521684053\n",
      "Rbeta: 912.9686649142991\n",
      "\n",
      "Train set: Avg. loss: 0.000057566, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.15122 107.19854\n",
      "l2 norm: 912.726744104052\n",
      "l1 norm: 765.3146659941233\n",
      "Rbeta: 912.9225654820209\n",
      "\n",
      "Train set: Avg. loss: 0.000057426, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.18293 107.22215\n",
      "l2 norm: 912.6851341313463\n",
      "l1 norm: 765.2807687162635\n",
      "Rbeta: 912.8802577882587\n",
      "\n",
      "Train set: Avg. loss: 0.000057287, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.21465 107.24573\n",
      "l2 norm: 912.6432402884434\n",
      "l1 norm: 765.2465792043245\n",
      "Rbeta: 912.8376079004336\n",
      "\n",
      "Train set: Avg. loss: 0.000057148, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.24637 107.269325\n",
      "l2 norm: 912.6064056702735\n",
      "l1 norm: 765.2166772664303\n",
      "Rbeta: 912.7999384896495\n",
      "\n",
      "Train set: Avg. loss: 0.000057009, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.2781 107.29293\n",
      "l2 norm: 912.5580244494837\n",
      "l1 norm: 765.1771979797619\n",
      "Rbeta: 912.7508170147834\n",
      "\n",
      "Train set: Avg. loss: 0.000056871, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.30984 107.316536\n",
      "l2 norm: 912.4978372904664\n",
      "l1 norm: 765.1277738325648\n",
      "Rbeta: 912.689881078254\n",
      "\n",
      "Train set: Avg. loss: 0.000056733, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.341576 107.34013\n",
      "l2 norm: 912.4389374106601\n",
      "l1 norm: 765.0793557684108\n",
      "Rbeta: 912.6302151876085\n",
      "\n",
      "Train set: Avg. loss: 0.000056596, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.37332 107.36368\n",
      "l2 norm: 912.3842458163232\n",
      "l1 norm: 765.0345607521438\n",
      "Rbeta: 912.574813854639\n",
      "\n",
      "Train set: Avg. loss: 0.000056459, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.40506 107.38713\n",
      "l2 norm: 912.3320269741171\n",
      "l1 norm: 764.9918811628511\n",
      "Rbeta: 912.5218114010456\n",
      "\n",
      "Train set: Avg. loss: 0.000056322, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.43681 107.41059\n",
      "l2 norm: 912.2789339064361\n",
      "l1 norm: 764.94845376503\n",
      "Rbeta: 912.4679746010504\n",
      "\n",
      "Train set: Avg. loss: 0.000056186, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.468575 107.43401\n",
      "l2 norm: 912.2154971770779\n",
      "l1 norm: 764.8963657380601\n",
      "Rbeta: 912.4037398061197\n",
      "\n",
      "Train set: Avg. loss: 0.000056050, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.500336 107.45737\n",
      "l2 norm: 912.165279177086\n",
      "l1 norm: 764.8553359628326\n",
      "Rbeta: 912.3527478498172\n",
      "\n",
      "Train set: Avg. loss: 0.000055915, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.532104 107.480644\n",
      "l2 norm: 912.1146353732388\n",
      "l1 norm: 764.8138819401777\n",
      "Rbeta: 912.3014067949089\n",
      "\n",
      "Train set: Avg. loss: 0.000055780, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.56387 107.50385\n",
      "l2 norm: 912.0685778440647\n",
      "l1 norm: 764.776258635686\n",
      "Rbeta: 912.2544615698115\n",
      "\n",
      "Train set: Avg. loss: 0.000055646, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.59566 107.52701\n",
      "l2 norm: 912.0166416020942\n",
      "l1 norm: 764.7336618899853\n",
      "Rbeta: 912.2018414779623\n",
      "\n",
      "Train set: Avg. loss: 0.000055512, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.62744 107.549995\n",
      "l2 norm: 911.9790363900247\n",
      "l1 norm: 764.7030323825625\n",
      "Rbeta: 912.1634349442331\n",
      "\n",
      "Train set: Avg. loss: 0.000055379, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.659225 107.5729\n",
      "l2 norm: 911.942140323699\n",
      "l1 norm: 764.6729788143232\n",
      "Rbeta: 912.1257198517413\n",
      "\n",
      "Train set: Avg. loss: 0.000055247, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.69102 107.59578\n",
      "l2 norm: 911.8954912508623\n",
      "l1 norm: 764.6347590252277\n",
      "Rbeta: 912.078346836869\n",
      "\n",
      "Train set: Avg. loss: 0.000055114, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.72282 107.6186\n",
      "l2 norm: 911.8446923453906\n",
      "l1 norm: 764.592989668067\n",
      "Rbeta: 912.0267446040274\n",
      "\n",
      "Train set: Avg. loss: 0.000054983, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.754616 107.64135\n",
      "l2 norm: 911.7898070285013\n",
      "l1 norm: 764.5478027104982\n",
      "Rbeta: 911.9710422672689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000054851, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.78642 107.664024\n",
      "l2 norm: 911.7325319661654\n",
      "l1 norm: 764.5005692648411\n",
      "Rbeta: 911.9129953364561\n",
      "\n",
      "Train set: Avg. loss: 0.000054720, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.81823 107.686615\n",
      "l2 norm: 911.6775979843312\n",
      "l1 norm: 764.4551623969417\n",
      "Rbeta: 911.857300087602\n",
      "\n",
      "Train set: Avg. loss: 0.000054590, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.850044 107.70911\n",
      "l2 norm: 911.6275887558422\n",
      "l1 norm: 764.413823967601\n",
      "Rbeta: 911.8064430111571\n",
      "\n",
      "Train set: Avg. loss: 0.000054460, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.88186 107.73152\n",
      "l2 norm: 911.5719533066067\n",
      "l1 norm: 764.3678817997229\n",
      "Rbeta: 911.749969311147\n",
      "\n",
      "Train set: Avg. loss: 0.000054333, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.91294 107.75387\n",
      "l2 norm: 911.5132463571407\n",
      "l1 norm: 764.3194370608037\n",
      "Rbeta: 911.6905235646013\n",
      "\n",
      "Train set: Avg. loss: 0.000054208, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.94296 107.77618\n",
      "l2 norm: 911.4512125395516\n",
      "l1 norm: 764.2681191176246\n",
      "Rbeta: 911.6278473389568\n",
      "\n",
      "Train set: Avg. loss: 0.000054084, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.972984 107.798416\n",
      "l2 norm: 911.3947091263108\n",
      "l1 norm: 764.2214567798488\n",
      "Rbeta: 911.5706051152293\n",
      "\n",
      "Train set: Avg. loss: 0.000053960, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.003006 107.82061\n",
      "l2 norm: 911.3373548268789\n",
      "l1 norm: 764.1740694804432\n",
      "Rbeta: 911.512565610636\n",
      "\n",
      "Train set: Avg. loss: 0.000053837, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.03304 107.84279\n",
      "l2 norm: 911.2871620850815\n",
      "l1 norm: 764.1326903227296\n",
      "Rbeta: 911.4617485955232\n",
      "\n",
      "Train set: Avg. loss: 0.000053714, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.06308 107.86498\n",
      "l2 norm: 911.2390422939005\n",
      "l1 norm: 764.0929986733603\n",
      "Rbeta: 911.4130209403396\n",
      "\n",
      "Train set: Avg. loss: 0.000053591, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.093124 107.887146\n",
      "l2 norm: 911.1872470333992\n",
      "l1 norm: 764.0502208019204\n",
      "Rbeta: 911.360505788802\n",
      "\n",
      "Train set: Avg. loss: 0.000053468, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.12316 107.909256\n",
      "l2 norm: 911.1388474430844\n",
      "l1 norm: 764.0104241360182\n",
      "Rbeta: 911.3113788217042\n",
      "\n",
      "Train set: Avg. loss: 0.000053346, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.153206 107.93133\n",
      "l2 norm: 911.0913845618244\n",
      "l1 norm: 763.9715300574768\n",
      "Rbeta: 911.2632075922295\n",
      "\n",
      "Train set: Avg. loss: 0.000053224, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.183266 107.95343\n",
      "l2 norm: 911.0424199805533\n",
      "l1 norm: 763.931362019925\n",
      "Rbeta: 911.2136214678536\n",
      "\n",
      "Train set: Avg. loss: 0.000053103, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.21332 107.97547\n",
      "l2 norm: 910.9977511165814\n",
      "l1 norm: 763.8947143816887\n",
      "Rbeta: 911.1682340658556\n",
      "\n",
      "Train set: Avg. loss: 0.000052982, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.243385 107.99749\n",
      "l2 norm: 910.9496375224071\n",
      "l1 norm: 763.8551167633905\n",
      "Rbeta: 911.1195513195303\n",
      "\n",
      "Train set: Avg. loss: 0.000052861, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.27345 108.01939\n",
      "l2 norm: 910.8963352154658\n",
      "l1 norm: 763.8112568201057\n",
      "Rbeta: 911.0655380334338\n",
      "\n",
      "Train set: Avg. loss: 0.000052741, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.30352 108.041275\n",
      "l2 norm: 910.8386412642901\n",
      "l1 norm: 763.7636919503526\n",
      "Rbeta: 911.007116870776\n",
      "\n",
      "Train set: Avg. loss: 0.000052621, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.3336 108.06314\n",
      "l2 norm: 910.7690879478922\n",
      "l1 norm: 763.7062711968189\n",
      "Rbeta: 910.9369208146504\n",
      "\n",
      "Train set: Avg. loss: 0.000052501, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.36368 108.08495\n",
      "l2 norm: 910.7033009403472\n",
      "l1 norm: 763.6519872611074\n",
      "Rbeta: 910.8704654009841\n",
      "\n",
      "Train set: Avg. loss: 0.000052382, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.39376 108.10671\n",
      "l2 norm: 910.6420635524698\n",
      "l1 norm: 763.6014845588741\n",
      "Rbeta: 910.8085024413766\n",
      "\n",
      "Train set: Avg. loss: 0.000052263, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.42385 108.12843\n",
      "l2 norm: 910.5903184632629\n",
      "l1 norm: 763.5590398228162\n",
      "Rbeta: 910.7561003578884\n",
      "\n",
      "Train set: Avg. loss: 0.000052145, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.45394 108.15007\n",
      "l2 norm: 910.5451889226424\n",
      "l1 norm: 763.5222233337574\n",
      "Rbeta: 910.7102624535408\n",
      "\n",
      "Train set: Avg. loss: 0.000052026, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.48404 108.17171\n",
      "l2 norm: 910.508488170023\n",
      "l1 norm: 763.4924006984185\n",
      "Rbeta: 910.6728848917907\n",
      "\n",
      "Train set: Avg. loss: 0.000051909, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.514145 108.193306\n",
      "l2 norm: 910.4712004009268\n",
      "l1 norm: 763.4621505965202\n",
      "Rbeta: 910.6349912764514\n",
      "\n",
      "Train set: Avg. loss: 0.000051791, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.54425 108.214874\n",
      "l2 norm: 910.4347050276929\n",
      "l1 norm: 763.4326081457571\n",
      "Rbeta: 910.5977363538751\n",
      "\n",
      "Train set: Avg. loss: 0.000051674, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.574356 108.23642\n",
      "l2 norm: 910.4028113048897\n",
      "l1 norm: 763.4069128698904\n",
      "Rbeta: 910.5651398056286\n",
      "\n",
      "Train set: Avg. loss: 0.000051557, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.60448 108.25792\n",
      "l2 norm: 910.3647243287353\n",
      "l1 norm: 763.3760970005528\n",
      "Rbeta: 910.5263949615634\n",
      "\n",
      "Train set: Avg. loss: 0.000051441, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.6346 108.2794\n",
      "l2 norm: 910.3282284901605\n",
      "l1 norm: 763.3465681118612\n",
      "Rbeta: 910.4892894777641\n",
      "\n",
      "Train set: Avg. loss: 0.000051324, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.66472 108.30089\n",
      "l2 norm: 910.2785357082714\n",
      "l1 norm: 763.3059539397077\n",
      "Rbeta: 910.4388652875513\n",
      "\n",
      "Train set: Avg. loss: 0.000051208, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.69485 108.32232\n",
      "l2 norm: 910.2387726830018\n",
      "l1 norm: 763.2736814405365\n",
      "Rbeta: 910.3984216314946\n",
      "\n",
      "Train set: Avg. loss: 0.000051093, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.724976 108.343735\n",
      "l2 norm: 910.206831324562\n",
      "l1 norm: 763.2479685833184\n",
      "Rbeta: 910.3657605474572\n",
      "\n",
      "Train set: Avg. loss: 0.000050977, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.75511 108.365105\n",
      "l2 norm: 910.1716780466261\n",
      "l1 norm: 763.2195450680673\n",
      "Rbeta: 910.3299090806557\n",
      "\n",
      "Train set: Avg. loss: 0.000050863, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.78525 108.38637\n",
      "l2 norm: 910.1467407336559\n",
      "l1 norm: 763.1996299669452\n",
      "Rbeta: 910.3042869051179\n",
      "\n",
      "Train set: Avg. loss: 0.000050748, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.8154 108.40748\n",
      "l2 norm: 910.1105268278636\n",
      "l1 norm: 763.170274123766\n",
      "Rbeta: 910.2673722604148\n",
      "\n",
      "Train set: Avg. loss: 0.000050634, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.84555 108.42857\n",
      "l2 norm: 910.0656063787443\n",
      "l1 norm: 763.1335904930771\n",
      "Rbeta: 910.2217953099544\n",
      "\n",
      "Train set: Avg. loss: 0.000050521, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.8757 108.449646\n",
      "l2 norm: 910.0117346174216\n",
      "l1 norm: 763.0893897329129\n",
      "Rbeta: 910.1672032338734\n",
      "\n",
      "Train set: Avg. loss: 0.000050407, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.90586 108.47071\n",
      "l2 norm: 909.9562542717131\n",
      "l1 norm: 763.0438212549516\n",
      "Rbeta: 910.1110242631286\n",
      "\n",
      "Train set: Avg. loss: 0.000050294, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.93603 108.491684\n",
      "l2 norm: 909.9037080368604\n",
      "l1 norm: 763.000679107279\n",
      "Rbeta: 910.0577847577152\n",
      "\n",
      "Train set: Avg. loss: 0.000050182, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.966194 108.51259\n",
      "l2 norm: 909.8476280557563\n",
      "l1 norm: 762.9545078092876\n",
      "Rbeta: 910.0010292704337\n",
      "\n",
      "Train set: Avg. loss: 0.000050070, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.99636 108.5335\n",
      "l2 norm: 909.7900135419835\n",
      "l1 norm: 762.9069489596857\n",
      "Rbeta: 909.9426828836181\n",
      "\n",
      "Train set: Avg. loss: 0.000049958, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.02654 108.55438\n",
      "l2 norm: 909.7288313858779\n",
      "l1 norm: 762.8564064156521\n",
      "Rbeta: 909.8808485067705\n",
      "\n",
      "Train set: Avg. loss: 0.000049846, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.05672 108.57524\n",
      "l2 norm: 909.6687479016691\n",
      "l1 norm: 762.8068463467794\n",
      "Rbeta: 909.8200119053512\n",
      "\n",
      "Train set: Avg. loss: 0.000049737, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.08579 108.59609\n",
      "l2 norm: 909.6201218617505\n",
      "l1 norm: 762.7668591208785\n",
      "Rbeta: 909.7707536963852\n",
      "\n",
      "Train set: Avg. loss: 0.000049630, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.11417 108.61699\n",
      "l2 norm: 909.568722293617\n",
      "l1 norm: 762.7244690662247\n",
      "Rbeta: 909.7188465048783\n",
      "\n",
      "Train set: Avg. loss: 0.000049523, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.14253 108.63786\n",
      "l2 norm: 909.5193875711977\n",
      "l1 norm: 762.6837792789902\n",
      "Rbeta: 909.668915746867\n",
      "\n",
      "Train set: Avg. loss: 0.000049416, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.170906 108.658676\n",
      "l2 norm: 909.4665980248508\n",
      "l1 norm: 762.6401830817222\n",
      "Rbeta: 909.6155898087076\n",
      "\n",
      "Train set: Avg. loss: 0.000049309, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.19929 108.679474\n",
      "l2 norm: 909.4166826209901\n",
      "l1 norm: 762.5990390780205\n",
      "Rbeta: 909.5650468313358\n",
      "\n",
      "Train set: Avg. loss: 0.000049204, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.22768 108.70018\n",
      "l2 norm: 909.3768236873975\n",
      "l1 norm: 762.566257358008\n",
      "Rbeta: 909.5246448905482\n",
      "\n",
      "Train set: Avg. loss: 0.000049098, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.256065 108.72079\n",
      "l2 norm: 909.3382083843641\n",
      "l1 norm: 762.5345371507456\n",
      "Rbeta: 909.4854815719813\n",
      "\n",
      "Train set: Avg. loss: 0.000048993, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.284454 108.741394\n",
      "l2 norm: 909.3097857672611\n",
      "l1 norm: 762.5113883657821\n",
      "Rbeta: 909.4564745965677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000048888, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.31285 108.762054\n",
      "l2 norm: 909.2804020219119\n",
      "l1 norm: 762.4873637352287\n",
      "Rbeta: 909.4265330828573\n",
      "\n",
      "Train set: Avg. loss: 0.000048783, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.34125 108.78267\n",
      "l2 norm: 909.2333602866125\n",
      "l1 norm: 762.4485051004885\n",
      "Rbeta: 909.3789173327609\n",
      "\n",
      "Train set: Avg. loss: 0.000048678, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.36965 108.80323\n",
      "l2 norm: 909.1792612800976\n",
      "l1 norm: 762.4037406860989\n",
      "Rbeta: 909.3242303683153\n",
      "\n",
      "Train set: Avg. loss: 0.000048574, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.398056 108.82378\n",
      "l2 norm: 909.1227071500124\n",
      "l1 norm: 762.3568348301449\n",
      "Rbeta: 909.2670948060742\n",
      "\n",
      "Train set: Avg. loss: 0.000048470, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.42647 108.84426\n",
      "l2 norm: 909.0582150775685\n",
      "l1 norm: 762.3032607608118\n",
      "Rbeta: 909.201972573831\n",
      "\n",
      "Train set: Avg. loss: 0.000048366, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.45488 108.864685\n",
      "l2 norm: 909.0042417736748\n",
      "l1 norm: 762.2585322017153\n",
      "Rbeta: 909.1474416014177\n",
      "\n",
      "Train set: Avg. loss: 0.000048263, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.48329 108.88503\n",
      "l2 norm: 908.9441766732047\n",
      "l1 norm: 762.2086822003407\n",
      "Rbeta: 909.0867515365863\n",
      "\n",
      "Train set: Avg. loss: 0.000048160, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.51171 108.90528\n",
      "l2 norm: 908.8840650513457\n",
      "l1 norm: 762.1588026251995\n",
      "Rbeta: 909.0260608497967\n",
      "\n",
      "Train set: Avg. loss: 0.000048058, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.540146 108.925476\n",
      "l2 norm: 908.8252016310793\n",
      "l1 norm: 762.1099422473638\n",
      "Rbeta: 908.9666469474508\n",
      "\n",
      "Train set: Avg. loss: 0.000047955, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.56857 108.94564\n",
      "l2 norm: 908.7678578772603\n",
      "l1 norm: 762.0623415094593\n",
      "Rbeta: 908.9087452394663\n",
      "\n",
      "Train set: Avg. loss: 0.000047853, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.597 108.965775\n",
      "l2 norm: 908.7096627057113\n",
      "l1 norm: 762.0140531967593\n",
      "Rbeta: 908.84988011554\n",
      "\n",
      "Train set: Avg. loss: 0.000047752, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.62544 108.985886\n",
      "l2 norm: 908.6578949532518\n",
      "l1 norm: 761.9710724678678\n",
      "Rbeta: 908.7975292770186\n",
      "\n",
      "Train set: Avg. loss: 0.000047653, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.65264 109.00593\n",
      "l2 norm: 908.620190440689\n",
      "l1 norm: 761.9399241597635\n",
      "Rbeta: 908.7593160257142\n",
      "\n",
      "Train set: Avg. loss: 0.000047556, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.67925 109.02595\n",
      "l2 norm: 908.5781148885477\n",
      "l1 norm: 761.9050756305563\n",
      "Rbeta: 908.7167629182877\n",
      "\n",
      "Train set: Avg. loss: 0.000047460, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.70513 109.04592\n",
      "l2 norm: 908.5228463145805\n",
      "l1 norm: 761.8592009590204\n",
      "Rbeta: 908.6610852913375\n",
      "\n",
      "Train set: Avg. loss: 0.000047367, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.72993 109.06587\n",
      "l2 norm: 908.4791061097368\n",
      "l1 norm: 761.8229840396818\n",
      "Rbeta: 908.6169991596931\n",
      "\n",
      "Train set: Avg. loss: 0.000047274, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.75473 109.08577\n",
      "l2 norm: 908.4401506106005\n",
      "l1 norm: 761.7908913870053\n",
      "Rbeta: 908.5776588750331\n",
      "\n",
      "Train set: Avg. loss: 0.000047182, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.779526 109.1056\n",
      "l2 norm: 908.4026485731172\n",
      "l1 norm: 761.7600531069586\n",
      "Rbeta: 908.539797506223\n",
      "\n",
      "Train set: Avg. loss: 0.000047090, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.80434 109.12543\n",
      "l2 norm: 908.364777829529\n",
      "l1 norm: 761.7288886596687\n",
      "Rbeta: 908.501554450096\n",
      "\n",
      "Train set: Avg. loss: 0.000046998, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.82915 109.145256\n",
      "l2 norm: 908.3367336309952\n",
      "l1 norm: 761.7059765600959\n",
      "Rbeta: 908.4731129709875\n",
      "\n",
      "Train set: Avg. loss: 0.000046906, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.85396 109.16502\n",
      "l2 norm: 908.3099608135861\n",
      "l1 norm: 761.6841657645897\n",
      "Rbeta: 908.4459786868679\n",
      "\n",
      "Train set: Avg. loss: 0.000046814, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.87877 109.18474\n",
      "l2 norm: 908.2789957527127\n",
      "l1 norm: 761.658869496453\n",
      "Rbeta: 908.4146542308406\n",
      "\n",
      "Train set: Avg. loss: 0.000046723, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.90359 109.2044\n",
      "l2 norm: 908.2412770724576\n",
      "l1 norm: 761.6279094309787\n",
      "Rbeta: 908.3765209348826\n",
      "\n",
      "Train set: Avg. loss: 0.000046632, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.92841 109.224014\n",
      "l2 norm: 908.2097993210344\n",
      "l1 norm: 761.6022080644049\n",
      "Rbeta: 908.3447350597456\n",
      "\n",
      "Train set: Avg. loss: 0.000046544, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.95203 109.24362\n",
      "l2 norm: 908.1795565273433\n",
      "l1 norm: 761.5775387696643\n",
      "Rbeta: 908.314146167932\n",
      "\n",
      "Train set: Avg. loss: 0.000046457, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.97503 109.26322\n",
      "l2 norm: 908.1407850547791\n",
      "l1 norm: 761.5457977709214\n",
      "Rbeta: 908.275170432137\n",
      "\n",
      "Train set: Avg. loss: 0.000046370, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.998024 109.28275\n",
      "l2 norm: 908.0934744782915\n",
      "l1 norm: 761.5069050878341\n",
      "Rbeta: 908.2275555287331\n",
      "\n",
      "Train set: Avg. loss: 0.000046284, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.02103 109.30221\n",
      "l2 norm: 908.0430685863089\n",
      "l1 norm: 761.4654044971569\n",
      "Rbeta: 908.1768530735701\n",
      "\n",
      "Train set: Avg. loss: 0.000046198, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.04404 109.321625\n",
      "l2 norm: 907.9981436385692\n",
      "l1 norm: 761.4284651907925\n",
      "Rbeta: 908.13168933622\n",
      "\n",
      "Train set: Avg. loss: 0.000046112, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.06704 109.34099\n",
      "l2 norm: 907.9579873743666\n",
      "l1 norm: 761.3954860160425\n",
      "Rbeta: 908.091196411404\n",
      "\n",
      "Train set: Avg. loss: 0.000046027, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.09005 109.3603\n",
      "l2 norm: 907.9172375746983\n",
      "l1 norm: 761.3619148673912\n",
      "Rbeta: 908.0501951959336\n",
      "\n",
      "Train set: Avg. loss: 0.000045941, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.11306 109.37953\n",
      "l2 norm: 907.8852565154531\n",
      "l1 norm: 761.3356976782695\n",
      "Rbeta: 908.0178930502027\n",
      "\n",
      "Train set: Avg. loss: 0.000045856, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.13607 109.39874\n",
      "l2 norm: 907.8542454858198\n",
      "l1 norm: 761.3102196270914\n",
      "Rbeta: 907.9866318285843\n",
      "\n",
      "Train set: Avg. loss: 0.000045771, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.159096 109.41797\n",
      "l2 norm: 907.826782421647\n",
      "l1 norm: 761.2877677390458\n",
      "Rbeta: 907.9588609942483\n",
      "\n",
      "Train set: Avg. loss: 0.000045687, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.18211 109.437195\n",
      "l2 norm: 907.7895626109442\n",
      "l1 norm: 761.2571152822244\n",
      "Rbeta: 907.9213684332649\n",
      "\n",
      "Train set: Avg. loss: 0.000045602, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.20513 109.45637\n",
      "l2 norm: 907.7615278962678\n",
      "l1 norm: 761.2341315583304\n",
      "Rbeta: 907.8930338672724\n",
      "\n",
      "Train set: Avg. loss: 0.000045518, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.228165 109.47547\n",
      "l2 norm: 907.7393018340379\n",
      "l1 norm: 761.2160398905157\n",
      "Rbeta: 907.8706119291105\n",
      "\n",
      "Train set: Avg. loss: 0.000045434, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.25118 109.49456\n",
      "l2 norm: 907.7144226495791\n",
      "l1 norm: 761.1958136634282\n",
      "Rbeta: 907.8453317588976\n",
      "\n",
      "Train set: Avg. loss: 0.000045350, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.274216 109.513626\n",
      "l2 norm: 907.6845417356415\n",
      "l1 norm: 761.1714801905651\n",
      "Rbeta: 907.8151685798271\n",
      "\n",
      "Train set: Avg. loss: 0.000045266, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.29725 109.532715\n",
      "l2 norm: 907.6420792552459\n",
      "l1 norm: 761.1366316024116\n",
      "Rbeta: 907.7724528245599\n",
      "\n",
      "Train set: Avg. loss: 0.000045182, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.32028 109.5518\n",
      "l2 norm: 907.5988979944847\n",
      "l1 norm: 761.1011739773926\n",
      "Rbeta: 907.7289687611786\n",
      "\n",
      "Train set: Avg. loss: 0.000045099, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.34332 109.5709\n",
      "l2 norm: 907.5572583372168\n",
      "l1 norm: 761.0669775199542\n",
      "Rbeta: 907.6869842024744\n",
      "\n",
      "Train set: Avg. loss: 0.000045016, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.366356 109.59001\n",
      "l2 norm: 907.5206853828605\n",
      "l1 norm: 761.0369886213421\n",
      "Rbeta: 907.6501432002581\n",
      "\n",
      "Train set: Avg. loss: 0.000044932, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.3894 109.60915\n",
      "l2 norm: 907.4841217371771\n",
      "l1 norm: 761.0070769856977\n",
      "Rbeta: 907.6132845964146\n",
      "\n",
      "Train set: Avg. loss: 0.000044849, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.412445 109.62835\n",
      "l2 norm: 907.4408107175831\n",
      "l1 norm: 760.9714457892405\n",
      "Rbeta: 907.5697686006617\n",
      "\n",
      "Train set: Avg. loss: 0.000044766, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.435486 109.64753\n",
      "l2 norm: 907.3865297215284\n",
      "l1 norm: 760.9266625891241\n",
      "Rbeta: 907.5150632471889\n",
      "\n",
      "Train set: Avg. loss: 0.000044683, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.45854 109.666695\n",
      "l2 norm: 907.326633418153\n",
      "l1 norm: 760.8772386265427\n",
      "Rbeta: 907.4550066811307\n",
      "\n",
      "Train set: Avg. loss: 0.000044601, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.4816 109.6858\n",
      "l2 norm: 907.272889191591\n",
      "l1 norm: 760.8328959941168\n",
      "Rbeta: 907.4008973335636\n",
      "\n",
      "Train set: Avg. loss: 0.000044518, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.504654 109.70488\n",
      "l2 norm: 907.2276576585017\n",
      "l1 norm: 760.7955990796274\n",
      "Rbeta: 907.3554401424645\n",
      "\n",
      "Train set: Avg. loss: 0.000044436, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.5277 109.72386\n",
      "l2 norm: 907.1835690038042\n",
      "l1 norm: 760.7592795875335\n",
      "Rbeta: 907.310967903438\n",
      "\n",
      "Train set: Avg. loss: 0.000044354, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.55076 109.74283\n",
      "l2 norm: 907.1313390257958\n",
      "l1 norm: 760.7161235929857\n",
      "Rbeta: 907.2584648015528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000044273, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.57382 109.76169\n",
      "l2 norm: 907.0761992387042\n",
      "l1 norm: 760.6705410106346\n",
      "Rbeta: 907.2029965256035\n",
      "\n",
      "Train set: Avg. loss: 0.000044191, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.596886 109.78052\n",
      "l2 norm: 907.0225436941021\n",
      "l1 norm: 760.6261695917188\n",
      "Rbeta: 907.1490720565688\n",
      "\n",
      "Train set: Avg. loss: 0.000044110, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.61996 109.799286\n",
      "l2 norm: 906.9736103168902\n",
      "l1 norm: 760.5857339787489\n",
      "Rbeta: 907.0998479678013\n",
      "\n",
      "Train set: Avg. loss: 0.000044030, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.64303 109.81806\n",
      "l2 norm: 906.9250352049955\n",
      "l1 norm: 760.5456809729826\n",
      "Rbeta: 907.050988071213\n",
      "\n",
      "Train set: Avg. loss: 0.000043949, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.6661 109.83683\n",
      "l2 norm: 906.8796368673087\n",
      "l1 norm: 760.5082668738355\n",
      "Rbeta: 907.0052664523496\n",
      "\n",
      "Train set: Avg. loss: 0.000043868, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.68918 109.85552\n",
      "l2 norm: 906.8337368973614\n",
      "l1 norm: 760.4704137127324\n",
      "Rbeta: 906.9590850566535\n",
      "\n",
      "Train set: Avg. loss: 0.000043788, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.71225 109.874214\n",
      "l2 norm: 906.7838206083153\n",
      "l1 norm: 760.4291599762352\n",
      "Rbeta: 906.908780701958\n",
      "\n",
      "Train set: Avg. loss: 0.000043708, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.73533 109.89286\n",
      "l2 norm: 906.7412989253128\n",
      "l1 norm: 760.3941445546275\n",
      "Rbeta: 906.8659612731169\n",
      "\n",
      "Train set: Avg. loss: 0.000043628, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.75841 109.91145\n",
      "l2 norm: 906.7048290368501\n",
      "l1 norm: 760.3642228255824\n",
      "Rbeta: 906.8291845155234\n",
      "\n",
      "Train set: Avg. loss: 0.000043549, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.781494 109.93004\n",
      "l2 norm: 906.6704375039988\n",
      "l1 norm: 760.336097077239\n",
      "Rbeta: 906.7944603180987\n",
      "\n",
      "Train set: Avg. loss: 0.000043469, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.80459 109.94863\n",
      "l2 norm: 906.6360597903004\n",
      "l1 norm: 760.3079992341104\n",
      "Rbeta: 906.7598159633303\n",
      "\n",
      "Train set: Avg. loss: 0.000043390, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.827675 109.96722\n",
      "l2 norm: 906.5954933340871\n",
      "l1 norm: 760.2747691720842\n",
      "Rbeta: 906.7189077436815\n",
      "\n",
      "Train set: Avg. loss: 0.000043311, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.85076 109.98577\n",
      "l2 norm: 906.552755509704\n",
      "l1 norm: 760.2397932139464\n",
      "Rbeta: 906.675857591426\n",
      "\n",
      "Train set: Avg. loss: 0.000043232, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.87386 110.004295\n",
      "l2 norm: 906.5147768738316\n",
      "l1 norm: 760.2088207272793\n",
      "Rbeta: 906.6375647892067\n",
      "\n",
      "Train set: Avg. loss: 0.000043153, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.89695 110.02279\n",
      "l2 norm: 906.4808351893229\n",
      "l1 norm: 760.1812692326484\n",
      "Rbeta: 906.603276759728\n",
      "\n",
      "Train set: Avg. loss: 0.000043074, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.92006 110.04118\n",
      "l2 norm: 906.4512842925034\n",
      "l1 norm: 760.1574238860159\n",
      "Rbeta: 906.573427228293\n",
      "\n",
      "Train set: Avg. loss: 0.000042996, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.94315 110.059616\n",
      "l2 norm: 906.4144276741227\n",
      "l1 norm: 760.1273738357845\n",
      "Rbeta: 906.5362694876251\n",
      "\n",
      "Train set: Avg. loss: 0.000042918, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.96626 110.07804\n",
      "l2 norm: 906.3769653124086\n",
      "l1 norm: 760.096808030763\n",
      "Rbeta: 906.4984520143107\n",
      "\n",
      "Train set: Avg. loss: 0.000042840, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.989365 110.09647\n",
      "l2 norm: 906.3404042465971\n",
      "l1 norm: 760.0670786177491\n",
      "Rbeta: 906.4616109826798\n",
      "\n",
      "Train set: Avg. loss: 0.000042762, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.012474 110.11485\n",
      "l2 norm: 906.3073863119351\n",
      "l1 norm: 760.0403179907348\n",
      "Rbeta: 906.4281973247395\n",
      "\n",
      "Train set: Avg. loss: 0.000042684, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.03559 110.13316\n",
      "l2 norm: 906.2605880491109\n",
      "l1 norm: 760.0019253357273\n",
      "Rbeta: 906.3811426288454\n",
      "\n",
      "Train set: Avg. loss: 0.000042607, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.0587 110.151344\n",
      "l2 norm: 906.2085628312824\n",
      "l1 norm: 759.9590797848017\n",
      "Rbeta: 906.3287404623652\n",
      "\n",
      "Train set: Avg. loss: 0.000042530, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.08182 110.1694\n",
      "l2 norm: 906.1606196973037\n",
      "l1 norm: 759.9196421273127\n",
      "Rbeta: 906.2804745447986\n",
      "\n",
      "Train set: Avg. loss: 0.000042453, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.10494 110.18739\n",
      "l2 norm: 906.1219734002801\n",
      "l1 norm: 759.8879799598717\n",
      "Rbeta: 906.2414391835486\n",
      "\n",
      "Train set: Avg. loss: 0.000042380, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.12635 110.205376\n",
      "l2 norm: 906.0809347347302\n",
      "l1 norm: 759.8543531717431\n",
      "Rbeta: 906.2001945875968\n",
      "\n",
      "Train set: Avg. loss: 0.000042307, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.14764 110.22328\n",
      "l2 norm: 906.046336462625\n",
      "l1 norm: 759.8260768199724\n",
      "Rbeta: 906.1653395512208\n",
      "\n",
      "Train set: Avg. loss: 0.000042235, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.16892 110.241135\n",
      "l2 norm: 906.025850797919\n",
      "l1 norm: 759.8096382046277\n",
      "Rbeta: 906.1446668503781\n",
      "\n",
      "Train set: Avg. loss: 0.000042162, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.190216 110.25897\n",
      "l2 norm: 906.0071863264187\n",
      "l1 norm: 759.7946780415893\n",
      "Rbeta: 906.125732113481\n",
      "\n",
      "Train set: Avg. loss: 0.000042090, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.2115 110.27679\n",
      "l2 norm: 905.9839865398427\n",
      "l1 norm: 759.7758716621021\n",
      "Rbeta: 906.1022338995942\n",
      "\n",
      "Train set: Avg. loss: 0.000042018, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.232796 110.29454\n",
      "l2 norm: 905.9572908642709\n",
      "l1 norm: 759.7541322723996\n",
      "Rbeta: 906.075356379704\n",
      "\n",
      "Train set: Avg. loss: 0.000041946, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.25409 110.31223\n",
      "l2 norm: 905.9248011509463\n",
      "l1 norm: 759.7275411246258\n",
      "Rbeta: 906.0425425761168\n",
      "\n",
      "Train set: Avg. loss: 0.000041874, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.27539 110.32987\n",
      "l2 norm: 905.8987587510966\n",
      "l1 norm: 759.7063758393675\n",
      "Rbeta: 906.0162487105318\n",
      "\n",
      "Train set: Avg. loss: 0.000041805, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.29571 110.34751\n",
      "l2 norm: 905.8671314837579\n",
      "l1 norm: 759.6805099042992\n",
      "Rbeta: 905.9844394957602\n",
      "\n",
      "Train set: Avg. loss: 0.000041737, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.31517 110.36512\n",
      "l2 norm: 905.8314411846393\n",
      "l1 norm: 759.6511995844717\n",
      "Rbeta: 905.9486232453393\n",
      "\n",
      "Train set: Avg. loss: 0.000041669, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.33464 110.38272\n",
      "l2 norm: 905.8001571761062\n",
      "l1 norm: 759.625472493202\n",
      "Rbeta: 905.9171959380824\n",
      "\n",
      "Train set: Avg. loss: 0.000041601, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.3541 110.40034\n",
      "l2 norm: 905.7711656824042\n",
      "l1 norm: 759.6016509563191\n",
      "Rbeta: 905.8880350957498\n",
      "\n",
      "Train set: Avg. loss: 0.000041534, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.373566 110.41794\n",
      "l2 norm: 905.7375170986336\n",
      "l1 norm: 759.5739077841122\n",
      "Rbeta: 905.8542188418049\n",
      "\n",
      "Train set: Avg. loss: 0.000041466, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.393036 110.43555\n",
      "l2 norm: 905.6983082659788\n",
      "l1 norm: 759.5415120727491\n",
      "Rbeta: 905.8148671773223\n",
      "\n",
      "Train set: Avg. loss: 0.000041399, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.412506 110.45311\n",
      "l2 norm: 905.6526660226953\n",
      "l1 norm: 759.5036814390048\n",
      "Rbeta: 905.7690572680763\n",
      "\n",
      "Train set: Avg. loss: 0.000041332, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.43197 110.47068\n",
      "l2 norm: 905.6085554563086\n",
      "l1 norm: 759.4672007134945\n",
      "Rbeta: 905.7247822137227\n",
      "\n",
      "Train set: Avg. loss: 0.000041264, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.45145 110.48822\n",
      "l2 norm: 905.5637005869157\n",
      "l1 norm: 759.4300953546294\n",
      "Rbeta: 905.6797901224645\n",
      "\n",
      "Train set: Avg. loss: 0.000041198, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.47092 110.50569\n",
      "l2 norm: 905.5176808546902\n",
      "l1 norm: 759.3920266573969\n",
      "Rbeta: 905.6335476419367\n",
      "\n",
      "Train set: Avg. loss: 0.000041131, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.490395 110.52315\n",
      "l2 norm: 905.473479137089\n",
      "l1 norm: 759.355513281829\n",
      "Rbeta: 905.5892313512508\n",
      "\n",
      "Train set: Avg. loss: 0.000041064, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.50987 110.5406\n",
      "l2 norm: 905.4297981326885\n",
      "l1 norm: 759.3194287597639\n",
      "Rbeta: 905.545319347384\n",
      "\n",
      "Train set: Avg. loss: 0.000041000, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.52788 110.5581\n",
      "l2 norm: 905.3847964490699\n",
      "l1 norm: 759.2822808315235\n",
      "Rbeta: 905.5002330956758\n",
      "\n",
      "Train set: Avg. loss: 0.000040937, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.545525 110.57561\n",
      "l2 norm: 905.3427448051302\n",
      "l1 norm: 759.247602135961\n",
      "Rbeta: 905.4581411840273\n",
      "\n",
      "Train set: Avg. loss: 0.000040874, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.563156 110.59312\n",
      "l2 norm: 905.2981720849294\n",
      "l1 norm: 759.2107769768268\n",
      "Rbeta: 905.4135469846286\n",
      "\n",
      "Train set: Avg. loss: 0.000040811, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.5808 110.61061\n",
      "l2 norm: 905.2455054986107\n",
      "l1 norm: 759.1671766053403\n",
      "Rbeta: 905.3607838549659\n",
      "\n",
      "Train set: Avg. loss: 0.000040748, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.59844 110.62808\n",
      "l2 norm: 905.1922282457159\n",
      "l1 norm: 759.1231089216054\n",
      "Rbeta: 905.3074842218017\n",
      "\n",
      "Train set: Avg. loss: 0.000040685, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.61609 110.64555\n",
      "l2 norm: 905.1514005236965\n",
      "l1 norm: 759.0894550602853\n",
      "Rbeta: 905.2665653322239\n",
      "\n",
      "Train set: Avg. loss: 0.000040623, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.633736 110.66302\n",
      "l2 norm: 905.1090045333111\n",
      "l1 norm: 759.0544425047631\n",
      "Rbeta: 905.2240971145333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000040560, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.65139 110.68047\n",
      "l2 norm: 905.0732909089994\n",
      "l1 norm: 759.0250645475178\n",
      "Rbeta: 905.1883571008032\n",
      "\n",
      "Train set: Avg. loss: 0.000040498, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.66904 110.69788\n",
      "l2 norm: 905.044354260855\n",
      "l1 norm: 759.0013716290518\n",
      "Rbeta: 905.1593110925096\n",
      "\n",
      "Train set: Avg. loss: 0.000040436, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.686676 110.71528\n",
      "l2 norm: 905.0133981017125\n",
      "l1 norm: 758.9760168263234\n",
      "Rbeta: 905.128256654234\n",
      "\n",
      "Train set: Avg. loss: 0.000040373, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.70433 110.73265\n",
      "l2 norm: 904.9801653444172\n",
      "l1 norm: 758.9488180086873\n",
      "Rbeta: 905.0949789684637\n",
      "\n",
      "Train set: Avg. loss: 0.000040311, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.72199 110.749985\n",
      "l2 norm: 904.9453958368377\n",
      "l1 norm: 758.9203886382483\n",
      "Rbeta: 905.0601912222277\n",
      "\n",
      "Train set: Avg. loss: 0.000040250, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.73965 110.767265\n",
      "l2 norm: 904.9060438270627\n",
      "l1 norm: 758.8881091021474\n",
      "Rbeta: 905.0207444847972\n",
      "\n",
      "Train set: Avg. loss: 0.000040188, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.7573 110.78451\n",
      "l2 norm: 904.8652657640478\n",
      "l1 norm: 758.854637306471\n",
      "Rbeta: 904.9799030444736\n",
      "\n",
      "Train set: Avg. loss: 0.000040127, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.774956 110.8017\n",
      "l2 norm: 904.817617276887\n",
      "l1 norm: 758.8153930156689\n",
      "Rbeta: 904.9321894556908\n",
      "\n",
      "Train set: Avg. loss: 0.000040068, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.79142 110.81882\n",
      "l2 norm: 904.7707160637012\n",
      "l1 norm: 758.7768532590849\n",
      "Rbeta: 904.8852184733294\n",
      "\n",
      "Train set: Avg. loss: 0.000040010, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.80723 110.835915\n",
      "l2 norm: 904.7257276766512\n",
      "l1 norm: 758.7399541924252\n",
      "Rbeta: 904.8402385692555\n",
      "\n",
      "Train set: Avg. loss: 0.000039952, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.82305 110.85301\n",
      "l2 norm: 904.6830366541742\n",
      "l1 norm: 758.7049588116099\n",
      "Rbeta: 904.7975638764157\n",
      "\n",
      "Train set: Avg. loss: 0.000039894, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.83887 110.870056\n",
      "l2 norm: 904.6357531428106\n",
      "l1 norm: 758.6660913166852\n",
      "Rbeta: 904.7503107259395\n",
      "\n",
      "Train set: Avg. loss: 0.000039837, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.85468 110.88705\n",
      "l2 norm: 904.5869965603631\n",
      "l1 norm: 758.6260008150556\n",
      "Rbeta: 904.7015418589762\n",
      "\n",
      "Train set: Avg. loss: 0.000039780, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.87051 110.90401\n",
      "l2 norm: 904.5390508460387\n",
      "l1 norm: 758.5865188906737\n",
      "Rbeta: 904.6535585625693\n",
      "\n",
      "Train set: Avg. loss: 0.000039722, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.88633 110.92094\n",
      "l2 norm: 904.501286992959\n",
      "l1 norm: 758.5555669426035\n",
      "Rbeta: 904.6158415229185\n",
      "\n",
      "Train set: Avg. loss: 0.000039665, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.90215 110.93786\n",
      "l2 norm: 904.4651511243279\n",
      "l1 norm: 758.526004441448\n",
      "Rbeta: 904.5797473514314\n",
      "\n",
      "Train set: Avg. loss: 0.000039608, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.91798 110.95476\n",
      "l2 norm: 904.4371585162165\n",
      "l1 norm: 758.5032705397014\n",
      "Rbeta: 904.5517923805513\n",
      "\n",
      "Train set: Avg. loss: 0.000039551, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.9338 110.97159\n",
      "l2 norm: 904.4128519118834\n",
      "l1 norm: 758.4836213767883\n",
      "Rbeta: 904.5274864928833\n",
      "\n",
      "Train set: Avg. loss: 0.000039495, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.94962 110.98843\n",
      "l2 norm: 904.3919775727633\n",
      "l1 norm: 758.4668044386889\n",
      "Rbeta: 904.5065858090678\n",
      "\n",
      "Train set: Avg. loss: 0.000039438, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.965454 111.005226\n",
      "l2 norm: 904.3742368763228\n",
      "l1 norm: 758.4526069275053\n",
      "Rbeta: 904.4887952597608\n",
      "\n",
      "Train set: Avg. loss: 0.000039382, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.981285 111.02203\n",
      "l2 norm: 904.3481367576618\n",
      "l1 norm: 758.4314100689077\n",
      "Rbeta: 904.4627875227637\n",
      "\n",
      "Train set: Avg. loss: 0.000039325, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.99711 111.03883\n",
      "l2 norm: 904.3215692045077\n",
      "l1 norm: 758.4098362729909\n",
      "Rbeta: 904.4362178273551\n",
      "\n",
      "Train set: Avg. loss: 0.000039269, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.01295 111.05558\n",
      "l2 norm: 904.2960676926172\n",
      "l1 norm: 758.389156866715\n",
      "Rbeta: 904.4106926553666\n",
      "\n",
      "Train set: Avg. loss: 0.000039213, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.02878 111.0723\n",
      "l2 norm: 904.2798209557582\n",
      "l1 norm: 758.3762497572997\n",
      "Rbeta: 904.3943959082735\n",
      "\n",
      "Train set: Avg. loss: 0.000039157, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.04461 111.089\n",
      "l2 norm: 904.2705637873546\n",
      "l1 norm: 758.3692099253963\n",
      "Rbeta: 904.3851607060471\n",
      "\n",
      "Train set: Avg. loss: 0.000039101, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.06045 111.10569\n",
      "l2 norm: 904.2625203926837\n",
      "l1 norm: 758.3631140431658\n",
      "Rbeta: 904.377151832404\n",
      "\n",
      "Train set: Avg. loss: 0.000039045, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.07629 111.12231\n",
      "l2 norm: 904.2477687608421\n",
      "l1 norm: 758.3513095400813\n",
      "Rbeta: 904.362381169313\n",
      "\n",
      "Train set: Avg. loss: 0.000038989, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.09212 111.13893\n",
      "l2 norm: 904.2345007488134\n",
      "l1 norm: 758.3407135551196\n",
      "Rbeta: 904.3490606781056\n",
      "\n",
      "Train set: Avg. loss: 0.000038934, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.107956 111.15558\n",
      "l2 norm: 904.2188405136884\n",
      "l1 norm: 758.3280964585687\n",
      "Rbeta: 904.3334249065676\n",
      "\n",
      "Train set: Avg. loss: 0.000038878, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.123795 111.17224\n",
      "l2 norm: 904.201252103572\n",
      "l1 norm: 758.3138867269923\n",
      "Rbeta: 904.3158567562774\n",
      "\n",
      "Train set: Avg. loss: 0.000038822, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.13963 111.188896\n",
      "l2 norm: 904.1764454621302\n",
      "l1 norm: 758.2935969853871\n",
      "Rbeta: 904.2909535135371\n",
      "\n",
      "Train set: Avg. loss: 0.000038767, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.15548 111.20555\n",
      "l2 norm: 904.1535243021958\n",
      "l1 norm: 758.2749275406018\n",
      "Rbeta: 904.26806603454\n",
      "\n",
      "Train set: Avg. loss: 0.000038712, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.171326 111.22221\n",
      "l2 norm: 904.1298371928103\n",
      "l1 norm: 758.2556662069851\n",
      "Rbeta: 904.2444163568225\n",
      "\n",
      "Train set: Avg. loss: 0.000038657, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.187164 111.23888\n",
      "l2 norm: 904.1008842587495\n",
      "l1 norm: 758.2320418575287\n",
      "Rbeta: 904.2154372952219\n",
      "\n",
      "Train set: Avg. loss: 0.000038601, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.20301 111.25554\n",
      "l2 norm: 904.0682870257633\n",
      "l1 norm: 758.2053183248144\n",
      "Rbeta: 904.182867790982\n",
      "\n",
      "Train set: Avg. loss: 0.000038546, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.21886 111.272194\n",
      "l2 norm: 904.0344912368292\n",
      "l1 norm: 758.1775724232423\n",
      "Rbeta: 904.1490819506198\n",
      "\n",
      "Train set: Avg. loss: 0.000038491, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.2347 111.28881\n",
      "l2 norm: 904.0094074933113\n",
      "l1 norm: 758.157096446997\n",
      "Rbeta: 904.1239425299694\n",
      "\n",
      "Train set: Avg. loss: 0.000038436, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.25055 111.3055\n",
      "l2 norm: 903.974666504294\n",
      "l1 norm: 758.1285390640359\n",
      "Rbeta: 904.0891959729618\n",
      "\n",
      "Train set: Avg. loss: 0.000038381, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.2664 111.32217\n",
      "l2 norm: 903.936286961387\n",
      "l1 norm: 758.0969761112739\n",
      "Rbeta: 904.0507945108398\n",
      "\n",
      "Train set: Avg. loss: 0.000038327, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.28226 111.33876\n",
      "l2 norm: 903.8985643264942\n",
      "l1 norm: 758.0659819777212\n",
      "Rbeta: 904.0130436324029\n",
      "\n",
      "Train set: Avg. loss: 0.000038272, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.2981 111.35534\n",
      "l2 norm: 903.8596173310787\n",
      "l1 norm: 758.0339342378237\n",
      "Rbeta: 903.9740727403673\n",
      "\n",
      "Train set: Avg. loss: 0.000038218, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.313965 111.371895\n",
      "l2 norm: 903.8179911520241\n",
      "l1 norm: 757.9996006943484\n",
      "Rbeta: 903.9324490375391\n",
      "\n",
      "Train set: Avg. loss: 0.000038163, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.32981 111.38843\n",
      "l2 norm: 903.7801996999826\n",
      "l1 norm: 757.9684938028736\n",
      "Rbeta: 903.8945945965828\n",
      "\n",
      "Train set: Avg. loss: 0.000038109, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.34567 111.40497\n",
      "l2 norm: 903.7444693301112\n",
      "l1 norm: 757.9391271728305\n",
      "Rbeta: 903.8589297098929\n",
      "\n",
      "Train set: Avg. loss: 0.000038055, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.36153 111.42147\n",
      "l2 norm: 903.7104735338954\n",
      "l1 norm: 757.9111408080107\n",
      "Rbeta: 903.8248548688006\n",
      "\n",
      "Train set: Avg. loss: 0.000038001, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.37739 111.437965\n",
      "l2 norm: 903.6755530773698\n",
      "l1 norm: 757.8823377293292\n",
      "Rbeta: 903.7899693956973\n",
      "\n",
      "Train set: Avg. loss: 0.000037947, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.39325 111.454445\n",
      "l2 norm: 903.6466917741843\n",
      "l1 norm: 757.8586646614426\n",
      "Rbeta: 903.7610611234642\n",
      "\n",
      "Train set: Avg. loss: 0.000037893, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.4091 111.47087\n",
      "l2 norm: 903.615339755647\n",
      "l1 norm: 757.8329077554336\n",
      "Rbeta: 903.7297057314913\n",
      "\n",
      "Train set: Avg. loss: 0.000037840, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.424965 111.48727\n",
      "l2 norm: 903.579289571522\n",
      "l1 norm: 757.8031685956014\n",
      "Rbeta: 903.6935880083937\n",
      "\n",
      "Train set: Avg. loss: 0.000037786, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.440834 111.50369\n",
      "l2 norm: 903.5393594775171\n",
      "l1 norm: 757.7702079615987\n",
      "Rbeta: 903.653681150576\n",
      "\n",
      "Train set: Avg. loss: 0.000037733, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.456696 111.520096\n",
      "l2 norm: 903.4976774672501\n",
      "l1 norm: 757.7357723675793\n",
      "Rbeta: 903.61195691302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000037679, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.472565 111.53648\n",
      "l2 norm: 903.4606838719667\n",
      "l1 norm: 757.7052688733736\n",
      "Rbeta: 903.5749969844231\n",
      "\n",
      "Train set: Avg. loss: 0.000037626, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.488434 111.55283\n",
      "l2 norm: 903.4270557893108\n",
      "l1 norm: 757.6776212982631\n",
      "Rbeta: 903.5412906403224\n",
      "\n",
      "Train set: Avg. loss: 0.000037573, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.504295 111.56914\n",
      "l2 norm: 903.3900643506493\n",
      "l1 norm: 757.647149335144\n",
      "Rbeta: 903.5042581715232\n",
      "\n",
      "Train set: Avg. loss: 0.000037520, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.520164 111.585396\n",
      "l2 norm: 903.3598541278366\n",
      "l1 norm: 757.6223241173675\n",
      "Rbeta: 903.4740218942243\n",
      "\n",
      "Train set: Avg. loss: 0.000037468, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.5355 111.60165\n",
      "l2 norm: 903.3293401022195\n",
      "l1 norm: 757.5972851638592\n",
      "Rbeta: 903.4435310557392\n",
      "\n",
      "Train set: Avg. loss: 0.000037418, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.54952 111.61789\n",
      "l2 norm: 903.3004683596109\n",
      "l1 norm: 757.5737017617867\n",
      "Rbeta: 903.4147369399358\n",
      "\n",
      "Train set: Avg. loss: 0.000037368, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.563545 111.634155\n",
      "l2 norm: 903.2778041893291\n",
      "l1 norm: 757.5552841417424\n",
      "Rbeta: 903.3921380789875\n",
      "\n",
      "Train set: Avg. loss: 0.000037318, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.57756 111.650444\n",
      "l2 norm: 903.2484628771005\n",
      "l1 norm: 757.5312642761454\n",
      "Rbeta: 903.362824796183\n",
      "\n",
      "Train set: Avg. loss: 0.000037269, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.59158 111.66675\n",
      "l2 norm: 903.2117947289684\n",
      "l1 norm: 757.5011216175283\n",
      "Rbeta: 903.3262491340312\n",
      "\n",
      "Train set: Avg. loss: 0.000037219, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.605606 111.683044\n",
      "l2 norm: 903.1732243019723\n",
      "l1 norm: 757.4693467820236\n",
      "Rbeta: 903.2877473875003\n",
      "\n",
      "Train set: Avg. loss: 0.000037170, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.61964 111.6993\n",
      "l2 norm: 903.1398086653686\n",
      "l1 norm: 757.4418692320048\n",
      "Rbeta: 903.2544317361253\n",
      "\n",
      "Train set: Avg. loss: 0.000037120, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.63366 111.71555\n",
      "l2 norm: 903.1076359043927\n",
      "l1 norm: 757.4153846796419\n",
      "Rbeta: 903.222334047223\n",
      "\n",
      "Train set: Avg. loss: 0.000037071, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.64769 111.73174\n",
      "l2 norm: 903.0741614790282\n",
      "l1 norm: 757.3878169032184\n",
      "Rbeta: 903.1888941585481\n",
      "\n",
      "Train set: Avg. loss: 0.000037022, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.66172 111.74791\n",
      "l2 norm: 903.0349188169806\n",
      "l1 norm: 757.3553698725772\n",
      "Rbeta: 903.1497859029055\n",
      "\n",
      "Train set: Avg. loss: 0.000036973, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.67574 111.76401\n",
      "l2 norm: 902.9944724159458\n",
      "l1 norm: 757.3219047458479\n",
      "Rbeta: 903.1093794135704\n",
      "\n",
      "Train set: Avg. loss: 0.000036924, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.68977 111.780075\n",
      "l2 norm: 902.94932647393\n",
      "l1 norm: 757.2845127102967\n",
      "Rbeta: 903.064232482619\n",
      "\n",
      "Train set: Avg. loss: 0.000036875, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.703804 111.79615\n",
      "l2 norm: 902.9156463044898\n",
      "l1 norm: 757.2567690443859\n",
      "Rbeta: 903.0306823524357\n",
      "\n",
      "Train set: Avg. loss: 0.000036827, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.717834 111.812195\n",
      "l2 norm: 902.8830491926577\n",
      "l1 norm: 757.2299499715398\n",
      "Rbeta: 902.9981208487433\n",
      "\n",
      "Train set: Avg. loss: 0.000036778, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.73187 111.828224\n",
      "l2 norm: 902.8487289600457\n",
      "l1 norm: 757.2016832123641\n",
      "Rbeta: 902.9638555481062\n",
      "\n",
      "Train set: Avg. loss: 0.000036730, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.7459 111.84424\n",
      "l2 norm: 902.8106215916937\n",
      "l1 norm: 757.1702085379363\n",
      "Rbeta: 902.9258572106997\n",
      "\n",
      "Train set: Avg. loss: 0.000036681, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.75993 111.86024\n",
      "l2 norm: 902.7718801568619\n",
      "l1 norm: 757.1382081211173\n",
      "Rbeta: 902.8871418632081\n",
      "\n",
      "Train set: Avg. loss: 0.000036633, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.77388 111.876205\n",
      "l2 norm: 902.7466194593836\n",
      "l1 norm: 757.1175612516103\n",
      "Rbeta: 902.8619234167302\n",
      "\n",
      "Train set: Avg. loss: 0.000036588, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.786064 111.89208\n",
      "l2 norm: 902.7240731295132\n",
      "l1 norm: 757.0992064821949\n",
      "Rbeta: 902.839562178165\n",
      "\n",
      "Train set: Avg. loss: 0.000036543, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.798256 111.90792\n",
      "l2 norm: 902.701768601226\n",
      "l1 norm: 757.0810555079469\n",
      "Rbeta: 902.8174130825845\n",
      "\n",
      "Train set: Avg. loss: 0.000036498, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.81043 111.92376\n",
      "l2 norm: 902.6754409584852\n",
      "l1 norm: 757.0595072689571\n",
      "Rbeta: 902.7912467913999\n",
      "\n",
      "Train set: Avg. loss: 0.000036453, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.82262 111.93957\n",
      "l2 norm: 902.6475220301289\n",
      "l1 norm: 757.0366207824695\n",
      "Rbeta: 902.7634235729182\n",
      "\n",
      "Train set: Avg. loss: 0.000036408, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.83481 111.955345\n",
      "l2 norm: 902.6226049424931\n",
      "l1 norm: 757.0162006375498\n",
      "Rbeta: 902.7387441243752\n",
      "\n",
      "Train set: Avg. loss: 0.000036364, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.84699 111.97107\n",
      "l2 norm: 902.6044284908353\n",
      "l1 norm: 757.0015351383977\n",
      "Rbeta: 902.7206268316236\n",
      "\n",
      "Train set: Avg. loss: 0.000036319, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.859184 111.9868\n",
      "l2 norm: 902.5856274913737\n",
      "l1 norm: 756.9863646360005\n",
      "Rbeta: 902.7020347812041\n",
      "\n",
      "Train set: Avg. loss: 0.000036275, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.871376 112.0025\n",
      "l2 norm: 902.5620224171797\n",
      "l1 norm: 756.9671396831166\n",
      "Rbeta: 902.6785958533503\n",
      "\n",
      "Train set: Avg. loss: 0.000036230, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.88356 112.01818\n",
      "l2 norm: 902.5373319434618\n",
      "l1 norm: 756.9469934025228\n",
      "Rbeta: 902.6539950778158\n",
      "\n",
      "Train set: Avg. loss: 0.000036186, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.89575 112.03384\n",
      "l2 norm: 902.5079082083437\n",
      "l1 norm: 756.9228482015999\n",
      "Rbeta: 902.624785885619\n",
      "\n",
      "Train set: Avg. loss: 0.000036142, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.90794 112.04944\n",
      "l2 norm: 902.472891634856\n",
      "l1 norm: 756.8939486257727\n",
      "Rbeta: 902.5898900940506\n",
      "\n",
      "Train set: Avg. loss: 0.000036098, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.920135 112.06496\n",
      "l2 norm: 902.4422861507917\n",
      "l1 norm: 756.8687627430588\n",
      "Rbeta: 902.5594181186476\n",
      "\n",
      "Train set: Avg. loss: 0.000036054, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.93233 112.08045\n",
      "l2 norm: 902.4159859807048\n",
      "l1 norm: 756.8471911371656\n",
      "Rbeta: 902.5332870948762\n",
      "\n",
      "Train set: Avg. loss: 0.000036010, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.94453 112.09594\n",
      "l2 norm: 902.3975764862013\n",
      "l1 norm: 756.8323151171405\n",
      "Rbeta: 902.5150089054287\n",
      "\n",
      "Train set: Avg. loss: 0.000035966, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.95672 112.11146\n",
      "l2 norm: 902.3722494916813\n",
      "l1 norm: 756.8116891386543\n",
      "Rbeta: 902.4898332646027\n",
      "\n",
      "Train set: Avg. loss: 0.000035922, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.96891 112.12696\n",
      "l2 norm: 902.3450477909255\n",
      "l1 norm: 756.7895050807285\n",
      "Rbeta: 902.4627367707566\n",
      "\n",
      "Train set: Avg. loss: 0.000035879, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.98111 112.142494\n",
      "l2 norm: 902.3220601285105\n",
      "l1 norm: 756.7708188420058\n",
      "Rbeta: 902.4398892893961\n",
      "\n",
      "Train set: Avg. loss: 0.000035835, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.9933 112.158035\n",
      "l2 norm: 902.2980588615138\n",
      "l1 norm: 756.751254742731\n",
      "Rbeta: 902.4160135326998\n",
      "\n",
      "Train set: Avg. loss: 0.000035791, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.00551 112.173584\n",
      "l2 norm: 902.2739365627407\n",
      "l1 norm: 756.7315578815931\n",
      "Rbeta: 902.3921524210982\n",
      "\n",
      "Train set: Avg. loss: 0.000035748, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.0177 112.18907\n",
      "l2 norm: 902.2485141202407\n",
      "l1 norm: 756.7108069781841\n",
      "Rbeta: 902.3667806002148\n",
      "\n",
      "Train set: Avg. loss: 0.000035704, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.02989 112.20456\n",
      "l2 norm: 902.2215201988306\n",
      "l1 norm: 756.6887814267928\n",
      "Rbeta: 902.3399358367604\n",
      "\n",
      "Train set: Avg. loss: 0.000035661, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.0421 112.22001\n",
      "l2 norm: 902.187423886555\n",
      "l1 norm: 756.6607462625145\n",
      "Rbeta: 902.3060736556558\n",
      "\n",
      "Train set: Avg. loss: 0.000035618, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.05429 112.23544\n",
      "l2 norm: 902.1516418407974\n",
      "l1 norm: 756.6312956513697\n",
      "Rbeta: 902.2703364394482\n",
      "\n",
      "Train set: Avg. loss: 0.000035575, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.0665 112.25083\n",
      "l2 norm: 902.1164711758906\n",
      "l1 norm: 756.6024229823524\n",
      "Rbeta: 902.2353086826233\n",
      "\n",
      "Train set: Avg. loss: 0.000035532, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.078705 112.26618\n",
      "l2 norm: 902.0839784902803\n",
      "l1 norm: 756.575844803137\n",
      "Rbeta: 902.202962639931\n",
      "\n",
      "Train set: Avg. loss: 0.000035489, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.090904 112.28151\n",
      "l2 norm: 902.0490407622482\n",
      "l1 norm: 756.5472178343332\n",
      "Rbeta: 902.1681467837627\n",
      "\n",
      "Train set: Avg. loss: 0.000035446, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.1031 112.296844\n",
      "l2 norm: 902.0095106132386\n",
      "l1 norm: 756.5147288240947\n",
      "Rbeta: 902.128748575956\n",
      "\n",
      "Train set: Avg. loss: 0.000035403, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.11531 112.31219\n",
      "l2 norm: 901.97156269005\n",
      "l1 norm: 756.4835415529685\n",
      "Rbeta: 902.0909517403488\n",
      "\n",
      "Train set: Avg. loss: 0.000035360, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.12752 112.327515\n",
      "l2 norm: 901.9315193541925\n",
      "l1 norm: 756.4505591396635\n",
      "Rbeta: 902.0510869064469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000035317, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.139725 112.34284\n",
      "l2 norm: 901.8937969308968\n",
      "l1 norm: 756.4194901407959\n",
      "Rbeta: 902.0135345931247\n",
      "\n",
      "Train set: Avg. loss: 0.000035274, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.15193 112.35818\n",
      "l2 norm: 901.8649891321948\n",
      "l1 norm: 756.3959053922458\n",
      "Rbeta: 901.9847987745625\n",
      "\n",
      "Train set: Avg. loss: 0.000035232, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.16414 112.3735\n",
      "l2 norm: 901.8314184177522\n",
      "l1 norm: 756.3683023518981\n",
      "Rbeta: 901.9513660610761\n",
      "\n",
      "Train set: Avg. loss: 0.000035189, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.176346 112.38881\n",
      "l2 norm: 901.7965295327347\n",
      "l1 norm: 756.3396006193211\n",
      "Rbeta: 901.9166311364919\n",
      "\n",
      "Train set: Avg. loss: 0.000035147, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.18855 112.404106\n",
      "l2 norm: 901.7679547135539\n",
      "l1 norm: 756.3161612325335\n",
      "Rbeta: 901.8881282130623\n",
      "\n",
      "Train set: Avg. loss: 0.000035104, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.20076 112.41941\n",
      "l2 norm: 901.7424150669875\n",
      "l1 norm: 756.2952165017364\n",
      "Rbeta: 901.8628208669361\n",
      "\n",
      "Train set: Avg. loss: 0.000035062, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.212975 112.43469\n",
      "l2 norm: 901.7161121496891\n",
      "l1 norm: 756.2736363159995\n",
      "Rbeta: 901.8366862421515\n",
      "\n",
      "Train set: Avg. loss: 0.000035021, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.223976 112.44998\n",
      "l2 norm: 901.6927534020316\n",
      "l1 norm: 756.254529249803\n",
      "Rbeta: 901.8134111543608\n",
      "\n",
      "Train set: Avg. loss: 0.000034985, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.232475 112.46525\n",
      "l2 norm: 901.6693281442999\n",
      "l1 norm: 756.2354001982708\n",
      "Rbeta: 901.7904037783608\n",
      "\n",
      "Train set: Avg. loss: 0.000034948, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.240974 112.4805\n",
      "l2 norm: 901.6367617816574\n",
      "l1 norm: 756.2086084789894\n",
      "Rbeta: 901.7581516991255\n",
      "\n",
      "Train set: Avg. loss: 0.000034911, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.24947 112.495735\n",
      "l2 norm: 901.6049397832206\n",
      "l1 norm: 756.1824509013661\n",
      "Rbeta: 901.7266738007836\n",
      "\n",
      "Train set: Avg. loss: 0.000034875, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.25797 112.511\n",
      "l2 norm: 901.5729833386216\n",
      "l1 norm: 756.1562246685447\n",
      "Rbeta: 901.6951103975855\n",
      "\n",
      "Train set: Avg. loss: 0.000034839, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.26647 112.52628\n",
      "l2 norm: 901.5434145285952\n",
      "l1 norm: 756.1320252329929\n",
      "Rbeta: 901.6659046902952\n",
      "\n",
      "Train set: Avg. loss: 0.000034802, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.27497 112.54154\n",
      "l2 norm: 901.5103137577362\n",
      "l1 norm: 756.10485847553\n",
      "Rbeta: 901.6331499461626\n",
      "\n",
      "Train set: Avg. loss: 0.000034766, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.28348 112.556786\n",
      "l2 norm: 901.4724972870761\n",
      "l1 norm: 756.0737477973319\n",
      "Rbeta: 901.59570885592\n",
      "\n",
      "Train set: Avg. loss: 0.000034729, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.29198 112.572044\n",
      "l2 norm: 901.4360646483736\n",
      "l1 norm: 756.0437680330149\n",
      "Rbeta: 901.5595816639683\n",
      "\n",
      "Train set: Avg. loss: 0.000034693, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.30048 112.587265\n",
      "l2 norm: 901.4011598658453\n",
      "l1 norm: 756.0150496771254\n",
      "Rbeta: 901.5251571093336\n",
      "\n",
      "Train set: Avg. loss: 0.000034657, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.30898 112.602425\n",
      "l2 norm: 901.3684716189857\n",
      "l1 norm: 755.98816009138\n",
      "Rbeta: 901.4927171893364\n",
      "\n",
      "Train set: Avg. loss: 0.000034621, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.31749 112.61759\n",
      "l2 norm: 901.3362434529686\n",
      "l1 norm: 755.9615585779394\n",
      "Rbeta: 901.4609015291921\n",
      "\n",
      "Train set: Avg. loss: 0.000034585, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.32597 112.632744\n",
      "l2 norm: 901.3110122646717\n",
      "l1 norm: 755.9407836537545\n",
      "Rbeta: 901.4359933953342\n",
      "\n",
      "Train set: Avg. loss: 0.000034549, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.334305 112.6479\n",
      "l2 norm: 901.2912573172847\n",
      "l1 norm: 755.9246139866045\n",
      "Rbeta: 901.4166506718431\n",
      "\n",
      "Train set: Avg. loss: 0.000034514, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.34263 112.663055\n",
      "l2 norm: 901.2713477304668\n",
      "l1 norm: 755.9082885557095\n",
      "Rbeta: 901.3970798908006\n",
      "\n",
      "Train set: Avg. loss: 0.000034478, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.35095 112.67821\n",
      "l2 norm: 901.2501527937927\n",
      "l1 norm: 755.8908746294703\n",
      "Rbeta: 901.3763218890754\n",
      "\n",
      "Train set: Avg. loss: 0.000034442, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.359276 112.693344\n",
      "l2 norm: 901.2265396208564\n",
      "l1 norm: 755.871442611425\n",
      "Rbeta: 901.3530464800792\n",
      "\n",
      "Train set: Avg. loss: 0.000034407, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.3676 112.708496\n",
      "l2 norm: 901.2029534551184\n",
      "l1 norm: 755.85199637352\n",
      "Rbeta: 901.3298465378576\n",
      "\n",
      "Train set: Avg. loss: 0.000034371, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.37592 112.723656\n",
      "l2 norm: 901.1737322019466\n",
      "l1 norm: 755.8278313076421\n",
      "Rbeta: 901.3009844567105\n",
      "\n",
      "Train set: Avg. loss: 0.000034336, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.384254 112.73882\n",
      "l2 norm: 901.143803489565\n",
      "l1 norm: 755.8031159666068\n",
      "Rbeta: 901.2714293418863\n",
      "\n",
      "Train set: Avg. loss: 0.000034300, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.39258 112.75397\n",
      "l2 norm: 901.1145805289792\n",
      "l1 norm: 755.7790004352416\n",
      "Rbeta: 901.2425758582493\n",
      "\n",
      "Train set: Avg. loss: 0.000034265, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.40091 112.76911\n",
      "l2 norm: 901.0869842983004\n",
      "l1 norm: 755.7562348349466\n",
      "Rbeta: 901.215364543792\n",
      "\n",
      "Train set: Avg. loss: 0.000034229, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.40923 112.78424\n",
      "l2 norm: 901.0577876031799\n",
      "l1 norm: 755.7321697038794\n",
      "Rbeta: 901.1865811296561\n",
      "\n",
      "Train set: Avg. loss: 0.000034194, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.417564 112.79937\n",
      "l2 norm: 901.0296892811145\n",
      "l1 norm: 755.7090504980105\n",
      "Rbeta: 901.1588873284145\n",
      "\n",
      "Train set: Avg. loss: 0.000034159, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.425896 112.8145\n",
      "l2 norm: 901.0015419777153\n",
      "l1 norm: 755.6858620738042\n",
      "Rbeta: 901.1310921798572\n",
      "\n",
      "Train set: Avg. loss: 0.000034124, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.43422 112.8296\n",
      "l2 norm: 900.9766402669377\n",
      "l1 norm: 755.6654013378482\n",
      "Rbeta: 901.1065634658463\n",
      "\n",
      "Train set: Avg. loss: 0.000034089, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.44255 112.84471\n",
      "l2 norm: 900.9543595132636\n",
      "l1 norm: 755.647165567033\n",
      "Rbeta: 901.08465388196\n",
      "\n",
      "Train set: Avg. loss: 0.000034053, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.45088 112.859795\n",
      "l2 norm: 900.9329339392673\n",
      "l1 norm: 755.6296204277025\n",
      "Rbeta: 901.0636058763367\n",
      "\n",
      "Train set: Avg. loss: 0.000034018, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.45921 112.87482\n",
      "l2 norm: 900.9130990633815\n",
      "l1 norm: 755.6134170585087\n",
      "Rbeta: 901.0441734103941\n",
      "\n",
      "Train set: Avg. loss: 0.000033984, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.467545 112.88981\n",
      "l2 norm: 900.8943292104243\n",
      "l1 norm: 755.5981230292693\n",
      "Rbeta: 901.0258191239934\n",
      "\n",
      "Train set: Avg. loss: 0.000033949, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.475876 112.904816\n",
      "l2 norm: 900.8764712512768\n",
      "l1 norm: 755.5836188252495\n",
      "Rbeta: 901.0083397989139\n",
      "\n",
      "Train set: Avg. loss: 0.000033914, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.48421 112.919785\n",
      "l2 norm: 900.8617887076995\n",
      "l1 norm: 755.571806673727\n",
      "Rbeta: 900.9940045932026\n",
      "\n",
      "Train set: Avg. loss: 0.000033879, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.49254 112.93477\n",
      "l2 norm: 900.8469157029504\n",
      "l1 norm: 755.5598522073808\n",
      "Rbeta: 900.9795715066078\n",
      "\n",
      "Train set: Avg. loss: 0.000033844, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.50087 112.94972\n",
      "l2 norm: 900.8276334097603\n",
      "l1 norm: 755.5442423628967\n",
      "Rbeta: 900.9606682956905\n",
      "\n",
      "Train set: Avg. loss: 0.000033810, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.50921 112.96466\n",
      "l2 norm: 900.806643570852\n",
      "l1 norm: 755.5272314744414\n",
      "Rbeta: 900.9400844640618\n",
      "\n",
      "Train set: Avg. loss: 0.000033775, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.51754 112.9796\n",
      "l2 norm: 900.7842162777717\n",
      "l1 norm: 755.5090235738428\n",
      "Rbeta: 900.9179689922893\n",
      "\n",
      "Train set: Avg. loss: 0.000033741, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.52587 112.99452\n",
      "l2 norm: 900.7559282517241\n",
      "l1 norm: 755.4858620804193\n",
      "Rbeta: 900.890043816546\n",
      "\n",
      "Train set: Avg. loss: 0.000033706, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.5342 113.00943\n",
      "l2 norm: 900.7238697151804\n",
      "l1 norm: 755.4595010124171\n",
      "Rbeta: 900.8584310859461\n",
      "\n",
      "Train set: Avg. loss: 0.000033672, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.54254 113.02432\n",
      "l2 norm: 900.698362642931\n",
      "l1 norm: 755.4386538426495\n",
      "Rbeta: 900.8332776753713\n",
      "\n",
      "Train set: Avg. loss: 0.000033637, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.55088 113.03921\n",
      "l2 norm: 900.6768140022572\n",
      "l1 norm: 755.4211425568647\n",
      "Rbeta: 900.8121507926763\n",
      "\n",
      "Train set: Avg. loss: 0.000033603, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.55921 113.05409\n",
      "l2 norm: 900.6578847505159\n",
      "l1 norm: 755.4058620966885\n",
      "Rbeta: 900.7935821050675\n",
      "\n",
      "Train set: Avg. loss: 0.000033569, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.56755 113.06894\n",
      "l2 norm: 900.6366179011397\n",
      "l1 norm: 755.388626785593\n",
      "Rbeta: 900.772714877831\n",
      "\n",
      "Train set: Avg. loss: 0.000033535, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.57588 113.08375\n",
      "l2 norm: 900.616327580766\n",
      "l1 norm: 755.372162499158\n",
      "Rbeta: 900.7528154788494\n",
      "\n",
      "Train set: Avg. loss: 0.000033501, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.58423 113.09853\n",
      "l2 norm: 900.6018053909781\n",
      "l1 norm: 755.3604873761175\n",
      "Rbeta: 900.7387228100274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000033467, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.59256 113.11334\n",
      "l2 norm: 900.5848157906321\n",
      "l1 norm: 755.3467330301859\n",
      "Rbeta: 900.7220821651841\n",
      "\n",
      "Train set: Avg. loss: 0.000033433, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.6009 113.12813\n",
      "l2 norm: 900.5612790053325\n",
      "l1 norm: 755.3274737582412\n",
      "Rbeta: 900.6989325033159\n",
      "\n",
      "Train set: Avg. loss: 0.000033399, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.60924 113.14289\n",
      "l2 norm: 900.5341570770809\n",
      "l1 norm: 755.3052398993275\n",
      "Rbeta: 900.6721923755002\n",
      "\n",
      "Train set: Avg. loss: 0.000033365, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.61758 113.15762\n",
      "l2 norm: 900.5069903756458\n",
      "l1 norm: 755.2829343378944\n",
      "Rbeta: 900.6453902606236\n",
      "\n",
      "Train set: Avg. loss: 0.000033331, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.625916 113.17234\n",
      "l2 norm: 900.4810705219792\n",
      "l1 norm: 755.2616985441117\n",
      "Rbeta: 900.6198281515863\n",
      "\n",
      "Train set: Avg. loss: 0.000033297, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.63425 113.18703\n",
      "l2 norm: 900.4577558682918\n",
      "l1 norm: 755.2426653160321\n",
      "Rbeta: 900.59692312807\n",
      "\n",
      "Train set: Avg. loss: 0.000033263, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.642586 113.2017\n",
      "l2 norm: 900.4338607654595\n",
      "l1 norm: 755.2231731169699\n",
      "Rbeta: 900.5734200158089\n",
      "\n",
      "Train set: Avg. loss: 0.000033231, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.64986 113.216354\n",
      "l2 norm: 900.4138367598431\n",
      "l1 norm: 755.206888813446\n",
      "Rbeta: 900.5538416110561\n",
      "\n",
      "Train set: Avg. loss: 0.000033201, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.656334 113.231026\n",
      "l2 norm: 900.393849088413\n",
      "l1 norm: 755.1906071106837\n",
      "Rbeta: 900.5343767640163\n",
      "\n",
      "Train set: Avg. loss: 0.000033170, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.66282 113.24567\n",
      "l2 norm: 900.3699419879929\n",
      "l1 norm: 755.1709908969761\n",
      "Rbeta: 900.5109443089653\n",
      "\n",
      "Train set: Avg. loss: 0.000033139, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.669304 113.26029\n",
      "l2 norm: 900.3357386889182\n",
      "l1 norm: 755.1427137369283\n",
      "Rbeta: 900.4772736287564\n",
      "\n",
      "Train set: Avg. loss: 0.000033108, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.67578 113.27494\n",
      "l2 norm: 900.3000098028621\n",
      "l1 norm: 755.1131232968429\n",
      "Rbeta: 900.4420403184859\n",
      "\n",
      "Train set: Avg. loss: 0.000033077, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.68227 113.28955\n",
      "l2 norm: 900.2706158603503\n",
      "l1 norm: 755.0888612358431\n",
      "Rbeta: 900.4132024146314\n",
      "\n",
      "Train set: Avg. loss: 0.000033047, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.68875 113.30411\n",
      "l2 norm: 900.236683122269\n",
      "l1 norm: 755.0607521274462\n",
      "Rbeta: 900.3797373611206\n",
      "\n",
      "Train set: Avg. loss: 0.000033016, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.69524 113.31865\n",
      "l2 norm: 900.2054513507097\n",
      "l1 norm: 755.0349349076034\n",
      "Rbeta: 900.349036021521\n",
      "\n",
      "Train set: Avg. loss: 0.000032986, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.70172 113.33315\n",
      "l2 norm: 900.1776174056384\n",
      "l1 norm: 755.0119694880993\n",
      "Rbeta: 900.3217394698974\n",
      "\n",
      "Train set: Avg. loss: 0.000032955, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.708206 113.34766\n",
      "l2 norm: 900.1501946324166\n",
      "l1 norm: 754.9893782236204\n",
      "Rbeta: 900.2948213017968\n",
      "\n",
      "Train set: Avg. loss: 0.000032925, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.71468 113.36211\n",
      "l2 norm: 900.1176355196682\n",
      "l1 norm: 754.9624952210245\n",
      "Rbeta: 900.2626827407155\n",
      "\n",
      "Train set: Avg. loss: 0.000032895, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.721176 113.37651\n",
      "l2 norm: 900.088647076984\n",
      "l1 norm: 754.938685525922\n",
      "Rbeta: 900.2342799969214\n",
      "\n",
      "Train set: Avg. loss: 0.000032865, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.72766 113.39089\n",
      "l2 norm: 900.0594077324017\n",
      "l1 norm: 754.9146536494097\n",
      "Rbeta: 900.2055447541524\n",
      "\n",
      "Train set: Avg. loss: 0.000032834, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.73414 113.40524\n",
      "l2 norm: 900.0252111382758\n",
      "l1 norm: 754.886479813408\n",
      "Rbeta: 900.1717544910363\n",
      "\n",
      "Train set: Avg. loss: 0.000032804, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.74063 113.41962\n",
      "l2 norm: 899.9930318543908\n",
      "l1 norm: 754.8600399083682\n",
      "Rbeta: 900.1401669715713\n",
      "\n",
      "Train set: Avg. loss: 0.000032774, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.747116 113.43398\n",
      "l2 norm: 899.9631223679938\n",
      "l1 norm: 754.8355286375495\n",
      "Rbeta: 900.1107364680312\n",
      "\n",
      "Train set: Avg. loss: 0.000032744, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.7536 113.4483\n",
      "l2 norm: 899.936612173257\n",
      "l1 norm: 754.8138710607348\n",
      "Rbeta: 900.0847600163759\n",
      "\n",
      "Train set: Avg. loss: 0.000032714, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.760086 113.4626\n",
      "l2 norm: 899.9108624805868\n",
      "l1 norm: 754.7928262002058\n",
      "Rbeta: 900.0594780332849\n",
      "\n",
      "Train set: Avg. loss: 0.000032684, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.76658 113.47688\n",
      "l2 norm: 899.8886234960441\n",
      "l1 norm: 754.7746876156798\n",
      "Rbeta: 900.0378249068519\n",
      "\n",
      "Train set: Avg. loss: 0.000032655, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.77306 113.49114\n",
      "l2 norm: 899.8673940444066\n",
      "l1 norm: 754.7573614554597\n",
      "Rbeta: 900.0170735853557\n",
      "\n",
      "Train set: Avg. loss: 0.000032625, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.77956 113.505356\n",
      "l2 norm: 899.8462458496398\n",
      "l1 norm: 754.740117236255\n",
      "Rbeta: 899.9964202905619\n",
      "\n",
      "Train set: Avg. loss: 0.000032595, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.78604 113.519554\n",
      "l2 norm: 899.8285990175763\n",
      "l1 norm: 754.7258021658963\n",
      "Rbeta: 899.9792774791283\n",
      "\n",
      "Train set: Avg. loss: 0.000032565, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.792534 113.533775\n",
      "l2 norm: 899.8117357000046\n",
      "l1 norm: 754.7121555921756\n",
      "Rbeta: 899.9629565710941\n",
      "\n",
      "Train set: Avg. loss: 0.000032536, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.79902 113.547966\n",
      "l2 norm: 899.7917016728302\n",
      "l1 norm: 754.6958293283724\n",
      "Rbeta: 899.9433823079979\n",
      "\n",
      "Train set: Avg. loss: 0.000032506, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.80551 113.562195\n",
      "l2 norm: 899.7701335331765\n",
      "l1 norm: 754.6782314017895\n",
      "Rbeta: 899.9223761387145\n",
      "\n",
      "Train set: Avg. loss: 0.000032477, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.812 113.5764\n",
      "l2 norm: 899.7494966553503\n",
      "l1 norm: 754.6614186499824\n",
      "Rbeta: 899.9022242583765\n",
      "\n",
      "Train set: Avg. loss: 0.000032447, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.8185 113.59058\n",
      "l2 norm: 899.7311025663762\n",
      "l1 norm: 754.6464709673364\n",
      "Rbeta: 899.8844535484884\n",
      "\n",
      "Train set: Avg. loss: 0.000032418, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.82498 113.604744\n",
      "l2 norm: 899.7137775298081\n",
      "l1 norm: 754.6323829938\n",
      "Rbeta: 899.8675211161777\n",
      "\n",
      "Train set: Avg. loss: 0.000032388, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.831474 113.61891\n",
      "l2 norm: 899.6936025879472\n",
      "l1 norm: 754.6158518393631\n",
      "Rbeta: 899.8478534440159\n",
      "\n",
      "Train set: Avg. loss: 0.000032359, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.83796 113.63314\n",
      "l2 norm: 899.6738543797162\n",
      "l1 norm: 754.5996970616843\n",
      "Rbeta: 899.8286519944996\n",
      "\n",
      "Train set: Avg. loss: 0.000032329, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.84445 113.64738\n",
      "l2 norm: 899.6587913852259\n",
      "l1 norm: 754.5874535656322\n",
      "Rbeta: 899.8140975141829\n",
      "\n",
      "Train set: Avg. loss: 0.000032300, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.850945 113.661606\n",
      "l2 norm: 899.6390834483939\n",
      "l1 norm: 754.5713570542689\n",
      "Rbeta: 899.7949041943475\n",
      "\n",
      "Train set: Avg. loss: 0.000032270, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.85744 113.67584\n",
      "l2 norm: 899.6154473376445\n",
      "l1 norm: 754.5519946504811\n",
      "Rbeta: 899.7718369326052\n",
      "\n",
      "Train set: Avg. loss: 0.000032241, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.86392 113.69006\n",
      "l2 norm: 899.5914599602368\n",
      "l1 norm: 754.5323478426359\n",
      "Rbeta: 899.7483285410581\n",
      "\n",
      "Train set: Avg. loss: 0.000032212, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.87042 113.70426\n",
      "l2 norm: 899.5686044279432\n",
      "l1 norm: 754.5136784805438\n",
      "Rbeta: 899.7260052507754\n",
      "\n",
      "Train set: Avg. loss: 0.000032182, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.876915 113.71843\n",
      "l2 norm: 899.5529767118203\n",
      "l1 norm: 754.5010660560673\n",
      "Rbeta: 899.7109010678485\n",
      "\n",
      "Train set: Avg. loss: 0.000032153, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.88341 113.732574\n",
      "l2 norm: 899.536764700092\n",
      "l1 norm: 754.4879416424117\n",
      "Rbeta: 899.6952533336403\n",
      "\n",
      "Train set: Avg. loss: 0.000032124, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.8899 113.746704\n",
      "l2 norm: 899.5167544536158\n",
      "l1 norm: 754.4716369625343\n",
      "Rbeta: 899.6757109824954\n",
      "\n",
      "Train set: Avg. loss: 0.000032095, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.8964 113.76084\n",
      "l2 norm: 899.502235171959\n",
      "l1 norm: 754.4599334618163\n",
      "Rbeta: 899.661796225485\n",
      "\n",
      "Train set: Avg. loss: 0.000032066, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.90289 113.77494\n",
      "l2 norm: 899.4867707469269\n",
      "l1 norm: 754.4474241739467\n",
      "Rbeta: 899.6467762761388\n",
      "\n",
      "Train set: Avg. loss: 0.000032037, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.909386 113.78901\n",
      "l2 norm: 899.4665640534683\n",
      "l1 norm: 754.4309281165524\n",
      "Rbeta: 899.6271696284202\n",
      "\n",
      "Train set: Avg. loss: 0.000032009, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.914764 113.80302\n",
      "l2 norm: 899.4451080582239\n",
      "l1 norm: 754.4133553362254\n",
      "Rbeta: 899.606285300769\n",
      "\n",
      "Train set: Avg. loss: 0.000031983, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.9194 113.81703\n",
      "l2 norm: 899.4268982984904\n",
      "l1 norm: 754.398516593997\n",
      "Rbeta: 899.5887741524823\n",
      "\n",
      "Train set: Avg. loss: 0.000031957, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.924034 113.83104\n",
      "l2 norm: 899.4041855590086\n",
      "l1 norm: 754.3799068575261\n",
      "Rbeta: 899.5666709640253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000031931, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.92867 113.845024\n",
      "l2 norm: 899.3810656150571\n",
      "l1 norm: 754.3609552968785\n",
      "Rbeta: 899.5442492406058\n",
      "\n",
      "Train set: Avg. loss: 0.000031904, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.93331 113.859\n",
      "l2 norm: 899.3562468858485\n",
      "l1 norm: 754.3405603297306\n",
      "Rbeta: 899.5201008716604\n",
      "\n",
      "Train set: Avg. loss: 0.000031878, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.93794 113.872986\n",
      "l2 norm: 899.3314038512187\n",
      "l1 norm: 754.3201430208204\n",
      "Rbeta: 899.4959039472317\n",
      "\n",
      "Train set: Avg. loss: 0.000031852, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.94258 113.88696\n",
      "l2 norm: 899.3019505478691\n",
      "l1 norm: 754.2959041631399\n",
      "Rbeta: 899.4671341525194\n",
      "\n",
      "Train set: Avg. loss: 0.000031826, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.94721 113.90093\n",
      "l2 norm: 899.2711057193253\n",
      "l1 norm: 754.2705025593806\n",
      "Rbeta: 899.4369633504843\n",
      "\n",
      "Train set: Avg. loss: 0.000031800, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.95185 113.91486\n",
      "l2 norm: 899.2373533657819\n",
      "l1 norm: 754.2426790966792\n",
      "Rbeta: 899.4038770634855\n",
      "\n",
      "Train set: Avg. loss: 0.000031774, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.95649 113.928764\n",
      "l2 norm: 899.206882542518\n",
      "l1 norm: 754.2175962598508\n",
      "Rbeta: 899.374067781483\n",
      "\n",
      "Train set: Avg. loss: 0.000031748, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.96113 113.94267\n",
      "l2 norm: 899.1768555012761\n",
      "l1 norm: 754.1928799068335\n",
      "Rbeta: 899.3447573272159\n",
      "\n",
      "Train set: Avg. loss: 0.000031722, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.96577 113.95655\n",
      "l2 norm: 899.1451784030359\n",
      "l1 norm: 754.166829248888\n",
      "Rbeta: 899.3137090056016\n",
      "\n",
      "Train set: Avg. loss: 0.000031696, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.9704 113.970436\n",
      "l2 norm: 899.1141480181844\n",
      "l1 norm: 754.1413644263598\n",
      "Rbeta: 899.2833137041899\n",
      "\n",
      "Train set: Avg. loss: 0.000031670, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.97504 113.9843\n",
      "l2 norm: 899.0842835129699\n",
      "l1 norm: 754.1168715749825\n",
      "Rbeta: 899.2541091706271\n",
      "\n",
      "Train set: Avg. loss: 0.000031645, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.97968 113.99817\n",
      "l2 norm: 899.0534144449495\n",
      "l1 norm: 754.0915520974971\n",
      "Rbeta: 899.2239968254445\n",
      "\n",
      "Train set: Avg. loss: 0.000031619, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.98432 114.01197\n",
      "l2 norm: 899.0237726033171\n",
      "l1 norm: 754.0672772274129\n",
      "Rbeta: 899.1949911609673\n",
      "\n",
      "Train set: Avg. loss: 0.000031593, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.98896 114.025764\n",
      "l2 norm: 898.9919882074572\n",
      "l1 norm: 754.0412135148279\n",
      "Rbeta: 899.1638872508931\n",
      "\n",
      "Train set: Avg. loss: 0.000031568, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.9936 114.03953\n",
      "l2 norm: 898.9532120314705\n",
      "l1 norm: 754.0092588518714\n",
      "Rbeta: 899.1258391511409\n",
      "\n",
      "Train set: Avg. loss: 0.000031542, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.998245 114.05327\n",
      "l2 norm: 898.9154018846092\n",
      "l1 norm: 753.9780509715462\n",
      "Rbeta: 899.0886532460514\n",
      "\n",
      "Train set: Avg. loss: 0.000031516, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.002884 114.067055\n",
      "l2 norm: 898.8734032074249\n",
      "l1 norm: 753.9433327722217\n",
      "Rbeta: 899.0473599404983\n",
      "\n",
      "Train set: Avg. loss: 0.000031491, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.00751 114.08085\n",
      "l2 norm: 898.8334279677234\n",
      "l1 norm: 753.91032287644\n",
      "Rbeta: 899.0079970463321\n",
      "\n",
      "Train set: Avg. loss: 0.000031465, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.012146 114.094635\n",
      "l2 norm: 898.7974793842625\n",
      "l1 norm: 753.8806989971881\n",
      "Rbeta: 898.9727428595885\n",
      "\n",
      "Train set: Avg. loss: 0.000031440, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.016785 114.10844\n",
      "l2 norm: 898.7610136015097\n",
      "l1 norm: 753.8506238893718\n",
      "Rbeta: 898.936959971312\n",
      "\n",
      "Train set: Avg. loss: 0.000031414, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.02143 114.12224\n",
      "l2 norm: 898.7228553641202\n",
      "l1 norm: 753.8191311813434\n",
      "Rbeta: 898.8995355216157\n",
      "\n",
      "Train set: Avg. loss: 0.000031389, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.02607 114.13604\n",
      "l2 norm: 898.6835176649683\n",
      "l1 norm: 753.7866855125228\n",
      "Rbeta: 898.8608701009045\n",
      "\n",
      "Train set: Avg. loss: 0.000031363, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.03067 114.14984\n",
      "l2 norm: 898.6410305525927\n",
      "l1 norm: 753.7516054748248\n",
      "Rbeta: 898.8190848492535\n",
      "\n",
      "Train set: Avg. loss: 0.000031338, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.035126 114.16363\n",
      "l2 norm: 898.6029204554006\n",
      "l1 norm: 753.7201420053213\n",
      "Rbeta: 898.7816918050587\n",
      "\n",
      "Train set: Avg. loss: 0.000031313, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.03958 114.177444\n",
      "l2 norm: 898.5678522344361\n",
      "l1 norm: 753.6912320185517\n",
      "Rbeta: 898.7473216081867\n",
      "\n",
      "Train set: Avg. loss: 0.000031288, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.04351 114.19125\n",
      "l2 norm: 898.5384148519516\n",
      "l1 norm: 753.66706771981\n",
      "Rbeta: 898.7186093451049\n",
      "\n",
      "Train set: Avg. loss: 0.000031266, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.04611 114.20505\n",
      "l2 norm: 898.5055301092478\n",
      "l1 norm: 753.6399919933685\n",
      "Rbeta: 898.686607806989\n",
      "\n",
      "Train set: Avg. loss: 0.000031243, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.048706 114.21879\n",
      "l2 norm: 898.4730690016539\n",
      "l1 norm: 753.6132408082208\n",
      "Rbeta: 898.6549052385096\n",
      "\n",
      "Train set: Avg. loss: 0.000031221, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.05131 114.23253\n",
      "l2 norm: 898.4380071344502\n",
      "l1 norm: 753.5842953120698\n",
      "Rbeta: 898.6208102065001\n",
      "\n",
      "Train set: Avg. loss: 0.000031198, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.0539 114.24627\n",
      "l2 norm: 898.4084152172063\n",
      "l1 norm: 753.5599382044318\n",
      "Rbeta: 898.5920720605844\n",
      "\n",
      "Train set: Avg. loss: 0.000031176, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.0565 114.25997\n",
      "l2 norm: 898.3797118044962\n",
      "l1 norm: 753.536313796078\n",
      "Rbeta: 898.5641831376219\n",
      "\n",
      "Train set: Avg. loss: 0.000031153, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.0591 114.2737\n",
      "l2 norm: 898.3516754742858\n",
      "l1 norm: 753.5132571817222\n",
      "Rbeta: 898.5370657785577\n",
      "\n",
      "Train set: Avg. loss: 0.000031131, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.0617 114.2874\n",
      "l2 norm: 898.3237763763582\n",
      "l1 norm: 753.4903133907002\n",
      "Rbeta: 898.5099926464014\n",
      "\n",
      "Train set: Avg. loss: 0.000031109, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.06429 114.30109\n",
      "l2 norm: 898.2946076607792\n",
      "l1 norm: 753.4662583290535\n",
      "Rbeta: 898.4817455561665\n",
      "\n",
      "Train set: Avg. loss: 0.000031086, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.066895 114.31471\n",
      "l2 norm: 898.2682357107782\n",
      "l1 norm: 753.444556712617\n",
      "Rbeta: 898.4562008714396\n",
      "\n",
      "Train set: Avg. loss: 0.000031064, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.06949 114.32832\n",
      "l2 norm: 898.2417755799593\n",
      "l1 norm: 753.4227920858618\n",
      "Rbeta: 898.4306002395246\n",
      "\n",
      "Train set: Avg. loss: 0.000031042, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.07209 114.34192\n",
      "l2 norm: 898.2114233003708\n",
      "l1 norm: 753.3977873460549\n",
      "Rbeta: 898.4010849413797\n",
      "\n",
      "Train set: Avg. loss: 0.000031020, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.07469 114.3555\n",
      "l2 norm: 898.181637454511\n",
      "l1 norm: 753.3732596812816\n",
      "Rbeta: 898.3721652859869\n",
      "\n",
      "Train set: Avg. loss: 0.000030998, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.077286 114.3691\n",
      "l2 norm: 898.1513263985697\n",
      "l1 norm: 753.348302035082\n",
      "Rbeta: 898.3427158773792\n",
      "\n",
      "Train set: Avg. loss: 0.000030976, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.07989 114.38269\n",
      "l2 norm: 898.1254542427728\n",
      "l1 norm: 753.3270441825609\n",
      "Rbeta: 898.3177801502175\n",
      "\n",
      "Train set: Avg. loss: 0.000030954, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.08249 114.39628\n",
      "l2 norm: 898.1017136192138\n",
      "l1 norm: 753.3075977838851\n",
      "Rbeta: 898.2948698882453\n",
      "\n",
      "Train set: Avg. loss: 0.000030932, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.08509 114.40984\n",
      "l2 norm: 898.0763572230572\n",
      "l1 norm: 753.2868061530271\n",
      "Rbeta: 898.2704108989698\n",
      "\n",
      "Train set: Avg. loss: 0.000030910, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.08769 114.42341\n",
      "l2 norm: 898.050422327605\n",
      "l1 norm: 753.2655303542172\n",
      "Rbeta: 898.2454212493521\n",
      "\n",
      "Train set: Avg. loss: 0.000030888, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.09029 114.43695\n",
      "l2 norm: 898.0251354840489\n",
      "l1 norm: 753.2448097538162\n",
      "Rbeta: 898.2209495440987\n",
      "\n",
      "Train set: Avg. loss: 0.000030866, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.09289 114.450485\n",
      "l2 norm: 898.0016138856294\n",
      "l1 norm: 753.2255679275765\n",
      "Rbeta: 898.1983029849878\n",
      "\n",
      "Train set: Avg. loss: 0.000030844, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.09549 114.46402\n",
      "l2 norm: 897.9781075356874\n",
      "l1 norm: 753.2063555146058\n",
      "Rbeta: 898.1757218515733\n",
      "\n",
      "Train set: Avg. loss: 0.000030822, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.09809 114.47759\n",
      "l2 norm: 897.9512438705211\n",
      "l1 norm: 753.1843431172675\n",
      "Rbeta: 898.1497345005749\n",
      "\n",
      "Train set: Avg. loss: 0.000030800, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.10069 114.491165\n",
      "l2 norm: 897.9240365443319\n",
      "l1 norm: 753.1620510345506\n",
      "Rbeta: 898.123431289207\n",
      "\n",
      "Train set: Avg. loss: 0.000030778, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.103294 114.50471\n",
      "l2 norm: 897.8962779510803\n",
      "l1 norm: 753.139317653541\n",
      "Rbeta: 898.0965832954091\n",
      "\n",
      "Train set: Avg. loss: 0.000030756, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.105896 114.51821\n",
      "l2 norm: 897.8683687728902\n",
      "l1 norm: 753.1164585428608\n",
      "Rbeta: 898.0696187531985\n",
      "\n",
      "Train set: Avg. loss: 0.000030735, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.1085 114.53169\n",
      "l2 norm: 897.8448363791208\n",
      "l1 norm: 753.0973130085423\n",
      "Rbeta: 898.0469244435633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000030713, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.1111 114.54521\n",
      "l2 norm: 897.8192636546333\n",
      "l1 norm: 753.0764292925957\n",
      "Rbeta: 898.0222168984665\n",
      "\n",
      "Train set: Avg. loss: 0.000030691, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.11371 114.55878\n",
      "l2 norm: 897.7927124250315\n",
      "l1 norm: 753.0546504956028\n",
      "Rbeta: 897.9966885381968\n",
      "\n",
      "Train set: Avg. loss: 0.000030669, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.11631 114.572296\n",
      "l2 norm: 897.7658184685951\n",
      "l1 norm: 753.0325770668205\n",
      "Rbeta: 897.9706005171554\n",
      "\n",
      "Train set: Avg. loss: 0.000030647, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.11891 114.58583\n",
      "l2 norm: 897.7382521387516\n",
      "l1 norm: 753.0099101870823\n",
      "Rbeta: 897.9439980697367\n",
      "\n",
      "Train set: Avg. loss: 0.000030626, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.12151 114.599365\n",
      "l2 norm: 897.7130502795246\n",
      "l1 norm: 752.989123578602\n",
      "Rbeta: 897.9197533775078\n",
      "\n",
      "Train set: Avg. loss: 0.000030604, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.124115 114.612915\n",
      "l2 norm: 897.6896282183385\n",
      "l1 norm: 752.9698692822224\n",
      "Rbeta: 897.8972240750826\n",
      "\n",
      "Train set: Avg. loss: 0.000030582, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.126724 114.626465\n",
      "l2 norm: 897.6729735199159\n",
      "l1 norm: 752.9563205355357\n",
      "Rbeta: 897.8814948178851\n",
      "\n",
      "Train set: Avg. loss: 0.000030560, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.129326 114.64001\n",
      "l2 norm: 897.6537163803032\n",
      "l1 norm: 752.9405729399468\n",
      "Rbeta: 897.8631420750976\n",
      "\n",
      "Train set: Avg. loss: 0.000030539, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.13193 114.65355\n",
      "l2 norm: 897.6306710048725\n",
      "l1 norm: 752.9216336980414\n",
      "Rbeta: 897.8410520378985\n",
      "\n",
      "Train set: Avg. loss: 0.000030517, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.13454 114.66705\n",
      "l2 norm: 897.6069091484958\n",
      "l1 norm: 752.9021379260649\n",
      "Rbeta: 897.8182031994465\n",
      "\n",
      "Train set: Avg. loss: 0.000030496, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.13714 114.680534\n",
      "l2 norm: 897.5832379918472\n",
      "l1 norm: 752.8826994932435\n",
      "Rbeta: 897.795450628821\n",
      "\n",
      "Train set: Avg. loss: 0.000030474, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.13974 114.694\n",
      "l2 norm: 897.5570571764362\n",
      "l1 norm: 752.8611484700176\n",
      "Rbeta: 897.7701842946325\n",
      "\n",
      "Train set: Avg. loss: 0.000030452, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.14235 114.70746\n",
      "l2 norm: 897.5325872650847\n",
      "l1 norm: 752.8410612187733\n",
      "Rbeta: 897.7466321594945\n",
      "\n",
      "Train set: Avg. loss: 0.000030431, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.14496 114.720894\n",
      "l2 norm: 897.511551898461\n",
      "l1 norm: 752.823886253971\n",
      "Rbeta: 897.7264875301582\n",
      "\n",
      "Train set: Avg. loss: 0.000030410, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.14756 114.73432\n",
      "l2 norm: 897.4894214965987\n",
      "l1 norm: 752.8057940523142\n",
      "Rbeta: 897.7053854943737\n",
      "\n",
      "Train set: Avg. loss: 0.000030388, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.15016 114.74773\n",
      "l2 norm: 897.4647588183052\n",
      "l1 norm: 752.7855917200886\n",
      "Rbeta: 897.6816028062242\n",
      "\n",
      "Train set: Avg. loss: 0.000030367, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.15277 114.761116\n",
      "l2 norm: 897.43679529357\n",
      "l1 norm: 752.7626006578374\n",
      "Rbeta: 897.6545607291372\n",
      "\n",
      "Train set: Avg. loss: 0.000030345, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.15538 114.77452\n",
      "l2 norm: 897.4088643577693\n",
      "l1 norm: 752.739633102019\n",
      "Rbeta: 897.6276459124579\n",
      "\n",
      "Train set: Avg. loss: 0.000030324, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.15798 114.7879\n",
      "l2 norm: 897.3799902132952\n",
      "l1 norm: 752.7158669525782\n",
      "Rbeta: 897.5996540856495\n",
      "\n",
      "Train set: Avg. loss: 0.000030303, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.16059 114.80129\n",
      "l2 norm: 897.3502067098775\n",
      "l1 norm: 752.6913375294207\n",
      "Rbeta: 897.5708141584797\n",
      "\n",
      "Train set: Avg. loss: 0.000030282, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.1632 114.814674\n",
      "l2 norm: 897.3214902170606\n",
      "l1 norm: 752.667678508887\n",
      "Rbeta: 897.5430473213785\n",
      "\n",
      "Train set: Avg. loss: 0.000030260, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.1658 114.82809\n",
      "l2 norm: 897.2924210349663\n",
      "l1 norm: 752.6437194549412\n",
      "Rbeta: 897.514981086846\n",
      "\n",
      "Train set: Avg. loss: 0.000030239, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.16841 114.84147\n",
      "l2 norm: 897.2634285018459\n",
      "l1 norm: 752.6198424235465\n",
      "Rbeta: 897.486905565276\n",
      "After training:\n",
      "\n",
      "Train set: Avg. loss: 0.000030239, Accuracy: 512/512 (100%)\n",
      "\n",
      "tensor(49.) tensor(9.4985e+21)\n",
      "110.16841 114.84147\n",
      "l2 norm: 897.2634285018459\n",
      "l1 norm: 752.6198424235465\n",
      "Rbeta: 897.486905565276\n",
      "Time domain:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency domain:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "Before training:\n",
      "\n",
      "Train set: Avg. loss: 1.000048518, Accuracy: 228/512 (45%)\n",
      "\n",
      "0.00072914956 0.012995649\n",
      "l2 norm: 58.053532072325986\n",
      "l1 norm: 51.2257023049286\n",
      "Rbeta: -259.38339715285235\n",
      "Start training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300334a1f2f445789d74e78807a0e4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.966989934, Accuracy: 296/512 (58%)\n",
      "\n",
      "0.035415746 0.047796115\n",
      "l2 norm: 3.97187808222984\n",
      "l1 norm: 1.7019386155695861\n",
      "Rbeta: -4.502120905361696\n",
      "\n",
      "Train set: Avg. loss: 0.944907904, Accuracy: 317/512 (62%)\n",
      "\n",
      "0.090452164 0.10368604\n",
      "l2 norm: 6.458768007999148\n",
      "l1 norm: 3.570936924394961\n",
      "Rbeta: -6.801663793897234\n",
      "\n",
      "Train set: Avg. loss: 0.913627684, Accuracy: 346/512 (68%)\n",
      "\n",
      "0.20676899 0.22202069\n",
      "l2 norm: 15.834022808737474\n",
      "l1 norm: 10.132297484866964\n",
      "Rbeta: -16.20402862493677\n",
      "\n",
      "Train set: Avg. loss: 0.868691087, Accuracy: 370/512 (72%)\n",
      "\n",
      "0.4317637 0.4512568\n",
      "l2 norm: 28.44689303756783\n",
      "l1 norm: 19.536441955034405\n",
      "Rbeta: -28.766459577880628\n",
      "\n",
      "Train set: Avg. loss: 0.813787341, Accuracy: 378/512 (74%)\n",
      "\n",
      "0.7782303 0.80513054\n",
      "l2 norm: 55.69783169236105\n",
      "l1 norm: 40.15527173938792\n",
      "Rbeta: -56.04644165993278\n",
      "Learning rate change\n",
      "\n",
      "Train set: Avg. loss: 0.309514374, Accuracy: 499/512 (97%)\n",
      "\n",
      "8.370671 10.656515\n",
      "l2 norm: 833.8258100860053\n",
      "l1 norm: 682.4094748199216\n",
      "Rbeta: -840.2933231550824\n",
      "\n",
      "Train set: Avg. loss: 0.116307005, Accuracy: 511/512 (100%)\n",
      "\n",
      "18.350906 21.944897\n",
      "l2 norm: 132488.55524930998\n",
      "l1 norm: 109603.91875322028\n",
      "Rbeta: -133048.24442804427\n",
      "\n",
      "Train set: Avg. loss: 0.057922073, Accuracy: 512/512 (100%)\n",
      "\n",
      "25.554789 29.52951\n",
      "l2 norm: 1928.3151271311528\n",
      "l1 norm: 1599.7295294047424\n",
      "Rbeta: 1933.6739468879018\n",
      "\n",
      "Train set: Avg. loss: 0.035194833, Accuracy: 512/512 (100%)\n",
      "\n",
      "30.715473 34.836994\n",
      "l2 norm: 1435.0343190154936\n",
      "l1 norm: 1192.5815722898633\n",
      "Rbeta: 1438.0802946802507\n",
      "\n",
      "Train set: Avg. loss: 0.024432734, Accuracy: 512/512 (100%)\n",
      "\n",
      "34.642197 38.834663\n",
      "l2 norm: 1365.7798404288014\n",
      "l1 norm: 1136.3252220126146\n",
      "Rbeta: 1368.1802404107248\n",
      "\n",
      "Train set: Avg. loss: 0.018348763, Accuracy: 512/512 (100%)\n",
      "\n",
      "37.78354 42.016438\n",
      "l2 norm: 1316.0565570215476\n",
      "l1 norm: 1095.8155257733029\n",
      "Rbeta: 1318.06473961764\n",
      "\n",
      "Train set: Avg. loss: 0.014500310, Accuracy: 512/512 (100%)\n",
      "\n",
      "40.39006 44.64869\n",
      "l2 norm: 1277.945456293137\n",
      "l1 norm: 1064.6976004340895\n",
      "Rbeta: 1279.6906168381056\n",
      "\n",
      "Train set: Avg. loss: 0.011877215, Accuracy: 512/512 (100%)\n",
      "\n",
      "42.61165 46.887947\n",
      "l2 norm: 1247.8214166772927\n",
      "l1 norm: 1040.0669373403093\n",
      "Rbeta: 1249.378155536157\n",
      "\n",
      "Train set: Avg. loss: 0.009990717, Accuracy: 512/512 (100%)\n",
      "\n",
      "44.54411 48.8332\n",
      "l2 norm: 1223.4127723175345\n",
      "l1 norm: 1020.0914343787389\n",
      "Rbeta: 1224.8276401263831\n",
      "\n",
      "Train set: Avg. loss: 0.008577761, Accuracy: 512/512 (100%)\n",
      "\n",
      "46.25207 50.550713\n",
      "l2 norm: 1203.2065133726067\n",
      "l1 norm: 1003.5450062636775\n",
      "Rbeta: 1204.510646203617\n",
      "\n",
      "Train set: Avg. loss: 0.007485308, Accuracy: 512/512 (100%)\n",
      "\n",
      "47.780846 52.086937\n",
      "l2 norm: 1186.1664163821667\n",
      "l1 norm: 989.5851812887405\n",
      "Rbeta: 1187.3817187165319\n",
      "\n",
      "Train set: Avg. loss: 0.006618751, Accuracy: 512/512 (100%)\n",
      "\n",
      "49.16358 53.475582\n",
      "l2 norm: 1171.5691938854063\n",
      "l1 norm: 977.6227937039889\n",
      "Rbeta: 1172.7113375417932\n",
      "\n",
      "Train set: Avg. loss: 0.005916778, Accuracy: 512/512 (100%)\n",
      "\n",
      "50.42504 54.741867\n",
      "l2 norm: 1158.8929554178137\n",
      "l1 norm: 967.23207006343\n",
      "Rbeta: 1159.973746212179\n",
      "\n",
      "Train set: Avg. loss: 0.005338036, Accuracy: 512/512 (100%)\n",
      "\n",
      "51.58432 55.905132\n",
      "l2 norm: 1147.763126876418\n",
      "l1 norm: 958.1071783711768\n",
      "Rbeta: 1148.7917248083436\n",
      "\n",
      "Train set: Avg. loss: 0.004853729, Accuracy: 512/512 (100%)\n",
      "\n",
      "52.65632 56.980522\n",
      "l2 norm: 1137.8891959750274\n",
      "l1 norm: 950.0106839153987\n",
      "Rbeta: 1138.8726844216335\n",
      "\n",
      "Train set: Avg. loss: 0.004443198, Accuracy: 512/512 (100%)\n",
      "\n",
      "53.65309 57.980095\n",
      "l2 norm: 1129.0517251942385\n",
      "l1 norm: 942.7631222593022\n",
      "Rbeta: 1129.9958339228488\n",
      "\n",
      "Train set: Avg. loss: 0.004091339, Accuracy: 512/512 (100%)\n",
      "\n",
      "54.584167 58.913605\n",
      "l2 norm: 1121.0856166993258\n",
      "l1 norm: 936.2294851011065\n",
      "Rbeta: 1121.9949623917605\n",
      "\n",
      "Train set: Avg. loss: 0.003786817, Accuracy: 512/512 (100%)\n",
      "\n",
      "55.457474 59.789085\n",
      "l2 norm: 1113.856617015218\n",
      "l1 norm: 930.2999041494111\n",
      "Rbeta: 1114.7351141462345\n",
      "\n",
      "Train set: Avg. loss: 0.003520966, Accuracy: 512/512 (100%)\n",
      "\n",
      "56.279728 60.613167\n",
      "l2 norm: 1107.2562963894839\n",
      "l1 norm: 924.8855436005092\n",
      "Rbeta: 1108.107046319768\n",
      "\n",
      "Train set: Avg. loss: 0.003287108, Accuracy: 512/512 (100%)\n",
      "\n",
      "57.05638 61.391453\n",
      "l2 norm: 1101.1987818799348\n",
      "l1 norm: 919.9161175689842\n",
      "Rbeta: 1102.0244882952259\n",
      "\n",
      "Train set: Avg. loss: 0.003079973, Accuracy: 512/512 (100%)\n",
      "\n",
      "57.79218 62.128647\n",
      "l2 norm: 1095.6134694477325\n",
      "l1 norm: 915.3338303543046\n",
      "Rbeta: 1096.416480626797\n",
      "\n",
      "Train set: Avg. loss: 0.002895374, Accuracy: 512/512 (100%)\n",
      "\n",
      "58.49111 62.82881\n",
      "l2 norm: 1090.4418369467278\n",
      "l1 norm: 911.0906981499836\n",
      "Rbeta: 1091.224191018463\n",
      "\n",
      "Train set: Avg. loss: 0.002729952, Accuracy: 512/512 (100%)\n",
      "\n",
      "59.156536 63.495415\n",
      "l2 norm: 1085.6362056757346\n",
      "l1 norm: 907.1477228059589\n",
      "Rbeta: 1086.3995464884033\n",
      "\n",
      "Train set: Avg. loss: 0.002580971, Accuracy: 512/512 (100%)\n",
      "\n",
      "59.791374 64.131454\n",
      "l2 norm: 1081.1524660882092\n",
      "l1 norm: 903.4686703555187\n",
      "Rbeta: 1081.8983822235173\n",
      "\n",
      "Train set: Avg. loss: 0.002446129, Accuracy: 512/512 (100%)\n",
      "\n",
      "60.398685 64.73956\n",
      "l2 norm: 1076.9550986274012\n",
      "l1 norm: 900.0244480570119\n",
      "Rbeta: 1077.6847607161992\n",
      "\n",
      "Train set: Avg. loss: 0.002323621, Accuracy: 512/512 (100%)\n",
      "\n",
      "60.9802 65.32205\n",
      "l2 norm: 1073.0173863645527\n",
      "l1 norm: 896.793216612911\n",
      "Rbeta: 1073.732147177805\n",
      "\n",
      "Train set: Avg. loss: 0.002211847, Accuracy: 512/512 (100%)\n",
      "\n",
      "61.538292 65.88097\n",
      "l2 norm: 1069.311994002519\n",
      "l1 norm: 893.7525238366874\n",
      "Rbeta: 1070.0127140754541\n",
      "\n",
      "Train set: Avg. loss: 0.002109511, Accuracy: 512/512 (100%)\n",
      "\n",
      "62.07469 66.41809\n",
      "l2 norm: 1065.814868167038\n",
      "l1 norm: 890.882646386329\n",
      "Rbeta: 1066.5025342871784\n",
      "\n",
      "Train set: Avg. loss: 0.002015505, Accuracy: 512/512 (100%)\n",
      "\n",
      "62.59095 66.93503\n",
      "l2 norm: 1062.5093770076867\n",
      "l1 norm: 888.1699734784916\n",
      "Rbeta: 1063.1847551740618\n",
      "\n",
      "Train set: Avg. loss: 0.001928890, Accuracy: 512/512 (100%)\n",
      "\n",
      "63.088432 67.43323\n",
      "l2 norm: 1059.37917185114\n",
      "l1 norm: 885.6011307373665\n",
      "Rbeta: 1060.0433306387984\n",
      "\n",
      "Train set: Avg. loss: 0.001848855, Accuracy: 512/512 (100%)\n",
      "\n",
      "63.56843 67.913956\n",
      "l2 norm: 1056.4052554215082\n",
      "l1 norm: 883.1604738197891\n",
      "Rbeta: 1057.0586052141603\n",
      "\n",
      "Train set: Avg. loss: 0.001774677, Accuracy: 512/512 (100%)\n",
      "\n",
      "64.03238 68.3784\n",
      "l2 norm: 1053.5778134994998\n",
      "l1 norm: 880.8400023419406\n",
      "Rbeta: 1054.2208820733015\n",
      "\n",
      "Train set: Avg. loss: 0.001705766, Accuracy: 512/512 (100%)\n",
      "\n",
      "64.48119 68.82762\n",
      "l2 norm: 1050.885456446095\n",
      "l1 norm: 878.6303475974976\n",
      "Rbeta: 1051.5188642631083\n",
      "\n",
      "Train set: Avg. loss: 0.001641579, Accuracy: 512/512 (100%)\n",
      "\n",
      "64.916084 69.26254\n",
      "l2 norm: 1048.3141416637957\n",
      "l1 norm: 876.5199833483668\n",
      "Rbeta: 1048.9383694848775\n",
      "\n",
      "Train set: Avg. loss: 0.001581733, Accuracy: 512/512 (100%)\n",
      "\n",
      "65.33685 69.68402\n",
      "l2 norm: 1045.855332615861\n",
      "l1 norm: 874.5018985215789\n",
      "Rbeta: 1046.4710216916637\n",
      "\n",
      "Train set: Avg. loss: 0.001525755, Accuracy: 512/512 (100%)\n",
      "\n",
      "65.74523 70.09289\n",
      "l2 norm: 1043.5069599583103\n",
      "l1 norm: 872.5745149823407\n",
      "Rbeta: 1044.1144474015405\n",
      "\n",
      "Train set: Avg. loss: 0.001473285, Accuracy: 512/512 (100%)\n",
      "\n",
      "66.14206 70.489845\n",
      "l2 norm: 1041.2511970778482\n",
      "l1 norm: 870.7230477583239\n",
      "Rbeta: 1041.8507077922136\n",
      "\n",
      "Train set: Avg. loss: 0.001424056, Accuracy: 512/512 (100%)\n",
      "\n",
      "66.52731 70.87556\n",
      "l2 norm: 1039.0879458884815\n",
      "l1 norm: 868.9474857180167\n",
      "Rbeta: 1039.6801200041582\n",
      "\n",
      "Train set: Avg. loss: 0.001377770, Accuracy: 512/512 (100%)\n",
      "\n",
      "66.90189 71.250626\n",
      "l2 norm: 1037.0122605603653\n",
      "l1 norm: 867.243841549502\n",
      "Rbeta: 1037.5974628311887\n",
      "\n",
      "Train set: Avg. loss: 0.001334140, Accuracy: 512/512 (100%)\n",
      "\n",
      "67.26696 71.615654\n",
      "l2 norm: 1035.0144082835407\n",
      "l1 norm: 865.6040311071108\n",
      "Rbeta: 1035.5926894041204\n",
      "\n",
      "Train set: Avg. loss: 0.001293029, Accuracy: 512/512 (100%)\n",
      "\n",
      "67.62169 71.9711\n",
      "l2 norm: 1033.0925472963509\n",
      "l1 norm: 864.0266122412047\n",
      "Rbeta: 1033.6644119230452\n",
      "\n",
      "Train set: Avg. loss: 0.001254139, Accuracy: 512/512 (100%)\n",
      "\n",
      "67.96827 72.31747\n",
      "l2 norm: 1031.2362983413107\n",
      "l1 norm: 862.5029748374934\n",
      "Rbeta: 1031.8017827001433\n",
      "\n",
      "Train set: Avg. loss: 0.001217387, Accuracy: 512/512 (100%)\n",
      "\n",
      "68.30548 72.65521\n",
      "l2 norm: 1029.4501315899483\n",
      "l1 norm: 861.0368857927351\n",
      "Rbeta: 1030.0096906610338\n",
      "\n",
      "Train set: Avg. loss: 0.001182576, Accuracy: 512/512 (100%)\n",
      "\n",
      "68.63434 72.98473\n",
      "l2 norm: 1027.7217351521272\n",
      "l1 norm: 859.6181560417572\n",
      "Rbeta: 1028.2756795847383\n",
      "\n",
      "Train set: Avg. loss: 0.001149498, Accuracy: 512/512 (100%)\n",
      "\n",
      "68.956535 73.30641\n",
      "l2 norm: 1026.0552668705634\n",
      "l1 norm: 858.2502747140156\n",
      "Rbeta: 1026.6034663483767\n",
      "\n",
      "Train set: Avg. loss: 0.001118162, Accuracy: 512/512 (100%)\n",
      "\n",
      "69.269646 73.620636\n",
      "l2 norm: 1024.4417582769559\n",
      "l1 norm: 856.925888665499\n",
      "Rbeta: 1024.9849838473986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.001088308, Accuracy: 512/512 (100%)\n",
      "\n",
      "69.5768 73.92771\n",
      "l2 norm: 1022.8801456792365\n",
      "l1 norm: 855.6440168500469\n",
      "Rbeta: 1023.4181409489569\n",
      "\n",
      "Train set: Avg. loss: 0.001059883, Accuracy: 512/512 (100%)\n",
      "\n",
      "69.8773 74.22795\n",
      "l2 norm: 1021.3660138546181\n",
      "l1 norm: 854.4011078822032\n",
      "Rbeta: 1021.8989792253373\n",
      "\n",
      "Train set: Avg. loss: 0.001032836, Accuracy: 512/512 (100%)\n",
      "\n",
      "70.17039 74.52167\n",
      "l2 norm: 1019.9021831833326\n",
      "l1 norm: 853.1995929635901\n",
      "Rbeta: 1020.430417954143\n",
      "\n",
      "Train set: Avg. loss: 0.001007021, Accuracy: 512/512 (100%)\n",
      "\n",
      "70.457565 74.809105\n",
      "l2 norm: 1018.4803097429792\n",
      "l1 norm: 852.0324374983952\n",
      "Rbeta: 1019.0038797421059\n",
      "\n",
      "Train set: Avg. loss: 0.000982357, Accuracy: 512/512 (100%)\n",
      "\n",
      "70.73907 75.09055\n",
      "l2 norm: 1017.1000601439368\n",
      "l1 norm: 850.8994589362802\n",
      "Rbeta: 1017.6192032954921\n",
      "\n",
      "Train set: Avg. loss: 0.000958790, Accuracy: 512/512 (100%)\n",
      "\n",
      "71.0147 75.36621\n",
      "l2 norm: 1015.7591036724801\n",
      "l1 norm: 849.7987429087989\n",
      "Rbeta: 1016.2740367157555\n",
      "\n",
      "Train set: Avg. loss: 0.000936253, Accuracy: 512/512 (100%)\n",
      "\n",
      "71.284584 75.63635\n",
      "l2 norm: 1014.4551482978691\n",
      "l1 norm: 848.7283828823881\n",
      "Rbeta: 1014.965816423064\n",
      "\n",
      "Train set: Avg. loss: 0.000914688, Accuracy: 512/512 (100%)\n",
      "\n",
      "71.548874 75.90117\n",
      "l2 norm: 1013.1852234820504\n",
      "l1 norm: 847.6858944785135\n",
      "Rbeta: 1013.6919714594507\n",
      "\n",
      "Train set: Avg. loss: 0.000893998, Accuracy: 512/512 (100%)\n",
      "\n",
      "71.80867 76.160866\n",
      "l2 norm: 1011.9570098554584\n",
      "l1 norm: 846.6777813703484\n",
      "Rbeta: 1012.4598975332343\n",
      "\n",
      "Train set: Avg. loss: 0.000874135, Accuracy: 512/512 (100%)\n",
      "\n",
      "72.0641 76.41566\n",
      "l2 norm: 1010.7563657749861\n",
      "l1 norm: 845.6921859347037\n",
      "Rbeta: 1011.2553457408885\n",
      "\n",
      "Train set: Avg. loss: 0.000855135, Accuracy: 512/512 (100%)\n",
      "\n",
      "72.31304 76.66569\n",
      "l2 norm: 1009.5849194467629\n",
      "l1 norm: 844.7305692338352\n",
      "Rbeta: 1010.080329213384\n",
      "\n",
      "Train set: Avg. loss: 0.000836872, Accuracy: 512/512 (100%)\n",
      "\n",
      "72.557816 76.911156\n",
      "l2 norm: 1008.4453943971881\n",
      "l1 norm: 843.7951676027865\n",
      "Rbeta: 1008.9373155208006\n",
      "\n",
      "Train set: Avg. loss: 0.000819279, Accuracy: 512/512 (100%)\n",
      "\n",
      "72.799255 77.1522\n",
      "l2 norm: 1007.3341227273866\n",
      "l1 norm: 842.8828991378439\n",
      "Rbeta: 1007.8224850885792\n",
      "\n",
      "Train set: Avg. loss: 0.000802318, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.03753 77.389015\n",
      "l2 norm: 1006.2490898372433\n",
      "l1 norm: 841.9922407315637\n",
      "Rbeta: 1006.7337775015577\n",
      "\n",
      "Train set: Avg. loss: 0.000786080, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.269196 77.6217\n",
      "l2 norm: 1005.194316086021\n",
      "l1 norm: 841.1264452123102\n",
      "Rbeta: 1005.6759417004686\n",
      "\n",
      "Train set: Avg. loss: 0.000770457, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.49637 77.85043\n",
      "l2 norm: 1004.1621689200196\n",
      "l1 norm: 840.2791635417016\n",
      "Rbeta: 1004.6408399617243\n",
      "\n",
      "Train set: Avg. loss: 0.000755356, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.7211 78.0753\n",
      "l2 norm: 1003.1586257025444\n",
      "l1 norm: 839.4554278627375\n",
      "Rbeta: 1003.6341977243934\n",
      "\n",
      "Train set: Avg. loss: 0.000740755, Accuracy: 512/512 (100%)\n",
      "\n",
      "73.94325 78.29646\n",
      "l2 norm: 1002.1742073189172\n",
      "l1 norm: 838.647401754482\n",
      "Rbeta: 1002.6465406573149\n",
      "\n",
      "Train set: Avg. loss: 0.000726670, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.16176 78.51404\n",
      "l2 norm: 1001.2129693060375\n",
      "l1 norm: 837.8583373572064\n",
      "Rbeta: 1001.6821566548141\n",
      "\n",
      "Train set: Avg. loss: 0.000713100, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.37586 78.728096\n",
      "l2 norm: 1000.2702541780461\n",
      "l1 norm: 837.0844258327887\n",
      "Rbeta: 1000.7365980752055\n",
      "\n",
      "Train set: Avg. loss: 0.000700055, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.58455 78.9388\n",
      "l2 norm: 999.3497943337444\n",
      "l1 norm: 836.3288181228154\n",
      "Rbeta: 999.8137105826595\n",
      "\n",
      "Train set: Avg. loss: 0.000687405, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.791275 79.14626\n",
      "l2 norm: 998.4486615327295\n",
      "l1 norm: 835.589129400019\n",
      "Rbeta: 998.9099431571501\n",
      "\n",
      "Train set: Avg. loss: 0.000675135, Accuracy: 512/512 (100%)\n",
      "\n",
      "74.996155 79.35052\n",
      "l2 norm: 997.5649146787501\n",
      "l1 norm: 834.8636293011489\n",
      "Rbeta: 998.0234545423713\n",
      "\n",
      "Train set: Avg. loss: 0.000663257, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.19822 79.5517\n",
      "l2 norm: 996.7031536558464\n",
      "l1 norm: 834.1562328236037\n",
      "Rbeta: 997.1588719000488\n",
      "\n",
      "Train set: Avg. loss: 0.000651771, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.396866 79.7499\n",
      "l2 norm: 995.8506898151104\n",
      "l1 norm: 833.4563500837485\n",
      "Rbeta: 996.3037364298804\n",
      "\n",
      "Train set: Avg. loss: 0.000640681, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.591446 79.94516\n",
      "l2 norm: 995.0221908706694\n",
      "l1 norm: 832.7762280273481\n",
      "Rbeta: 995.4729161717303\n",
      "\n",
      "Train set: Avg. loss: 0.000629934, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.78328 80.13765\n",
      "l2 norm: 994.2150896645252\n",
      "l1 norm: 832.1138079105831\n",
      "Rbeta: 994.6635388550084\n",
      "\n",
      "Train set: Avg. loss: 0.000619498, Accuracy: 512/512 (100%)\n",
      "\n",
      "75.97309 80.32739\n",
      "l2 norm: 993.4195138112799\n",
      "l1 norm: 831.4606175242038\n",
      "Rbeta: 993.8655218123365\n",
      "\n",
      "Train set: Avg. loss: 0.000609389, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.15983 80.51444\n",
      "l2 norm: 992.6371581156307\n",
      "l1 norm: 830.8183730969547\n",
      "Rbeta: 993.0809196666858\n",
      "\n",
      "Train set: Avg. loss: 0.000599566, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.34454 80.69892\n",
      "l2 norm: 991.8680692260441\n",
      "l1 norm: 830.1869717707464\n",
      "Rbeta: 992.3094428825556\n",
      "\n",
      "Train set: Avg. loss: 0.000590039, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.52652 80.88086\n",
      "l2 norm: 991.1229154765949\n",
      "l1 norm: 829.575390709268\n",
      "Rbeta: 991.5621061674472\n",
      "\n",
      "Train set: Avg. loss: 0.000580793, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.70588 81.060394\n",
      "l2 norm: 990.3867594018167\n",
      "l1 norm: 828.9711826660853\n",
      "Rbeta: 990.8237802613769\n",
      "\n",
      "Train set: Avg. loss: 0.000571801, Accuracy: 512/512 (100%)\n",
      "\n",
      "76.8833 81.237465\n",
      "l2 norm: 989.6629610529973\n",
      "l1 norm: 828.3770182732752\n",
      "Rbeta: 990.09776204764\n",
      "\n",
      "Train set: Avg. loss: 0.000563085, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.05754 81.41225\n",
      "l2 norm: 988.9492612732847\n",
      "l1 norm: 827.7911996977707\n",
      "Rbeta: 989.382074263145\n",
      "\n",
      "Train set: Avg. loss: 0.000554598, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.23009 81.58478\n",
      "l2 norm: 988.2479289019465\n",
      "l1 norm: 827.2154620291963\n",
      "Rbeta: 988.6786984106017\n",
      "\n",
      "Train set: Avg. loss: 0.000546347, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.400406 81.75505\n",
      "l2 norm: 987.5586564319366\n",
      "l1 norm: 826.6495951703635\n",
      "Rbeta: 987.9874334507643\n",
      "\n",
      "Train set: Avg. loss: 0.000538327, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.568306 81.9232\n",
      "l2 norm: 986.8856626528582\n",
      "l1 norm: 826.0972551256558\n",
      "Rbeta: 987.312553818492\n",
      "\n",
      "Train set: Avg. loss: 0.000530497, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.73519 82.08925\n",
      "l2 norm: 986.2253716290396\n",
      "l1 norm: 825.555163923349\n",
      "Rbeta: 986.6500682564972\n",
      "\n",
      "Train set: Avg. loss: 0.000522903, Accuracy: 512/512 (100%)\n",
      "\n",
      "77.898834 82.25326\n",
      "l2 norm: 985.572176496199\n",
      "l1 norm: 825.0190269412276\n",
      "Rbeta: 985.9951762715864\n",
      "\n",
      "Train set: Avg. loss: 0.000515521, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.059875 82.41529\n",
      "l2 norm: 984.934332778262\n",
      "l1 norm: 824.4954975719113\n",
      "Rbeta: 985.3554658867979\n",
      "\n",
      "Train set: Avg. loss: 0.000508326, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.21927 82.57532\n",
      "l2 norm: 984.3007055315679\n",
      "l1 norm: 823.9753799657999\n",
      "Rbeta: 984.7202743521278\n",
      "\n",
      "Train set: Avg. loss: 0.000501282, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.37828 82.7334\n",
      "l2 norm: 983.68319791622\n",
      "l1 norm: 823.4685127277287\n",
      "Rbeta: 984.1007574429524\n",
      "\n",
      "Train set: Avg. loss: 0.000494429, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.53485 82.88965\n",
      "l2 norm: 983.0724848641073\n",
      "l1 norm: 822.9671921413951\n",
      "Rbeta: 983.4882403728044\n",
      "\n",
      "Train set: Avg. loss: 0.000487749, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.689606 83.04404\n",
      "l2 norm: 982.4752786082436\n",
      "l1 norm: 822.4768794439412\n",
      "Rbeta: 982.8892567132525\n",
      "\n",
      "Train set: Avg. loss: 0.000481239, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.842255 83.19675\n",
      "l2 norm: 981.8832847694341\n",
      "l1 norm: 821.9908340309066\n",
      "Rbeta: 982.2956323008208\n",
      "\n",
      "Train set: Avg. loss: 0.000474896, Accuracy: 512/512 (100%)\n",
      "\n",
      "78.99288 83.34769\n",
      "l2 norm: 981.3059905204301\n",
      "l1 norm: 821.5170296001077\n",
      "Rbeta: 981.716747039934\n",
      "\n",
      "Train set: Avg. loss: 0.000468711, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.14161 83.49692\n",
      "l2 norm: 980.7284964145649\n",
      "l1 norm: 821.0429689581426\n",
      "Rbeta: 981.1375647477895\n",
      "\n",
      "Train set: Avg. loss: 0.000462673, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.28877 83.64441\n",
      "l2 norm: 980.1645511373711\n",
      "l1 norm: 820.5800504338612\n",
      "Rbeta: 980.5721257093262\n",
      "\n",
      "Train set: Avg. loss: 0.000456773, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.43457 83.79033\n",
      "l2 norm: 979.6122635189975\n",
      "l1 norm: 820.1267396558146\n",
      "Rbeta: 980.0182611405367\n",
      "\n",
      "Train set: Avg. loss: 0.000451003, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.57913 83.93466\n",
      "l2 norm: 979.0694157447721\n",
      "l1 norm: 819.6811932011894\n",
      "Rbeta: 979.4738655969469\n",
      "\n",
      "Train set: Avg. loss: 0.000445366, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.72224 84.07744\n",
      "l2 norm: 978.5308809283639\n",
      "l1 norm: 819.2390023794508\n",
      "Rbeta: 978.9337291431096\n",
      "\n",
      "Train set: Avg. loss: 0.000439871, Accuracy: 512/512 (100%)\n",
      "\n",
      "79.863174 84.21868\n",
      "l2 norm: 977.994295460923\n",
      "l1 norm: 818.7984838987584\n",
      "Rbeta: 978.3956000505754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000434486, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.00336 84.35847\n",
      "l2 norm: 977.4807246303349\n",
      "l1 norm: 818.3769875819485\n",
      "Rbeta: 977.8805574719889\n",
      "\n",
      "Train set: Avg. loss: 0.000429222, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.142105 84.49678\n",
      "l2 norm: 976.9629243987016\n",
      "l1 norm: 817.9519516477626\n",
      "Rbeta: 977.3612293416904\n",
      "\n",
      "Train set: Avg. loss: 0.000424095, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.278366 84.63362\n",
      "l2 norm: 976.4549890628425\n",
      "l1 norm: 817.5349536286672\n",
      "Rbeta: 976.851993506752\n",
      "\n",
      "Train set: Avg. loss: 0.000419077, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.41345 84.769135\n",
      "l2 norm: 975.9512700221476\n",
      "l1 norm: 817.1213873426777\n",
      "Rbeta: 976.3469467005973\n",
      "\n",
      "Train set: Avg. loss: 0.000414156, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.54788 84.9032\n",
      "l2 norm: 975.4550641177508\n",
      "l1 norm: 816.7141285900582\n",
      "Rbeta: 975.8492335486783\n",
      "\n",
      "Train set: Avg. loss: 0.000409357, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.68007 85.03592\n",
      "l2 norm: 974.9649496710636\n",
      "l1 norm: 816.3117223353668\n",
      "Rbeta: 975.3578533414501\n",
      "\n",
      "Train set: Avg. loss: 0.000404655, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.81129 85.16731\n",
      "l2 norm: 974.483837263268\n",
      "l1 norm: 815.9167826665882\n",
      "Rbeta: 974.8753972403621\n",
      "\n",
      "Train set: Avg. loss: 0.000400042, Accuracy: 512/512 (100%)\n",
      "\n",
      "80.94179 85.29745\n",
      "l2 norm: 974.0107519736518\n",
      "l1 norm: 815.528445378492\n",
      "Rbeta: 974.4009258276659\n",
      "\n",
      "Train set: Avg. loss: 0.000395519, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.07135 85.42634\n",
      "l2 norm: 973.5402507160618\n",
      "l1 norm: 815.1421634879309\n",
      "Rbeta: 973.9289679768408\n",
      "\n",
      "Train set: Avg. loss: 0.000391101, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.1991 85.553894\n",
      "l2 norm: 973.0813244897915\n",
      "l1 norm: 814.765469787769\n",
      "Rbeta: 973.4687767385005\n",
      "\n",
      "Train set: Avg. loss: 0.000386786, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.32491 85.68034\n",
      "l2 norm: 972.6173864511957\n",
      "l1 norm: 814.3845638460252\n",
      "Rbeta: 973.0036266906513\n",
      "\n",
      "Train set: Avg. loss: 0.000382558, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.44948 85.80556\n",
      "l2 norm: 972.1576777698725\n",
      "l1 norm: 814.0071124847334\n",
      "Rbeta: 972.5428018477133\n",
      "\n",
      "Train set: Avg. loss: 0.000378405, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.57363 85.92956\n",
      "l2 norm: 971.7153150070733\n",
      "l1 norm: 813.6440635467893\n",
      "Rbeta: 972.099193485202\n",
      "\n",
      "Train set: Avg. loss: 0.000374339, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.696396 86.0524\n",
      "l2 norm: 971.2754463406271\n",
      "l1 norm: 813.2828748421057\n",
      "Rbeta: 971.6581392695226\n",
      "\n",
      "Train set: Avg. loss: 0.000370349, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.81827 86.17413\n",
      "l2 norm: 970.8408285996053\n",
      "l1 norm: 812.9260200782544\n",
      "Rbeta: 971.2222771085645\n",
      "\n",
      "Train set: Avg. loss: 0.000366428, Accuracy: 512/512 (100%)\n",
      "\n",
      "81.93964 86.29476\n",
      "l2 norm: 970.4141631710016\n",
      "l1 norm: 812.5759113360474\n",
      "Rbeta: 970.7943474244157\n",
      "\n",
      "Train set: Avg. loss: 0.000362584, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.05992 86.414246\n",
      "l2 norm: 969.997115462862\n",
      "l1 norm: 812.2336856744053\n",
      "Rbeta: 970.3760073475271\n",
      "\n",
      "Train set: Avg. loss: 0.000358816, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.179 86.532616\n",
      "l2 norm: 969.5791410778234\n",
      "l1 norm: 811.8905637123763\n",
      "Rbeta: 969.9566833197233\n",
      "\n",
      "Train set: Avg. loss: 0.000355144, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.295456 86.650024\n",
      "l2 norm: 969.1609526927782\n",
      "l1 norm: 811.5472358545229\n",
      "Rbeta: 969.5375601399901\n",
      "\n",
      "Train set: Avg. loss: 0.000351548, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.41045 86.76636\n",
      "l2 norm: 968.7471564938728\n",
      "l1 norm: 811.2074226194113\n",
      "Rbeta: 969.1229012677691\n",
      "\n",
      "Train set: Avg. loss: 0.000348012, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.52498 86.88164\n",
      "l2 norm: 968.3473647056671\n",
      "l1 norm: 810.8793199986603\n",
      "Rbeta: 968.7220463683806\n",
      "\n",
      "Train set: Avg. loss: 0.000344546, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.63823 86.99599\n",
      "l2 norm: 967.9504584395781\n",
      "l1 norm: 810.5534132412077\n",
      "Rbeta: 968.3243142642061\n",
      "\n",
      "Train set: Avg. loss: 0.000341135, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.75102 87.1094\n",
      "l2 norm: 967.5550197171046\n",
      "l1 norm: 810.2287669893851\n",
      "Rbeta: 967.9277817012004\n",
      "\n",
      "Train set: Avg. loss: 0.000337774, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.86374 87.22175\n",
      "l2 norm: 967.1658101781984\n",
      "l1 norm: 809.9093315102413\n",
      "Rbeta: 967.5374398237007\n",
      "\n",
      "Train set: Avg. loss: 0.000334461, Accuracy: 512/512 (100%)\n",
      "\n",
      "82.97644 87.33321\n",
      "l2 norm: 966.7967151342186\n",
      "l1 norm: 809.6066430926128\n",
      "Rbeta: 967.1672332098024\n",
      "\n",
      "Train set: Avg. loss: 0.000331209, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.08807 87.44368\n",
      "l2 norm: 966.4183792892944\n",
      "l1 norm: 809.295996089711\n",
      "Rbeta: 966.7876558057226\n",
      "\n",
      "Train set: Avg. loss: 0.000328028, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.19801 87.55316\n",
      "l2 norm: 966.0464334535623\n",
      "l1 norm: 808.9907030049228\n",
      "Rbeta: 966.4146253094337\n",
      "\n",
      "Train set: Avg. loss: 0.000324901, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.30722 87.661804\n",
      "l2 norm: 965.6676871849489\n",
      "l1 norm: 808.6796599850372\n",
      "Rbeta: 966.0347719193664\n",
      "\n",
      "Train set: Avg. loss: 0.000321845, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.41432 87.76956\n",
      "l2 norm: 965.2992683186816\n",
      "l1 norm: 808.377191642037\n",
      "Rbeta: 965.6654491008803\n",
      "\n",
      "Train set: Avg. loss: 0.000318838, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.52076 87.8765\n",
      "l2 norm: 964.941351179119\n",
      "l1 norm: 808.0834933113216\n",
      "Rbeta: 965.3066578715254\n",
      "\n",
      "Train set: Avg. loss: 0.000315880, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.62666 87.982635\n",
      "l2 norm: 964.5858853579902\n",
      "l1 norm: 807.791817615376\n",
      "Rbeta: 964.9502868791369\n",
      "\n",
      "Train set: Avg. loss: 0.000312975, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.73148 88.08778\n",
      "l2 norm: 964.2192269400168\n",
      "l1 norm: 807.4906599201802\n",
      "Rbeta: 964.5826862263546\n",
      "\n",
      "Train set: Avg. loss: 0.000310121, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.83543 88.19214\n",
      "l2 norm: 963.8669468488114\n",
      "l1 norm: 807.2015047590493\n",
      "Rbeta: 964.2293983470936\n",
      "\n",
      "Train set: Avg. loss: 0.000307305, Accuracy: 512/512 (100%)\n",
      "\n",
      "83.939316 88.2957\n",
      "l2 norm: 963.5151530425892\n",
      "l1 norm: 806.9127197861736\n",
      "Rbeta: 963.8767108014518\n",
      "\n",
      "Train set: Avg. loss: 0.000304537, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.04237 88.39838\n",
      "l2 norm: 963.1703576608253\n",
      "l1 norm: 806.6295783629582\n",
      "Rbeta: 963.5308976256063\n",
      "\n",
      "Train set: Avg. loss: 0.000301820, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.14427 88.50022\n",
      "l2 norm: 962.8379827361586\n",
      "l1 norm: 806.3569582176938\n",
      "Rbeta: 963.1976698316911\n",
      "\n",
      "Train set: Avg. loss: 0.000299144, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.245636 88.60138\n",
      "l2 norm: 962.4976951115873\n",
      "l1 norm: 806.0776386549961\n",
      "Rbeta: 962.8564142369613\n",
      "\n",
      "Train set: Avg. loss: 0.000296510, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.346405 88.70175\n",
      "l2 norm: 962.1600946222029\n",
      "l1 norm: 805.8003820557062\n",
      "Rbeta: 962.5178935489203\n",
      "\n",
      "Train set: Avg. loss: 0.000293920, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.44632 88.801346\n",
      "l2 norm: 961.833801387147\n",
      "l1 norm: 805.5325516260195\n",
      "Rbeta: 962.1906781218004\n",
      "\n",
      "Train set: Avg. loss: 0.000291369, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.545715 88.90018\n",
      "l2 norm: 961.49964603021\n",
      "l1 norm: 805.2581670402369\n",
      "Rbeta: 961.8555009987723\n",
      "\n",
      "Train set: Avg. loss: 0.000288864, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.64392 88.998245\n",
      "l2 norm: 961.17870280631\n",
      "l1 norm: 804.9946829049336\n",
      "Rbeta: 961.5336735234431\n",
      "\n",
      "Train set: Avg. loss: 0.000286412, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.74046 89.09562\n",
      "l2 norm: 960.8549208994821\n",
      "l1 norm: 804.7288596987385\n",
      "Rbeta: 961.2092978877762\n",
      "\n",
      "Train set: Avg. loss: 0.000283998, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.83627 89.192245\n",
      "l2 norm: 960.5335441098838\n",
      "l1 norm: 804.4649422334853\n",
      "Rbeta: 960.8870935946325\n",
      "\n",
      "Train set: Avg. loss: 0.000281612, Accuracy: 512/512 (100%)\n",
      "\n",
      "84.932144 89.28818\n",
      "l2 norm: 960.2145751941255\n",
      "l1 norm: 804.2030812762528\n",
      "Rbeta: 960.5673388001173\n",
      "\n",
      "Train set: Avg. loss: 0.000279258, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.0278 89.38342\n",
      "l2 norm: 959.9115474192427\n",
      "l1 norm: 803.9546059882384\n",
      "Rbeta: 960.2633954019088\n",
      "\n",
      "Train set: Avg. loss: 0.000276944, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.12245 89.47796\n",
      "l2 norm: 959.5949956471384\n",
      "l1 norm: 803.6946780350595\n",
      "Rbeta: 959.9459524468833\n",
      "\n",
      "Train set: Avg. loss: 0.000274664, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.21657 89.57184\n",
      "l2 norm: 959.2910462299945\n",
      "l1 norm: 803.4451815956191\n",
      "Rbeta: 959.6411691768187\n",
      "\n",
      "Train set: Avg. loss: 0.000272422, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.309845 89.665016\n",
      "l2 norm: 958.9849722600354\n",
      "l1 norm: 803.1937872413603\n",
      "Rbeta: 959.3343435634107\n",
      "\n",
      "Train set: Avg. loss: 0.000270217, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.402176 89.7575\n",
      "l2 norm: 958.6881825487892\n",
      "l1 norm: 802.9501003658204\n",
      "Rbeta: 959.0367193128869\n",
      "\n",
      "Train set: Avg. loss: 0.000268038, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.49434 89.84938\n",
      "l2 norm: 958.3893098779157\n",
      "l1 norm: 802.7046170736884\n",
      "Rbeta: 958.7370628621962\n",
      "\n",
      "Train set: Avg. loss: 0.000265897, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.58554 89.94066\n",
      "l2 norm: 958.1038914932154\n",
      "l1 norm: 802.4704317820649\n",
      "Rbeta: 958.4509294628268\n",
      "\n",
      "Train set: Avg. loss: 0.000263790, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.675766 90.03133\n",
      "l2 norm: 957.8103878171589\n",
      "l1 norm: 802.2295560928453\n",
      "Rbeta: 958.1566733401341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000261717, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.76518 90.12128\n",
      "l2 norm: 957.5237598554615\n",
      "l1 norm: 801.9943368538788\n",
      "Rbeta: 957.8693507528867\n",
      "\n",
      "Train set: Avg. loss: 0.000259678, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.853714 90.21065\n",
      "l2 norm: 957.2370188623054\n",
      "l1 norm: 801.7589775385579\n",
      "Rbeta: 957.5819581533804\n",
      "\n",
      "Train set: Avg. loss: 0.000257662, Accuracy: 512/512 (100%)\n",
      "\n",
      "85.94205 90.2995\n",
      "l2 norm: 956.9553458663486\n",
      "l1 norm: 801.5278248317729\n",
      "Rbeta: 957.2996353682295\n",
      "\n",
      "Train set: Avg. loss: 0.000255670, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.030426 90.38768\n",
      "l2 norm: 956.6782675122527\n",
      "l1 norm: 801.3004897398746\n",
      "Rbeta: 957.0217689936494\n",
      "\n",
      "Train set: Avg. loss: 0.000253704, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.11841 90.475235\n",
      "l2 norm: 956.4013033015814\n",
      "l1 norm: 801.0730902813448\n",
      "Rbeta: 956.7440235179347\n",
      "\n",
      "Train set: Avg. loss: 0.000251769, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.205536 90.562225\n",
      "l2 norm: 956.1232771043068\n",
      "l1 norm: 800.8448298793636\n",
      "Rbeta: 956.4652500691587\n",
      "\n",
      "Train set: Avg. loss: 0.000249859, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.29233 90.64865\n",
      "l2 norm: 955.8492193506455\n",
      "l1 norm: 800.6198121358536\n",
      "Rbeta: 956.1904839209718\n",
      "\n",
      "Train set: Avg. loss: 0.000247970, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.37904 90.7345\n",
      "l2 norm: 955.5815768893783\n",
      "l1 norm: 800.4000560980511\n",
      "Rbeta: 955.922019965977\n",
      "\n",
      "Train set: Avg. loss: 0.000246111, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.46481 90.81988\n",
      "l2 norm: 955.3071223589615\n",
      "l1 norm: 800.1746091039195\n",
      "Rbeta: 955.6468152428296\n",
      "\n",
      "Train set: Avg. loss: 0.000244284, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.5495 90.90468\n",
      "l2 norm: 955.0495189307916\n",
      "l1 norm: 799.9633056755606\n",
      "Rbeta: 955.3884790822942\n",
      "\n",
      "Train set: Avg. loss: 0.000242488, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.633026 90.98892\n",
      "l2 norm: 954.7862859727918\n",
      "l1 norm: 799.7473795201597\n",
      "Rbeta: 955.1246467363042\n",
      "\n",
      "Train set: Avg. loss: 0.000240715, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.71633 91.072586\n",
      "l2 norm: 954.5370144797491\n",
      "l1 norm: 799.543023630138\n",
      "Rbeta: 954.874766927778\n",
      "\n",
      "Train set: Avg. loss: 0.000238969, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.79874 91.15578\n",
      "l2 norm: 954.2665996333606\n",
      "l1 norm: 799.3208379487044\n",
      "Rbeta: 954.6038556248552\n",
      "\n",
      "Train set: Avg. loss: 0.000237245, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.88067 91.23847\n",
      "l2 norm: 954.0007501894964\n",
      "l1 norm: 799.1024673210536\n",
      "Rbeta: 954.3373597400208\n",
      "\n",
      "Train set: Avg. loss: 0.000235542, Accuracy: 512/512 (100%)\n",
      "\n",
      "86.962494 91.32051\n",
      "l2 norm: 953.7583359302193\n",
      "l1 norm: 798.9037523051078\n",
      "Rbeta: 954.094278410361\n",
      "\n",
      "Train set: Avg. loss: 0.000233859, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.04404 91.40202\n",
      "l2 norm: 953.4967988250058\n",
      "l1 norm: 798.6889175637264\n",
      "Rbeta: 953.8320974328209\n",
      "\n",
      "Train set: Avg. loss: 0.000232200, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.1248 91.48318\n",
      "l2 norm: 953.2332419120704\n",
      "l1 norm: 798.4723258817678\n",
      "Rbeta: 953.5679185884986\n",
      "\n",
      "Train set: Avg. loss: 0.000230562, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.205154 91.56386\n",
      "l2 norm: 952.9818402576705\n",
      "l1 norm: 798.2658691430021\n",
      "Rbeta: 953.3160128440757\n",
      "\n",
      "Train set: Avg. loss: 0.000228942, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.2854 91.64397\n",
      "l2 norm: 952.7370782720759\n",
      "l1 norm: 798.065029455865\n",
      "Rbeta: 953.0705672056323\n",
      "\n",
      "Train set: Avg. loss: 0.000227344, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.36511 91.72361\n",
      "l2 norm: 952.4975981032449\n",
      "l1 norm: 797.8684984588933\n",
      "Rbeta: 952.8303759253049\n",
      "\n",
      "Train set: Avg. loss: 0.000225761, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.44487 91.80275\n",
      "l2 norm: 952.2621980859299\n",
      "l1 norm: 797.6754515949327\n",
      "Rbeta: 952.5943402337554\n",
      "\n",
      "Train set: Avg. loss: 0.000224198, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.52423 91.88138\n",
      "l2 norm: 952.0298209931111\n",
      "l1 norm: 797.4848819023243\n",
      "Rbeta: 952.3612112494635\n",
      "\n",
      "Train set: Avg. loss: 0.000222654, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.60327 91.95953\n",
      "l2 norm: 951.7936625097175\n",
      "l1 norm: 797.2911367341077\n",
      "Rbeta: 952.1243237789225\n",
      "\n",
      "Train set: Avg. loss: 0.000221130, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.68176 92.03722\n",
      "l2 norm: 951.5468483450345\n",
      "l1 norm: 797.0883269652375\n",
      "Rbeta: 951.8768438133362\n",
      "\n",
      "Train set: Avg. loss: 0.000219634, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.75893 92.11452\n",
      "l2 norm: 951.3028019061364\n",
      "l1 norm: 796.8878126299021\n",
      "Rbeta: 951.6322087439681\n",
      "\n",
      "Train set: Avg. loss: 0.000218164, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.83479 92.19137\n",
      "l2 norm: 951.0654444219311\n",
      "l1 norm: 796.6929821641838\n",
      "Rbeta: 951.3942790741701\n",
      "\n",
      "Train set: Avg. loss: 0.000216714, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.91014 92.26769\n",
      "l2 norm: 950.8343850518521\n",
      "l1 norm: 796.5034047452289\n",
      "Rbeta: 951.1627667107759\n",
      "\n",
      "Train set: Avg. loss: 0.000215278, Accuracy: 512/512 (100%)\n",
      "\n",
      "87.985504 92.34357\n",
      "l2 norm: 950.6014657343372\n",
      "l1 norm: 796.3122788840378\n",
      "Rbeta: 950.92935370921\n",
      "\n",
      "Train set: Avg. loss: 0.000213857, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.06062 92.41909\n",
      "l2 norm: 950.3727287678597\n",
      "l1 norm: 796.1244891387322\n",
      "Rbeta: 950.7000964374654\n",
      "\n",
      "Train set: Avg. loss: 0.000212453, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.13536 92.49417\n",
      "l2 norm: 950.1526397553951\n",
      "l1 norm: 795.9437985991027\n",
      "Rbeta: 950.4794477518855\n",
      "\n",
      "Train set: Avg. loss: 0.000211065, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.209854 92.56879\n",
      "l2 norm: 949.9249942183719\n",
      "l1 norm: 795.7567336689822\n",
      "Rbeta: 950.2512546613372\n",
      "\n",
      "Train set: Avg. loss: 0.000209690, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.284386 92.64296\n",
      "l2 norm: 949.7029441430594\n",
      "l1 norm: 795.574537566544\n",
      "Rbeta: 950.0285940590527\n",
      "\n",
      "Train set: Avg. loss: 0.000208328, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.35877 92.716705\n",
      "l2 norm: 949.4783825284367\n",
      "l1 norm: 795.3903078686733\n",
      "Rbeta: 949.8033675827639\n",
      "\n",
      "Train set: Avg. loss: 0.000206980, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.433174 92.79007\n",
      "l2 norm: 949.2639536669432\n",
      "l1 norm: 795.2144642488348\n",
      "Rbeta: 949.5882632790219\n",
      "\n",
      "Train set: Avg. loss: 0.000205645, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.50737 92.86302\n",
      "l2 norm: 949.0536394900317\n",
      "l1 norm: 795.0421557967913\n",
      "Rbeta: 949.3772244518849\n",
      "\n",
      "Train set: Avg. loss: 0.000204326, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.581184 92.93557\n",
      "l2 norm: 948.8251110848031\n",
      "l1 norm: 794.8544424790199\n",
      "Rbeta: 949.1479767951099\n",
      "\n",
      "Train set: Avg. loss: 0.000203027, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.654076 93.00777\n",
      "l2 norm: 948.6108166054687\n",
      "l1 norm: 794.6785615018867\n",
      "Rbeta: 948.933047283662\n",
      "\n",
      "Train set: Avg. loss: 0.000201746, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.72633 93.07947\n",
      "l2 norm: 948.3981377238217\n",
      "l1 norm: 794.5039864171329\n",
      "Rbeta: 948.7198272942757\n",
      "\n",
      "Train set: Avg. loss: 0.000200482, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.798 93.15077\n",
      "l2 norm: 948.1924107185799\n",
      "l1 norm: 794.3352964882533\n",
      "Rbeta: 948.5134318897366\n",
      "\n",
      "Train set: Avg. loss: 0.000199234, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.86915 93.22173\n",
      "l2 norm: 947.9787915877488\n",
      "l1 norm: 794.159922020484\n",
      "Rbeta: 948.2993263922291\n",
      "\n",
      "Train set: Avg. loss: 0.000198003, Accuracy: 512/512 (100%)\n",
      "\n",
      "88.93962 93.29224\n",
      "l2 norm: 947.766605906888\n",
      "l1 norm: 793.985738064112\n",
      "Rbeta: 948.0866063767404\n",
      "\n",
      "Train set: Avg. loss: 0.000196784, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.01004 93.36233\n",
      "l2 norm: 947.5714855660244\n",
      "l1 norm: 793.8258222569203\n",
      "Rbeta: 947.8909450134627\n",
      "\n",
      "Train set: Avg. loss: 0.000195575, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.08049 93.43205\n",
      "l2 norm: 947.3704558555633\n",
      "l1 norm: 793.6608831607099\n",
      "Rbeta: 947.6892969268404\n",
      "\n",
      "Train set: Avg. loss: 0.000194377, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.15089 93.50147\n",
      "l2 norm: 947.1616915415435\n",
      "l1 norm: 793.4893510301423\n",
      "Rbeta: 947.4798516632234\n",
      "\n",
      "Train set: Avg. loss: 0.000193189, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.22121 93.57056\n",
      "l2 norm: 946.9594664520556\n",
      "l1 norm: 793.3234691136163\n",
      "Rbeta: 947.2769500748859\n",
      "\n",
      "Train set: Avg. loss: 0.000192019, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.29067 93.639275\n",
      "l2 norm: 946.745793557551\n",
      "l1 norm: 793.1480210355487\n",
      "Rbeta: 947.0627179377672\n",
      "\n",
      "Train set: Avg. loss: 0.000190865, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.35951 93.70758\n",
      "l2 norm: 946.53669145619\n",
      "l1 norm: 792.9762334611662\n",
      "Rbeta: 946.853025975246\n",
      "\n",
      "Train set: Avg. loss: 0.000189727, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.42769 93.77551\n",
      "l2 norm: 946.3428623324085\n",
      "l1 norm: 792.8172390987686\n",
      "Rbeta: 946.6586424296929\n",
      "\n",
      "Train set: Avg. loss: 0.000188598, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.49577 93.84316\n",
      "l2 norm: 946.1406352613964\n",
      "l1 norm: 792.6511407151852\n",
      "Rbeta: 946.4558868394611\n",
      "\n",
      "Train set: Avg. loss: 0.000187488, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.56273 93.91048\n",
      "l2 norm: 945.939606241735\n",
      "l1 norm: 792.4859745686722\n",
      "Rbeta: 946.254404678383\n",
      "\n",
      "Train set: Avg. loss: 0.000186390, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.629456 93.97739\n",
      "l2 norm: 945.7403488238486\n",
      "l1 norm: 792.3223030427628\n",
      "Rbeta: 946.0547221055381\n",
      "\n",
      "Train set: Avg. loss: 0.000185308, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.69528 94.043915\n",
      "l2 norm: 945.5464287038656\n",
      "l1 norm: 792.1631147989187\n",
      "Rbeta: 945.8603522008912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000184237, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.761024 94.1101\n",
      "l2 norm: 945.3544671963044\n",
      "l1 norm: 792.005606354095\n",
      "Rbeta: 945.6679855589116\n",
      "\n",
      "Train set: Avg. loss: 0.000183174, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.82669 94.17598\n",
      "l2 norm: 945.1649563776999\n",
      "l1 norm: 791.8500987867828\n",
      "Rbeta: 945.4780667063698\n",
      "\n",
      "Train set: Avg. loss: 0.000182125, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.891785 94.241516\n",
      "l2 norm: 944.9774997431205\n",
      "l1 norm: 791.6962614516933\n",
      "Rbeta: 945.290158804763\n",
      "\n",
      "Train set: Avg. loss: 0.000181086, Accuracy: 512/512 (100%)\n",
      "\n",
      "89.956696 94.3068\n",
      "l2 norm: 944.8025423993479\n",
      "l1 norm: 791.5529289403689\n",
      "Rbeta: 945.1147690636574\n",
      "\n",
      "Train set: Avg. loss: 0.000180058, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.0214 94.37169\n",
      "l2 norm: 944.6258107966088\n",
      "l1 norm: 791.4081567714472\n",
      "Rbeta: 944.9376585614012\n",
      "\n",
      "Train set: Avg. loss: 0.000179039, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.08592 94.43628\n",
      "l2 norm: 944.4363923665086\n",
      "l1 norm: 791.2526037510561\n",
      "Rbeta: 944.7476893571306\n",
      "\n",
      "Train set: Avg. loss: 0.000178031, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.15001 94.5006\n",
      "l2 norm: 944.2403541463349\n",
      "l1 norm: 791.0915384881018\n",
      "Rbeta: 944.5512315906266\n",
      "\n",
      "Train set: Avg. loss: 0.000177040, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.213 94.564575\n",
      "l2 norm: 944.0602102795534\n",
      "l1 norm: 790.9438362116675\n",
      "Rbeta: 944.3707401805027\n",
      "\n",
      "Train set: Avg. loss: 0.000176059, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.275665 94.62825\n",
      "l2 norm: 943.8708693189697\n",
      "l1 norm: 790.7882820361883\n",
      "Rbeta: 944.181120489072\n",
      "\n",
      "Train set: Avg. loss: 0.000175087, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.3383 94.691574\n",
      "l2 norm: 943.7018366086895\n",
      "l1 norm: 790.6497957279324\n",
      "Rbeta: 944.0117066121559\n",
      "\n",
      "Train set: Avg. loss: 0.000174123, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.400955 94.75455\n",
      "l2 norm: 943.5205653844505\n",
      "l1 norm: 790.5009875616684\n",
      "Rbeta: 943.8300470579085\n",
      "\n",
      "Train set: Avg. loss: 0.000173168, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.463394 94.81723\n",
      "l2 norm: 943.3392816409571\n",
      "l1 norm: 790.3519866574371\n",
      "Rbeta: 943.6482875181479\n",
      "\n",
      "Train set: Avg. loss: 0.000172222, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.52548 94.879715\n",
      "l2 norm: 943.1555220626686\n",
      "l1 norm: 790.2011667417471\n",
      "Rbeta: 943.4641909468376\n",
      "\n",
      "Train set: Avg. loss: 0.000171288, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.58711 94.94182\n",
      "l2 norm: 942.9811497193723\n",
      "l1 norm: 790.0580919234903\n",
      "Rbeta: 943.289438849479\n",
      "\n",
      "Train set: Avg. loss: 0.000170367, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.647995 95.00358\n",
      "l2 norm: 942.7923062653435\n",
      "l1 norm: 789.9028826666822\n",
      "Rbeta: 943.1002713514501\n",
      "\n",
      "Train set: Avg. loss: 0.000169457, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.70842 95.065\n",
      "l2 norm: 942.6234609720217\n",
      "l1 norm: 789.7644211801853\n",
      "Rbeta: 942.9310727874155\n",
      "\n",
      "Train set: Avg. loss: 0.000168554, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.76886 95.12619\n",
      "l2 norm: 942.4580797885985\n",
      "l1 norm: 789.6288248085673\n",
      "Rbeta: 942.7654191938283\n",
      "\n",
      "Train set: Avg. loss: 0.000167661, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.82881 95.18712\n",
      "l2 norm: 942.2889787331621\n",
      "l1 norm: 789.490039331431\n",
      "Rbeta: 942.5960380713631\n",
      "\n",
      "Train set: Avg. loss: 0.000166779, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.88795 95.247826\n",
      "l2 norm: 942.1237699280451\n",
      "l1 norm: 789.354582609936\n",
      "Rbeta: 942.4305009427051\n",
      "\n",
      "Train set: Avg. loss: 0.000165905, Accuracy: 512/512 (100%)\n",
      "\n",
      "90.947075 95.30827\n",
      "l2 norm: 941.9510501907225\n",
      "l1 norm: 789.2128207044162\n",
      "Rbeta: 942.2575235242408\n",
      "\n",
      "Train set: Avg. loss: 0.000165038, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.006226 95.368416\n",
      "l2 norm: 941.7744328213744\n",
      "l1 norm: 789.0676679074106\n",
      "Rbeta: 942.080604696412\n",
      "\n",
      "Train set: Avg. loss: 0.000164176, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.06539 95.42827\n",
      "l2 norm: 941.6016629273247\n",
      "l1 norm: 788.9257379279373\n",
      "Rbeta: 941.9074825615018\n",
      "\n",
      "Train set: Avg. loss: 0.000163322, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.12458 95.48779\n",
      "l2 norm: 941.4392359951443\n",
      "l1 norm: 788.7924533609535\n",
      "Rbeta: 941.7447187642315\n",
      "\n",
      "Train set: Avg. loss: 0.000162479, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.18315 95.547035\n",
      "l2 norm: 941.2727201782759\n",
      "l1 norm: 788.6557037509986\n",
      "Rbeta: 941.5778969338883\n",
      "\n",
      "Train set: Avg. loss: 0.000161643, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.24144 95.606064\n",
      "l2 norm: 941.1051815571634\n",
      "l1 norm: 788.518223666506\n",
      "Rbeta: 941.4100027973993\n",
      "\n",
      "Train set: Avg. loss: 0.000160813, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.29975 95.66485\n",
      "l2 norm: 940.9342759647817\n",
      "l1 norm: 788.3778651263701\n",
      "Rbeta: 941.2387617189909\n",
      "\n",
      "Train set: Avg. loss: 0.000159989, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.35806 95.72339\n",
      "l2 norm: 940.776828651515\n",
      "l1 norm: 788.2486184242321\n",
      "Rbeta: 941.0809592849154\n",
      "\n",
      "Train set: Avg. loss: 0.000159176, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.415726 95.78172\n",
      "l2 norm: 940.6145876163573\n",
      "l1 norm: 788.1154069799713\n",
      "Rbeta: 940.9184069544001\n",
      "\n",
      "Train set: Avg. loss: 0.000158369, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.47328 95.83972\n",
      "l2 norm: 940.4529797828731\n",
      "l1 norm: 787.9827786822373\n",
      "Rbeta: 940.7564241630765\n",
      "\n",
      "Train set: Avg. loss: 0.000157569, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.53085 95.89743\n",
      "l2 norm: 940.3036227409331\n",
      "l1 norm: 787.86041653093\n",
      "Rbeta: 940.606682786525\n",
      "\n",
      "Train set: Avg. loss: 0.000156774, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.588455 95.95488\n",
      "l2 norm: 940.1563047746297\n",
      "l1 norm: 787.7397942930418\n",
      "Rbeta: 940.4589768413338\n",
      "\n",
      "Train set: Avg. loss: 0.000155985, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.64607 96.01204\n",
      "l2 norm: 939.9956041009402\n",
      "l1 norm: 787.6079631003137\n",
      "Rbeta: 940.2978355829085\n",
      "\n",
      "Train set: Avg. loss: 0.000155204, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.70342 96.069\n",
      "l2 norm: 939.8359686552581\n",
      "l1 norm: 787.4770400471798\n",
      "Rbeta: 940.13781083997\n",
      "\n",
      "Train set: Avg. loss: 0.000154429, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.76045 96.12575\n",
      "l2 norm: 939.6734398073808\n",
      "l1 norm: 787.3435552133832\n",
      "Rbeta: 939.9747964963194\n",
      "\n",
      "Train set: Avg. loss: 0.000153663, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.81712 96.18229\n",
      "l2 norm: 939.5262940897693\n",
      "l1 norm: 787.2230010803214\n",
      "Rbeta: 939.8272820469424\n",
      "\n",
      "Train set: Avg. loss: 0.000152906, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.87302 96.23868\n",
      "l2 norm: 939.3631510844267\n",
      "l1 norm: 787.0889301205793\n",
      "Rbeta: 939.6637946691486\n",
      "\n",
      "Train set: Avg. loss: 0.000152156, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.928925 96.29474\n",
      "l2 norm: 939.2022180898775\n",
      "l1 norm: 786.9566897326617\n",
      "Rbeta: 939.5025297407425\n",
      "\n",
      "Train set: Avg. loss: 0.000151415, Accuracy: 512/512 (100%)\n",
      "\n",
      "91.98418 96.35055\n",
      "l2 norm: 939.0519563234143\n",
      "l1 norm: 786.8334625533355\n",
      "Rbeta: 939.3518856188646\n",
      "\n",
      "Train set: Avg. loss: 0.000150680, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.03937 96.40605\n",
      "l2 norm: 938.9083400183372\n",
      "l1 norm: 786.7157553158115\n",
      "Rbeta: 939.2080185863753\n",
      "\n",
      "Train set: Avg. loss: 0.000149951, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.094475 96.46138\n",
      "l2 norm: 938.7535875635261\n",
      "l1 norm: 786.5886641024657\n",
      "Rbeta: 939.0528219817479\n",
      "\n",
      "Train set: Avg. loss: 0.000149229, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.14908 96.516464\n",
      "l2 norm: 938.6099357465162\n",
      "l1 norm: 786.4708433987063\n",
      "Rbeta: 938.9089043036157\n",
      "\n",
      "Train set: Avg. loss: 0.000148514, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.20351 96.571335\n",
      "l2 norm: 938.4571807340117\n",
      "l1 norm: 786.3454526540983\n",
      "Rbeta: 938.7558512249617\n",
      "\n",
      "Train set: Avg. loss: 0.000147805, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.25777 96.62598\n",
      "l2 norm: 938.3093486719214\n",
      "l1 norm: 786.2242015256024\n",
      "Rbeta: 938.607725747486\n",
      "\n",
      "Train set: Avg. loss: 0.000147101, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.31204 96.680374\n",
      "l2 norm: 938.1571254665328\n",
      "l1 norm: 786.0992447165131\n",
      "Rbeta: 938.4551397038883\n",
      "\n",
      "Train set: Avg. loss: 0.000146402, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.36633 96.73448\n",
      "l2 norm: 938.0063705254736\n",
      "l1 norm: 785.975544928695\n",
      "Rbeta: 938.303982657\n",
      "\n",
      "Train set: Avg. loss: 0.000145708, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.42065 96.78833\n",
      "l2 norm: 937.862881795966\n",
      "l1 norm: 785.8578571232591\n",
      "Rbeta: 938.1601610571325\n",
      "\n",
      "Train set: Avg. loss: 0.000145018, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.474976 96.84192\n",
      "l2 norm: 937.7181526956721\n",
      "l1 norm: 785.7391721382863\n",
      "Rbeta: 938.0149162841714\n",
      "\n",
      "Train set: Avg. loss: 0.000144334, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.52931 96.89527\n",
      "l2 norm: 937.5809047682158\n",
      "l1 norm: 785.626692856518\n",
      "Rbeta: 937.8772659403223\n",
      "\n",
      "Train set: Avg. loss: 0.000143653, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.58367 96.94838\n",
      "l2 norm: 937.4352324546785\n",
      "l1 norm: 785.5071070774354\n",
      "Rbeta: 937.7309995878763\n",
      "\n",
      "Train set: Avg. loss: 0.000142978, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.63783 97.00136\n",
      "l2 norm: 937.2927338640484\n",
      "l1 norm: 785.3902289668259\n",
      "Rbeta: 937.5880636767166\n",
      "\n",
      "Train set: Avg. loss: 0.000142310, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.691574 97.05417\n",
      "l2 norm: 937.1467248002656\n",
      "l1 norm: 785.2704520147557\n",
      "Rbeta: 937.4415644534907\n",
      "\n",
      "Train set: Avg. loss: 0.000141652, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.74446 97.10683\n",
      "l2 norm: 937.0106575923559\n",
      "l1 norm: 785.158996627468\n",
      "Rbeta: 937.3051286295577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000140998, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.79726 97.15934\n",
      "l2 norm: 936.8758045514788\n",
      "l1 norm: 785.0486284503013\n",
      "Rbeta: 937.1699595630539\n",
      "\n",
      "Train set: Avg. loss: 0.000140349, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.84996 97.211525\n",
      "l2 norm: 936.7384479621697\n",
      "l1 norm: 784.9361377374881\n",
      "Rbeta: 937.0321844288875\n",
      "\n",
      "Train set: Avg. loss: 0.000139709, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.90212 97.26349\n",
      "l2 norm: 936.6024044762872\n",
      "l1 norm: 784.8246628681976\n",
      "Rbeta: 936.8958639454803\n",
      "\n",
      "Train set: Avg. loss: 0.000139073, Accuracy: 512/512 (100%)\n",
      "\n",
      "92.95413 97.315254\n",
      "l2 norm: 936.4754502801269\n",
      "l1 norm: 784.7207738258123\n",
      "Rbeta: 936.7684441144365\n",
      "\n",
      "Train set: Avg. loss: 0.000138445, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.00549 97.36685\n",
      "l2 norm: 936.3460233852205\n",
      "l1 norm: 784.6146772505249\n",
      "Rbeta: 936.6387359920304\n",
      "\n",
      "Train set: Avg. loss: 0.000137822, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.056656 97.41826\n",
      "l2 norm: 936.2150242057386\n",
      "l1 norm: 784.5072460422934\n",
      "Rbeta: 936.5073905921214\n",
      "\n",
      "Train set: Avg. loss: 0.000137204, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.107834 97.46948\n",
      "l2 norm: 936.0797712467194\n",
      "l1 norm: 784.3962680594626\n",
      "Rbeta: 936.3718337280573\n",
      "\n",
      "Train set: Avg. loss: 0.000136589, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.15903 97.52048\n",
      "l2 norm: 935.9463559870433\n",
      "l1 norm: 784.2869128970153\n",
      "Rbeta: 936.2380842285295\n",
      "\n",
      "Train set: Avg. loss: 0.000135979, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.210175 97.57114\n",
      "l2 norm: 935.8209025595426\n",
      "l1 norm: 784.1842337176336\n",
      "Rbeta: 936.1122265682351\n",
      "\n",
      "Train set: Avg. loss: 0.000135374, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.26121 97.6216\n",
      "l2 norm: 935.6878991646174\n",
      "l1 norm: 784.0751202386515\n",
      "Rbeta: 935.9788854847938\n",
      "\n",
      "Train set: Avg. loss: 0.000134773, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.312256 97.67184\n",
      "l2 norm: 935.5484583436601\n",
      "l1 norm: 783.9606237984087\n",
      "Rbeta: 935.8390740040588\n",
      "\n",
      "Train set: Avg. loss: 0.000134178, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.362885 97.72189\n",
      "l2 norm: 935.4095129541846\n",
      "l1 norm: 783.8465350829839\n",
      "Rbeta: 935.699604369304\n",
      "\n",
      "Train set: Avg. loss: 0.000133587, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.41339 97.77184\n",
      "l2 norm: 935.2624501297468\n",
      "l1 norm: 783.7255419748635\n",
      "Rbeta: 935.5522512107096\n",
      "\n",
      "Train set: Avg. loss: 0.000133000, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.46391 97.82155\n",
      "l2 norm: 935.1210592132427\n",
      "l1 norm: 783.6092744017034\n",
      "Rbeta: 935.4104488020185\n",
      "\n",
      "Train set: Avg. loss: 0.000132416, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.514435 97.871056\n",
      "l2 norm: 934.9797928271676\n",
      "l1 norm: 783.4931080816493\n",
      "Rbeta: 935.268692174063\n",
      "\n",
      "Train set: Avg. loss: 0.000131835, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.56499 97.920494\n",
      "l2 norm: 934.8508863712193\n",
      "l1 norm: 783.3873234103219\n",
      "Rbeta: 935.1393497702535\n",
      "\n",
      "Train set: Avg. loss: 0.000131265, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.61437 97.969696\n",
      "l2 norm: 934.72491253076\n",
      "l1 norm: 783.2840132883659\n",
      "Rbeta: 935.0130635036336\n",
      "\n",
      "Train set: Avg. loss: 0.000130699, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.66371 98.01871\n",
      "l2 norm: 934.5992143769934\n",
      "l1 norm: 783.1810399662704\n",
      "Rbeta: 934.8870349471696\n",
      "\n",
      "Train set: Avg. loss: 0.000130137, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.71306 98.067566\n",
      "l2 norm: 934.4802967447666\n",
      "l1 norm: 783.0838031094032\n",
      "Rbeta: 934.7677646608686\n",
      "\n",
      "Train set: Avg. loss: 0.000129580, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.761986 98.116196\n",
      "l2 norm: 934.3609438699336\n",
      "l1 norm: 782.9861802072344\n",
      "Rbeta: 934.6480287631216\n",
      "\n",
      "Train set: Avg. loss: 0.000129027, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.81078 98.16462\n",
      "l2 norm: 934.2373442745467\n",
      "l1 norm: 782.8849801838376\n",
      "Rbeta: 934.5241059793361\n",
      "\n",
      "Train set: Avg. loss: 0.000128478, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.85959 98.21289\n",
      "l2 norm: 934.1233977278478\n",
      "l1 norm: 782.7917910288645\n",
      "Rbeta: 934.4097596776235\n",
      "\n",
      "Train set: Avg. loss: 0.000127934, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.90807 98.260994\n",
      "l2 norm: 933.990953277628\n",
      "l1 norm: 782.6831540557041\n",
      "Rbeta: 934.2770060706554\n",
      "\n",
      "Train set: Avg. loss: 0.000127398, Accuracy: 512/512 (100%)\n",
      "\n",
      "93.95567 98.30888\n",
      "l2 norm: 933.8578893871301\n",
      "l1 norm: 782.5739244161479\n",
      "Rbeta: 934.1436845844879\n",
      "\n",
      "Train set: Avg. loss: 0.000126866, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.00316 98.35664\n",
      "l2 norm: 933.7288301397898\n",
      "l1 norm: 782.4679320333261\n",
      "Rbeta: 934.01434884992\n",
      "\n",
      "Train set: Avg. loss: 0.000126338, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.05049 98.40433\n",
      "l2 norm: 933.6028578453187\n",
      "l1 norm: 782.364412471974\n",
      "Rbeta: 933.8881178770322\n",
      "\n",
      "Train set: Avg. loss: 0.000125812, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.09782 98.45181\n",
      "l2 norm: 933.4803679476365\n",
      "l1 norm: 782.263847521381\n",
      "Rbeta: 933.7653280741382\n",
      "\n",
      "Train set: Avg. loss: 0.000125291, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.14515 98.49907\n",
      "l2 norm: 933.365562145475\n",
      "l1 norm: 782.1697309974043\n",
      "Rbeta: 933.6502931239845\n",
      "\n",
      "Train set: Avg. loss: 0.000124775, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.191925 98.54616\n",
      "l2 norm: 933.2362636700988\n",
      "l1 norm: 782.0635296955588\n",
      "Rbeta: 933.5206609399085\n",
      "\n",
      "Train set: Avg. loss: 0.000124263, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.23867 98.59297\n",
      "l2 norm: 933.1174482644287\n",
      "l1 norm: 781.9661299507413\n",
      "Rbeta: 933.4014954952512\n",
      "\n",
      "Train set: Avg. loss: 0.000123754, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.28542 98.63966\n",
      "l2 norm: 932.9932827900038\n",
      "l1 norm: 781.8643339630615\n",
      "Rbeta: 933.2771154771747\n",
      "\n",
      "Train set: Avg. loss: 0.000123248, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.33209 98.6862\n",
      "l2 norm: 932.8730214584709\n",
      "l1 norm: 781.765725821286\n",
      "Rbeta: 933.15655539954\n",
      "\n",
      "Train set: Avg. loss: 0.000122746, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.37868 98.73259\n",
      "l2 norm: 932.7515035641505\n",
      "l1 norm: 781.6658956949027\n",
      "Rbeta: 933.0347616972068\n",
      "\n",
      "Train set: Avg. loss: 0.000122246, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.42526 98.77876\n",
      "l2 norm: 932.626600804776\n",
      "l1 norm: 781.56323927772\n",
      "Rbeta: 932.9094675717664\n",
      "\n",
      "Train set: Avg. loss: 0.000121750, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.47187 98.824776\n",
      "l2 norm: 932.5090571637282\n",
      "l1 norm: 781.4667869417997\n",
      "Rbeta: 932.7915794536045\n",
      "\n",
      "Train set: Avg. loss: 0.000121256, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.518486 98.87069\n",
      "l2 norm: 932.3913706045636\n",
      "l1 norm: 781.3701619182339\n",
      "Rbeta: 932.6735265189859\n",
      "\n",
      "Train set: Avg. loss: 0.000120764, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.56512 98.916435\n",
      "l2 norm: 932.274757242586\n",
      "l1 norm: 781.2744290541848\n",
      "Rbeta: 932.556555321712\n",
      "\n",
      "Train set: Avg. loss: 0.000120276, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.61175 98.96205\n",
      "l2 norm: 932.1572741231267\n",
      "l1 norm: 781.178031265596\n",
      "Rbeta: 932.438661975804\n",
      "\n",
      "Train set: Avg. loss: 0.000119793, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.65793 99.007484\n",
      "l2 norm: 932.0347494530706\n",
      "l1 norm: 781.0775141034155\n",
      "Rbeta: 932.3157588088604\n",
      "\n",
      "Train set: Avg. loss: 0.000119316, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.70346 99.052734\n",
      "l2 norm: 931.9189866977556\n",
      "l1 norm: 780.9825215009181\n",
      "Rbeta: 932.1997329025668\n",
      "\n",
      "Train set: Avg. loss: 0.000118842, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.748886 99.0978\n",
      "l2 norm: 931.7987421462017\n",
      "l1 norm: 780.8837532804232\n",
      "Rbeta: 932.0791047338399\n",
      "\n",
      "Train set: Avg. loss: 0.000118371, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.79433 99.14275\n",
      "l2 norm: 931.6769045081958\n",
      "l1 norm: 780.7837088116383\n",
      "Rbeta: 931.956979120704\n",
      "\n",
      "Train set: Avg. loss: 0.000117903, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.839775 99.18758\n",
      "l2 norm: 931.5477229844976\n",
      "l1 norm: 780.6774215307905\n",
      "Rbeta: 931.8274777064813\n",
      "\n",
      "Train set: Avg. loss: 0.000117440, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.884735 99.23221\n",
      "l2 norm: 931.4325646812406\n",
      "l1 norm: 780.5829292022432\n",
      "Rbeta: 931.7120358226898\n",
      "\n",
      "Train set: Avg. loss: 0.000116983, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.92886 99.2767\n",
      "l2 norm: 931.3133798618461\n",
      "l1 norm: 780.4850964269697\n",
      "Rbeta: 931.5925772323378\n",
      "\n",
      "Train set: Avg. loss: 0.000116529, Accuracy: 512/512 (100%)\n",
      "\n",
      "94.97296 99.32104\n",
      "l2 norm: 931.1951600832207\n",
      "l1 norm: 780.3880309575811\n",
      "Rbeta: 931.4741770299927\n",
      "\n",
      "Train set: Avg. loss: 0.000116081, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.01651 99.36525\n",
      "l2 norm: 931.0852169660803\n",
      "l1 norm: 780.2977481678704\n",
      "Rbeta: 931.363995539359\n",
      "\n",
      "Train set: Avg. loss: 0.000115637, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.05968 99.4093\n",
      "l2 norm: 930.9698531158352\n",
      "l1 norm: 780.2029646225035\n",
      "Rbeta: 931.2484582293724\n",
      "\n",
      "Train set: Avg. loss: 0.000115197, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.10255 99.453125\n",
      "l2 norm: 930.8488547631723\n",
      "l1 norm: 780.103618184396\n",
      "Rbeta: 931.1273097647701\n",
      "\n",
      "Train set: Avg. loss: 0.000114760, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.145355 99.49676\n",
      "l2 norm: 930.7458668994592\n",
      "l1 norm: 780.0194072449518\n",
      "Rbeta: 931.0242507860156\n",
      "\n",
      "Train set: Avg. loss: 0.000114326, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.188156 99.54024\n",
      "l2 norm: 930.6380276599596\n",
      "l1 norm: 779.9310831100788\n",
      "Rbeta: 930.9161202485919\n",
      "\n",
      "Train set: Avg. loss: 0.000113894, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.23098 99.583595\n",
      "l2 norm: 930.5347853358089\n",
      "l1 norm: 779.8465614752579\n",
      "Rbeta: 930.8127096970651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000113464, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.2738 99.626785\n",
      "l2 norm: 930.4395298976041\n",
      "l1 norm: 779.7686626672576\n",
      "Rbeta: 930.7172215983232\n",
      "\n",
      "Train set: Avg. loss: 0.000113037, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.31665 99.669846\n",
      "l2 norm: 930.3358099282418\n",
      "l1 norm: 779.6836957539889\n",
      "Rbeta: 930.6133191702476\n",
      "\n",
      "Train set: Avg. loss: 0.000112611, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.3595 99.71275\n",
      "l2 norm: 930.2309781050168\n",
      "l1 norm: 779.5977346600853\n",
      "Rbeta: 930.5082083049518\n",
      "\n",
      "Train set: Avg. loss: 0.000112189, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.40224 99.75552\n",
      "l2 norm: 930.1245214934668\n",
      "l1 norm: 779.5105299706743\n",
      "Rbeta: 930.4015208304326\n",
      "\n",
      "Train set: Avg. loss: 0.000111771, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.44453 99.798134\n",
      "l2 norm: 930.0060660826105\n",
      "l1 norm: 779.4133025509896\n",
      "Rbeta: 930.2827958880299\n",
      "\n",
      "Train set: Avg. loss: 0.000111356, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.48673 99.84064\n",
      "l2 norm: 929.8965933345779\n",
      "l1 norm: 779.3236631527831\n",
      "Rbeta: 930.1730946019621\n",
      "\n",
      "Train set: Avg. loss: 0.000110945, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.52845 99.883026\n",
      "l2 norm: 929.7883939346353\n",
      "l1 norm: 779.2349612741061\n",
      "Rbeta: 930.0647055969947\n",
      "\n",
      "Train set: Avg. loss: 0.000110539, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.569626 99.92527\n",
      "l2 norm: 929.6855831465728\n",
      "l1 norm: 779.1507503671628\n",
      "Rbeta: 929.961876743044\n",
      "\n",
      "Train set: Avg. loss: 0.000110136, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.6108 99.96736\n",
      "l2 norm: 929.5695494346874\n",
      "l1 norm: 779.05550220245\n",
      "Rbeta: 929.8456395751666\n",
      "\n",
      "Train set: Avg. loss: 0.000109734, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.651985 100.00932\n",
      "l2 norm: 929.4590637689056\n",
      "l1 norm: 778.9648427672029\n",
      "Rbeta: 929.7350200998969\n",
      "\n",
      "Train set: Avg. loss: 0.000109335, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.693184 100.05109\n",
      "l2 norm: 929.3537387576336\n",
      "l1 norm: 778.8784094119658\n",
      "Rbeta: 929.6294273046697\n",
      "\n",
      "Train set: Avg. loss: 0.000108937, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.73439 100.092705\n",
      "l2 norm: 929.247734056444\n",
      "l1 norm: 778.7914217626322\n",
      "Rbeta: 929.5232369963129\n",
      "\n",
      "Train set: Avg. loss: 0.000108542, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.77556 100.13421\n",
      "l2 norm: 929.1432663468798\n",
      "l1 norm: 778.7056919325895\n",
      "Rbeta: 929.418605011203\n",
      "\n",
      "Train set: Avg. loss: 0.000108152, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.816154 100.17561\n",
      "l2 norm: 929.040221100969\n",
      "l1 norm: 778.6211613400881\n",
      "Rbeta: 929.3153663221556\n",
      "\n",
      "Train set: Avg. loss: 0.000107764, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.856766 100.21689\n",
      "l2 norm: 928.9401448659172\n",
      "l1 norm: 778.53913537413\n",
      "Rbeta: 929.2152085237967\n",
      "\n",
      "Train set: Avg. loss: 0.000107377, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.897385 100.25806\n",
      "l2 norm: 928.8493704059765\n",
      "l1 norm: 778.4648486251456\n",
      "Rbeta: 929.1242244810322\n",
      "\n",
      "Train set: Avg. loss: 0.000106992, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.938 100.29915\n",
      "l2 norm: 928.7515094235475\n",
      "l1 norm: 778.384661206077\n",
      "Rbeta: 929.0261568513749\n",
      "\n",
      "Train set: Avg. loss: 0.000106609, Accuracy: 512/512 (100%)\n",
      "\n",
      "95.978645 100.34009\n",
      "l2 norm: 928.6524619291663\n",
      "l1 norm: 778.3034528400126\n",
      "Rbeta: 928.9268867548706\n",
      "\n",
      "Train set: Avg. loss: 0.000106229, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.01929 100.380875\n",
      "l2 norm: 928.5471829585946\n",
      "l1 norm: 778.2170669465498\n",
      "Rbeta: 928.8214416790366\n",
      "\n",
      "Train set: Avg. loss: 0.000105850, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.05992 100.42154\n",
      "l2 norm: 928.4471620788084\n",
      "l1 norm: 778.135086611305\n",
      "Rbeta: 928.7211497629993\n",
      "\n",
      "Train set: Avg. loss: 0.000105472, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.10058 100.46216\n",
      "l2 norm: 928.3512919758858\n",
      "l1 norm: 778.0566254216703\n",
      "Rbeta: 928.6250742169191\n",
      "\n",
      "Train set: Avg. loss: 0.000105097, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.14124 100.50267\n",
      "l2 norm: 928.2527774899686\n",
      "l1 norm: 777.9759316421926\n",
      "Rbeta: 928.5263722932976\n",
      "\n",
      "Train set: Avg. loss: 0.000104724, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.181915 100.54297\n",
      "l2 norm: 928.151387833581\n",
      "l1 norm: 777.8928324507601\n",
      "Rbeta: 928.4246752830285\n",
      "\n",
      "Train set: Avg. loss: 0.000104352, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.22258 100.58316\n",
      "l2 norm: 928.0430620194592\n",
      "l1 norm: 777.8040269821412\n",
      "Rbeta: 928.3159901704317\n",
      "\n",
      "Train set: Avg. loss: 0.000103982, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.26327 100.62326\n",
      "l2 norm: 927.9378441743498\n",
      "l1 norm: 777.7177659040655\n",
      "Rbeta: 928.210522766281\n",
      "\n",
      "Train set: Avg. loss: 0.000103614, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.30397 100.66325\n",
      "l2 norm: 927.8314062331968\n",
      "l1 norm: 777.6304335833194\n",
      "Rbeta: 928.1038153707498\n",
      "\n",
      "Train set: Avg. loss: 0.000103252, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.34387 100.703064\n",
      "l2 norm: 927.730561227511\n",
      "l1 norm: 777.5476685649675\n",
      "Rbeta: 928.0027150382507\n",
      "\n",
      "Train set: Avg. loss: 0.000102894, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.38333 100.74277\n",
      "l2 norm: 927.6279330427843\n",
      "l1 norm: 777.46355110068\n",
      "Rbeta: 927.8998486900452\n",
      "\n",
      "Train set: Avg. loss: 0.000102537, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.422806 100.782364\n",
      "l2 norm: 927.5387615162484\n",
      "l1 norm: 777.3906540520913\n",
      "Rbeta: 927.8105115564305\n",
      "\n",
      "Train set: Avg. loss: 0.000102182, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.46229 100.821785\n",
      "l2 norm: 927.4442432117148\n",
      "l1 norm: 777.3133195585607\n",
      "Rbeta: 927.71572841168\n",
      "\n",
      "Train set: Avg. loss: 0.000101829, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.50178 100.86106\n",
      "l2 norm: 927.3467807407496\n",
      "l1 norm: 777.2334417355594\n",
      "Rbeta: 927.6180561168196\n",
      "\n",
      "Train set: Avg. loss: 0.000101478, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.541275 100.90016\n",
      "l2 norm: 927.2501419147052\n",
      "l1 norm: 777.154170179472\n",
      "Rbeta: 927.5211222035824\n",
      "\n",
      "Train set: Avg. loss: 0.000101129, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.58077 100.93914\n",
      "l2 norm: 927.154824904162\n",
      "l1 norm: 777.0760332903486\n",
      "Rbeta: 927.4255293782468\n",
      "\n",
      "Train set: Avg. loss: 0.000100782, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.62019 100.97795\n",
      "l2 norm: 927.0610952824445\n",
      "l1 norm: 776.9992184577709\n",
      "Rbeta: 927.3315452338038\n",
      "\n",
      "Train set: Avg. loss: 0.000100439, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.65908 101.01662\n",
      "l2 norm: 926.9743400416656\n",
      "l1 norm: 776.9282277723862\n",
      "Rbeta: 927.2444794148971\n",
      "\n",
      "Train set: Avg. loss: 0.000100097, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.69797 101.05521\n",
      "l2 norm: 926.8771293912152\n",
      "l1 norm: 776.8484642892672\n",
      "Rbeta: 927.1470425763457\n",
      "\n",
      "Train set: Avg. loss: 0.000099757, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.73688 101.09375\n",
      "l2 norm: 926.7757869195847\n",
      "l1 norm: 776.765251138554\n",
      "Rbeta: 927.0454812831088\n",
      "\n",
      "Train set: Avg. loss: 0.000099419, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.77579 101.13219\n",
      "l2 norm: 926.67480392952\n",
      "l1 norm: 776.6823274310782\n",
      "Rbeta: 926.9442330654128\n",
      "\n",
      "Train set: Avg. loss: 0.000099084, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.81416 101.17051\n",
      "l2 norm: 926.5823513798537\n",
      "l1 norm: 776.6065368071277\n",
      "Rbeta: 926.851594524719\n",
      "\n",
      "Train set: Avg. loss: 0.000098754, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.85193 101.2087\n",
      "l2 norm: 926.4943615874713\n",
      "l1 norm: 776.5345550255635\n",
      "Rbeta: 926.7634311631405\n",
      "\n",
      "Train set: Avg. loss: 0.000098426, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.8897 101.246796\n",
      "l2 norm: 926.3946609398284\n",
      "l1 norm: 776.4527343399042\n",
      "Rbeta: 926.6635576975211\n",
      "\n",
      "Train set: Avg. loss: 0.000098099, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.92747 101.284775\n",
      "l2 norm: 926.2969458349904\n",
      "l1 norm: 776.3724486930879\n",
      "Rbeta: 926.5655996001678\n",
      "\n",
      "Train set: Avg. loss: 0.000097773, Accuracy: 512/512 (100%)\n",
      "\n",
      "96.96524 101.32263\n",
      "l2 norm: 926.2031202444514\n",
      "l1 norm: 776.29546527872\n",
      "Rbeta: 926.4715529389485\n",
      "\n",
      "Train set: Avg. loss: 0.000097449, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.00302 101.36043\n",
      "l2 norm: 926.1111983866618\n",
      "l1 norm: 776.2201192288858\n",
      "Rbeta: 926.3794462062721\n",
      "\n",
      "Train set: Avg. loss: 0.000097128, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.04033 101.39817\n",
      "l2 norm: 926.0167200317047\n",
      "l1 norm: 776.1426501036922\n",
      "Rbeta: 926.2848129506706\n",
      "\n",
      "Train set: Avg. loss: 0.000096809, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.077545 101.43582\n",
      "l2 norm: 925.9279608177407\n",
      "l1 norm: 776.0699408974696\n",
      "Rbeta: 926.1958486812805\n",
      "\n",
      "Train set: Avg. loss: 0.000096492, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.11476 101.47333\n",
      "l2 norm: 925.8341823300158\n",
      "l1 norm: 775.9930318790491\n",
      "Rbeta: 926.1018875854317\n",
      "\n",
      "Train set: Avg. loss: 0.000096176, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.15199 101.51073\n",
      "l2 norm: 925.7367898959785\n",
      "l1 norm: 775.9131472686217\n",
      "Rbeta: 926.0043092583626\n",
      "\n",
      "Train set: Avg. loss: 0.000095861, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.189224 101.54806\n",
      "l2 norm: 925.6478321000214\n",
      "l1 norm: 775.8402460207915\n",
      "Rbeta: 925.9151471696065\n",
      "\n",
      "Train set: Avg. loss: 0.000095548, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.226456 101.58533\n",
      "l2 norm: 925.5555924167217\n",
      "l1 norm: 775.7645319191423\n",
      "Rbeta: 925.8227079560463\n",
      "\n",
      "Train set: Avg. loss: 0.000095236, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.26371 101.62245\n",
      "l2 norm: 925.4642124259619\n",
      "l1 norm: 775.6895125053554\n",
      "Rbeta: 925.7311366073374\n",
      "\n",
      "Train set: Avg. loss: 0.000094926, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.300964 101.65944\n",
      "l2 norm: 925.3749629350834\n",
      "l1 norm: 775.6162693720114\n",
      "Rbeta: 925.6416371037359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000094620, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.33766 101.69629\n",
      "l2 norm: 925.2904025795206\n",
      "l1 norm: 775.5469390560673\n",
      "Rbeta: 925.5569172148579\n",
      "\n",
      "Train set: Avg. loss: 0.000094315, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.374344 101.73303\n",
      "l2 norm: 925.2052291034598\n",
      "l1 norm: 775.4772193027212\n",
      "Rbeta: 925.4715157081308\n",
      "\n",
      "Train set: Avg. loss: 0.000094012, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.41103 101.76965\n",
      "l2 norm: 925.1154268376939\n",
      "l1 norm: 775.4035936901291\n",
      "Rbeta: 925.3815324813532\n",
      "\n",
      "Train set: Avg. loss: 0.000093710, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.44774 101.806145\n",
      "l2 norm: 925.0344801876471\n",
      "l1 norm: 775.3372677271904\n",
      "Rbeta: 925.3002793816044\n",
      "\n",
      "Train set: Avg. loss: 0.000093410, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.48445 101.84259\n",
      "l2 norm: 924.9551558599794\n",
      "l1 norm: 775.2722900620563\n",
      "Rbeta: 925.220757225657\n",
      "\n",
      "Train set: Avg. loss: 0.000093110, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.52116 101.878914\n",
      "l2 norm: 924.868911792569\n",
      "l1 norm: 775.201481866806\n",
      "Rbeta: 925.1342739415393\n",
      "\n",
      "Train set: Avg. loss: 0.000092813, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.55786 101.91507\n",
      "l2 norm: 924.7846784383476\n",
      "l1 norm: 775.1323706706128\n",
      "Rbeta: 925.0497765635798\n",
      "\n",
      "Train set: Avg. loss: 0.000092519, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.594 101.95117\n",
      "l2 norm: 924.7024787742376\n",
      "l1 norm: 775.0649823810221\n",
      "Rbeta: 924.9673758733961\n",
      "\n",
      "Train set: Avg. loss: 0.000092226, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.6301 101.987175\n",
      "l2 norm: 924.6203810647834\n",
      "l1 norm: 774.9977148074631\n",
      "Rbeta: 924.8851224487724\n",
      "\n",
      "Train set: Avg. loss: 0.000091935, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.6662 102.02313\n",
      "l2 norm: 924.5350871455857\n",
      "l1 norm: 774.9277658924464\n",
      "Rbeta: 924.7995331602651\n",
      "\n",
      "Train set: Avg. loss: 0.000091644, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.70232 102.059\n",
      "l2 norm: 924.4358093907936\n",
      "l1 norm: 774.8461228301464\n",
      "Rbeta: 924.700088960724\n",
      "\n",
      "Train set: Avg. loss: 0.000091356, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.738434 102.09475\n",
      "l2 norm: 924.3462297802884\n",
      "l1 norm: 774.7725234291175\n",
      "Rbeta: 924.610308870643\n",
      "\n",
      "Train set: Avg. loss: 0.000091068, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.77457 102.13033\n",
      "l2 norm: 924.2658111303325\n",
      "l1 norm: 774.7066755814561\n",
      "Rbeta: 924.5296532773775\n",
      "\n",
      "Train set: Avg. loss: 0.000090782, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.81069 102.16585\n",
      "l2 norm: 924.1906314530797\n",
      "l1 norm: 774.6451955270315\n",
      "Rbeta: 924.4541229184848\n",
      "\n",
      "Train set: Avg. loss: 0.000090497, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.846825 102.20123\n",
      "l2 norm: 924.1013761961143\n",
      "l1 norm: 774.5718966450621\n",
      "Rbeta: 924.3646154422408\n",
      "\n",
      "Train set: Avg. loss: 0.000090216, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.8825 102.23648\n",
      "l2 norm: 924.0125274127159\n",
      "l1 norm: 774.4988780599376\n",
      "Rbeta: 924.2755508468423\n",
      "\n",
      "Train set: Avg. loss: 0.000089936, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.91802 102.27162\n",
      "l2 norm: 923.9241908442664\n",
      "l1 norm: 774.4263500203751\n",
      "Rbeta: 924.1868879441292\n",
      "\n",
      "Train set: Avg. loss: 0.000089657, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.95355 102.30672\n",
      "l2 norm: 923.8303137510113\n",
      "l1 norm: 774.3491392560489\n",
      "Rbeta: 924.0928331306036\n",
      "\n",
      "Train set: Avg. loss: 0.000089380, Accuracy: 512/512 (100%)\n",
      "\n",
      "97.98906 102.34178\n",
      "l2 norm: 923.7427337302028\n",
      "l1 norm: 774.2771771714526\n",
      "Rbeta: 924.004996399233\n",
      "\n",
      "Train set: Avg. loss: 0.000089107, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.02368 102.376755\n",
      "l2 norm: 923.6732939822089\n",
      "l1 norm: 774.2203908378658\n",
      "Rbeta: 923.9353960533612\n",
      "\n",
      "Train set: Avg. loss: 0.000088836, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.058014 102.41162\n",
      "l2 norm: 923.6003837877631\n",
      "l1 norm: 774.1606403456685\n",
      "Rbeta: 923.8623766533265\n",
      "\n",
      "Train set: Avg. loss: 0.000088567, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.09234 102.44638\n",
      "l2 norm: 923.512159332621\n",
      "l1 norm: 774.0880876975427\n",
      "Rbeta: 923.7740257040949\n",
      "\n",
      "Train set: Avg. loss: 0.000088299, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.12666 102.481094\n",
      "l2 norm: 923.4265949162585\n",
      "l1 norm: 774.0178068235965\n",
      "Rbeta: 923.6882599854297\n",
      "\n",
      "Train set: Avg. loss: 0.000088032, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.161 102.51573\n",
      "l2 norm: 923.3384519308105\n",
      "l1 norm: 773.9453892884158\n",
      "Rbeta: 923.5999446135779\n",
      "\n",
      "Train set: Avg. loss: 0.000087768, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.19487 102.55026\n",
      "l2 norm: 923.2565099632861\n",
      "l1 norm: 773.8782027891431\n",
      "Rbeta: 923.5179093929805\n",
      "\n",
      "Train set: Avg. loss: 0.000087505, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.22864 102.58469\n",
      "l2 norm: 923.1833374118455\n",
      "l1 norm: 773.818319103249\n",
      "Rbeta: 923.4446436341485\n",
      "\n",
      "Train set: Avg. loss: 0.000087245, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.26208 102.61897\n",
      "l2 norm: 923.1075188901095\n",
      "l1 norm: 773.7562135774733\n",
      "Rbeta: 923.36869572317\n",
      "\n",
      "Train set: Avg. loss: 0.000086987, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.29522 102.65315\n",
      "l2 norm: 923.0300544320859\n",
      "l1 norm: 773.692762745481\n",
      "Rbeta: 923.2911150711785\n",
      "\n",
      "Train set: Avg. loss: 0.000086731, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.32837 102.687195\n",
      "l2 norm: 922.9490135320732\n",
      "l1 norm: 773.6263506699211\n",
      "Rbeta: 923.210006036143\n",
      "\n",
      "Train set: Avg. loss: 0.000086476, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.36153 102.721176\n",
      "l2 norm: 922.8809221051067\n",
      "l1 norm: 773.5707667238285\n",
      "Rbeta: 923.1418205301158\n",
      "\n",
      "Train set: Avg. loss: 0.000086221, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.394684 102.75507\n",
      "l2 norm: 922.8158009473557\n",
      "l1 norm: 773.517694358977\n",
      "Rbeta: 923.0765527650451\n",
      "\n",
      "Train set: Avg. loss: 0.000085968, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.42786 102.78888\n",
      "l2 norm: 922.7528654650276\n",
      "l1 norm: 773.4663686268551\n",
      "Rbeta: 923.0135813982992\n",
      "\n",
      "Train set: Avg. loss: 0.000085716, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.46103 102.82257\n",
      "l2 norm: 922.6793594835191\n",
      "l1 norm: 773.406180561521\n",
      "Rbeta: 922.9399479151898\n",
      "\n",
      "Train set: Avg. loss: 0.000085465, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.4942 102.856186\n",
      "l2 norm: 922.6076466692357\n",
      "l1 norm: 773.3475150685489\n",
      "Rbeta: 922.8680423408636\n",
      "\n",
      "Train set: Avg. loss: 0.000085215, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.52739 102.88971\n",
      "l2 norm: 922.5327696552692\n",
      "l1 norm: 773.2862212846744\n",
      "Rbeta: 922.7930741464425\n",
      "\n",
      "Train set: Avg. loss: 0.000084966, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.56058 102.92317\n",
      "l2 norm: 922.458851169069\n",
      "l1 norm: 773.2256428341136\n",
      "Rbeta: 922.7189586979001\n",
      "\n",
      "Train set: Avg. loss: 0.000084717, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.59377 102.95663\n",
      "l2 norm: 922.3834989539367\n",
      "l1 norm: 773.163797951282\n",
      "Rbeta: 922.6434566128694\n",
      "\n",
      "Train set: Avg. loss: 0.000084470, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.62697 102.99\n",
      "l2 norm: 922.2984107241596\n",
      "l1 norm: 773.0938038072009\n",
      "Rbeta: 922.5582319330541\n",
      "\n",
      "Train set: Avg. loss: 0.000084223, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.66017 103.02328\n",
      "l2 norm: 922.2185394583787\n",
      "l1 norm: 773.028180444069\n",
      "Rbeta: 922.4781234664835\n",
      "\n",
      "Train set: Avg. loss: 0.000083978, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.69339 103.05643\n",
      "l2 norm: 922.134828644534\n",
      "l1 norm: 772.9593319339076\n",
      "Rbeta: 922.3942696996107\n",
      "\n",
      "Train set: Avg. loss: 0.000083734, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.72661 103.08954\n",
      "l2 norm: 922.0512905537804\n",
      "l1 norm: 772.8907074887442\n",
      "Rbeta: 922.3105283523109\n",
      "\n",
      "Train set: Avg. loss: 0.000083490, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.75983 103.12262\n",
      "l2 norm: 921.9628838559628\n",
      "l1 norm: 772.8179726297302\n",
      "Rbeta: 922.221946825332\n",
      "\n",
      "Train set: Avg. loss: 0.000083248, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.79306 103.15558\n",
      "l2 norm: 921.8748602775082\n",
      "l1 norm: 772.7455403351278\n",
      "Rbeta: 922.1337056557352\n",
      "\n",
      "Train set: Avg. loss: 0.000083007, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.826294 103.188446\n",
      "l2 norm: 921.7967589250288\n",
      "l1 norm: 772.6814334731928\n",
      "Rbeta: 922.0554059753388\n",
      "\n",
      "Train set: Avg. loss: 0.000082767, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.85932 103.22122\n",
      "l2 norm: 921.720706284021\n",
      "l1 norm: 772.6189967298537\n",
      "Rbeta: 921.9791005577875\n",
      "\n",
      "Train set: Avg. loss: 0.000082532, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.89155 103.25389\n",
      "l2 norm: 921.644796153881\n",
      "l1 norm: 772.556626533546\n",
      "Rbeta: 921.9031441646185\n",
      "\n",
      "Train set: Avg. loss: 0.000082298, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.923615 103.28642\n",
      "l2 norm: 921.5685979338748\n",
      "l1 norm: 772.4939671068987\n",
      "Rbeta: 921.8267275616431\n",
      "\n",
      "Train set: Avg. loss: 0.000082065, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.95569 103.31889\n",
      "l2 norm: 921.4924819183065\n",
      "l1 norm: 772.4313618869096\n",
      "Rbeta: 921.7505393575514\n",
      "\n",
      "Train set: Avg. loss: 0.000081832, Accuracy: 512/512 (100%)\n",
      "\n",
      "98.987755 103.35134\n",
      "l2 norm: 921.4127889422612\n",
      "l1 norm: 772.3657785786905\n",
      "Rbeta: 921.6707074366889\n",
      "\n",
      "Train set: Avg. loss: 0.000081601, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.01984 103.383766\n",
      "l2 norm: 921.3331743861139\n",
      "l1 norm: 772.3003494656475\n",
      "Rbeta: 921.590933272722\n",
      "\n",
      "Train set: Avg. loss: 0.000081371, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.05173 103.416176\n",
      "l2 norm: 921.2491286405615\n",
      "l1 norm: 772.231238824092\n",
      "Rbeta: 921.5067585393422\n",
      "\n",
      "Train set: Avg. loss: 0.000081143, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.083176 103.44848\n",
      "l2 norm: 921.1726051028793\n",
      "l1 norm: 772.1684992329178\n",
      "Rbeta: 921.4300856695961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000080917, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.11463 103.480675\n",
      "l2 norm: 921.1087453898031\n",
      "l1 norm: 772.1164051241967\n",
      "Rbeta: 921.3661694231421\n",
      "\n",
      "Train set: Avg. loss: 0.000080691, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.1461 103.512695\n",
      "l2 norm: 921.0358445190138\n",
      "l1 norm: 772.0567143705546\n",
      "Rbeta: 921.2931762045966\n",
      "\n",
      "Train set: Avg. loss: 0.000080467, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.17757 103.54456\n",
      "l2 norm: 920.9582892228218\n",
      "l1 norm: 771.9931686192572\n",
      "Rbeta: 921.2154921841825\n",
      "\n",
      "Train set: Avg. loss: 0.000080246, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.20854 103.57631\n",
      "l2 norm: 920.8782944963211\n",
      "l1 norm: 771.9275345323812\n",
      "Rbeta: 921.1354644363745\n",
      "\n",
      "Train set: Avg. loss: 0.000080026, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.23938 103.60796\n",
      "l2 norm: 920.8009000598555\n",
      "l1 norm: 771.8640316117845\n",
      "Rbeta: 921.0579576450924\n",
      "\n",
      "Train set: Avg. loss: 0.000079806, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.27023 103.63955\n",
      "l2 norm: 920.7338241544387\n",
      "l1 norm: 771.809209638225\n",
      "Rbeta: 920.9908240077434\n",
      "\n",
      "Train set: Avg. loss: 0.000079588, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.30109 103.67111\n",
      "l2 norm: 920.674482932943\n",
      "l1 norm: 771.7608060467107\n",
      "Rbeta: 920.9313236762845\n",
      "\n",
      "Train set: Avg. loss: 0.000079370, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.33195 103.70264\n",
      "l2 norm: 920.6120498375686\n",
      "l1 norm: 771.7097901017092\n",
      "Rbeta: 920.8687418252185\n",
      "\n",
      "Train set: Avg. loss: 0.000079153, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.36281 103.73404\n",
      "l2 norm: 920.5402303963165\n",
      "l1 norm: 771.6508621515391\n",
      "Rbeta: 920.7968268528745\n",
      "\n",
      "Train set: Avg. loss: 0.000078937, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.39368 103.76535\n",
      "l2 norm: 920.463270034191\n",
      "l1 norm: 771.587613531437\n",
      "Rbeta: 920.7197645122156\n",
      "\n",
      "Train set: Avg. loss: 0.000078722, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.42455 103.796555\n",
      "l2 norm: 920.3813675638468\n",
      "l1 norm: 771.5202104110815\n",
      "Rbeta: 920.637679339703\n",
      "\n",
      "Train set: Avg. loss: 0.000078508, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.45544 103.8277\n",
      "l2 norm: 920.3028857469758\n",
      "l1 norm: 771.4556897355303\n",
      "Rbeta: 920.5591235899193\n",
      "\n",
      "Train set: Avg. loss: 0.000078295, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.48632 103.858795\n",
      "l2 norm: 920.2265746462635\n",
      "l1 norm: 771.3929343609109\n",
      "Rbeta: 920.4826501840054\n",
      "\n",
      "Train set: Avg. loss: 0.000078083, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.51689 103.88991\n",
      "l2 norm: 920.1538354564132\n",
      "l1 norm: 771.3330560047197\n",
      "Rbeta: 920.4098200068182\n",
      "\n",
      "Train set: Avg. loss: 0.000077873, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.547195 103.920944\n",
      "l2 norm: 920.0737188804785\n",
      "l1 norm: 771.2669964636343\n",
      "Rbeta: 920.3295720366258\n",
      "\n",
      "Train set: Avg. loss: 0.000077663, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.57751 103.95192\n",
      "l2 norm: 919.9949445573045\n",
      "l1 norm: 771.2020570985595\n",
      "Rbeta: 920.250660494394\n",
      "\n",
      "Train set: Avg. loss: 0.000077455, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.60782 103.982834\n",
      "l2 norm: 919.9188515794203\n",
      "l1 norm: 771.1394528391895\n",
      "Rbeta: 920.1745158013023\n",
      "\n",
      "Train set: Avg. loss: 0.000077247, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.63814 104.01365\n",
      "l2 norm: 919.848947537886\n",
      "l1 norm: 771.0820569830303\n",
      "Rbeta: 920.1045200299941\n",
      "\n",
      "Train set: Avg. loss: 0.000077040, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.668465 104.04435\n",
      "l2 norm: 919.7810929291088\n",
      "l1 norm: 771.0264236706719\n",
      "Rbeta: 920.0365022888632\n",
      "\n",
      "Train set: Avg. loss: 0.000076834, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.69879 104.07498\n",
      "l2 norm: 919.7028263171873\n",
      "l1 norm: 770.9620225740134\n",
      "Rbeta: 919.9581099986573\n",
      "\n",
      "Train set: Avg. loss: 0.000076629, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.72912 104.10556\n",
      "l2 norm: 919.6381357487504\n",
      "l1 norm: 770.9089169814627\n",
      "Rbeta: 919.8933661575409\n",
      "\n",
      "Train set: Avg. loss: 0.000076428, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.75835 104.13608\n",
      "l2 norm: 919.5754800854571\n",
      "l1 norm: 770.8575567053181\n",
      "Rbeta: 919.8306042626972\n",
      "\n",
      "Train set: Avg. loss: 0.000076229, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.787415 104.166504\n",
      "l2 norm: 919.5057173735523\n",
      "l1 norm: 770.8002480449459\n",
      "Rbeta: 919.7608531803418\n",
      "\n",
      "Train set: Avg. loss: 0.000076030, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.81648 104.196815\n",
      "l2 norm: 919.4350095060621\n",
      "l1 norm: 770.7421305237943\n",
      "Rbeta: 919.6901158259557\n",
      "\n",
      "Train set: Avg. loss: 0.000075832, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.84555 104.227066\n",
      "l2 norm: 919.3601556176388\n",
      "l1 norm: 770.6804954978836\n",
      "Rbeta: 919.6151963442308\n",
      "\n",
      "Train set: Avg. loss: 0.000075635, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.87463 104.2572\n",
      "l2 norm: 919.2766393812623\n",
      "l1 norm: 770.6115814005798\n",
      "Rbeta: 919.5316493064576\n",
      "\n",
      "Train set: Avg. loss: 0.000075439, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.90362 104.28726\n",
      "l2 norm: 919.2004233125156\n",
      "l1 norm: 770.5487725317953\n",
      "Rbeta: 919.4553685975718\n",
      "\n",
      "Train set: Avg. loss: 0.000075246, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.93211 104.31729\n",
      "l2 norm: 919.1258441797648\n",
      "l1 norm: 770.4873734612265\n",
      "Rbeta: 919.3807490283145\n",
      "\n",
      "Train set: Avg. loss: 0.000075053, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.96062 104.34726\n",
      "l2 norm: 919.0488912776841\n",
      "l1 norm: 770.4240218010682\n",
      "Rbeta: 919.3038209703974\n",
      "\n",
      "Train set: Avg. loss: 0.000074860, Accuracy: 512/512 (100%)\n",
      "\n",
      "99.98912 104.37717\n",
      "l2 norm: 918.9787820239421\n",
      "l1 norm: 770.3664051834292\n",
      "Rbeta: 919.2336373229468\n",
      "\n",
      "Train set: Avg. loss: 0.000074669, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.017624 104.40701\n",
      "l2 norm: 918.9107288436742\n",
      "l1 norm: 770.3105825802688\n",
      "Rbeta: 919.1655702751318\n",
      "\n",
      "Train set: Avg. loss: 0.000074478, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.04614 104.43675\n",
      "l2 norm: 918.8472920500725\n",
      "l1 norm: 770.258670009795\n",
      "Rbeta: 919.102126880644\n",
      "\n",
      "Train set: Avg. loss: 0.000074288, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.07466 104.46638\n",
      "l2 norm: 918.7842235678094\n",
      "l1 norm: 770.2071919833973\n",
      "Rbeta: 919.0390483811703\n",
      "\n",
      "Train set: Avg. loss: 0.000074100, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.10278 104.495926\n",
      "l2 norm: 918.7131782190804\n",
      "l1 norm: 770.1489881005468\n",
      "Rbeta: 918.9679781145878\n",
      "\n",
      "Train set: Avg. loss: 0.000073913, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.13072 104.525406\n",
      "l2 norm: 918.6521538510775\n",
      "l1 norm: 770.0991715891453\n",
      "Rbeta: 918.906929082019\n",
      "\n",
      "Train set: Avg. loss: 0.000073727, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.15867 104.554825\n",
      "l2 norm: 918.586691803258\n",
      "l1 norm: 770.0455731739556\n",
      "Rbeta: 918.8415379987097\n",
      "\n",
      "Train set: Avg. loss: 0.000073542, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.186615 104.5842\n",
      "l2 norm: 918.5148379217096\n",
      "l1 norm: 769.9866274896189\n",
      "Rbeta: 918.7696533169662\n",
      "\n",
      "Train set: Avg. loss: 0.000073357, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.21456 104.613525\n",
      "l2 norm: 918.4431679055027\n",
      "l1 norm: 769.9278000832585\n",
      "Rbeta: 918.6980516286332\n",
      "\n",
      "Train set: Avg. loss: 0.000073173, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.24251 104.642784\n",
      "l2 norm: 918.3788116851854\n",
      "l1 norm: 769.8750859191634\n",
      "Rbeta: 918.6335783714386\n",
      "\n",
      "Train set: Avg. loss: 0.000072989, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.27045 104.67198\n",
      "l2 norm: 918.3097473881451\n",
      "l1 norm: 769.818397039249\n",
      "Rbeta: 918.5644664657552\n",
      "\n",
      "Train set: Avg. loss: 0.000072806, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.2984 104.701096\n",
      "l2 norm: 918.2442057509561\n",
      "l1 norm: 769.7646801635302\n",
      "Rbeta: 918.4989249904877\n",
      "\n",
      "Train set: Avg. loss: 0.000072625, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.326 104.73012\n",
      "l2 norm: 918.1769394174174\n",
      "l1 norm: 769.7094887752063\n",
      "Rbeta: 918.4316925823993\n",
      "\n",
      "Train set: Avg. loss: 0.000072446, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.35332 104.75905\n",
      "l2 norm: 918.1048484117669\n",
      "l1 norm: 769.650177534239\n",
      "Rbeta: 918.3595517852053\n",
      "\n",
      "Train set: Avg. loss: 0.000072267, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.380646 104.787926\n",
      "l2 norm: 918.0282533620791\n",
      "l1 norm: 769.5870220090492\n",
      "Rbeta: 918.2830381852335\n",
      "\n",
      "Train set: Avg. loss: 0.000072089, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.407974 104.81674\n",
      "l2 norm: 917.9562457009389\n",
      "l1 norm: 769.5276739948499\n",
      "Rbeta: 918.2110128271149\n",
      "\n",
      "Train set: Avg. loss: 0.000071912, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.43531 104.845505\n",
      "l2 norm: 917.8871585144035\n",
      "l1 norm: 769.4707191934355\n",
      "Rbeta: 918.1419612242895\n",
      "\n",
      "Train set: Avg. loss: 0.000071735, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.46265 104.874214\n",
      "l2 norm: 917.8138383025765\n",
      "l1 norm: 769.4102550705572\n",
      "Rbeta: 918.0685797066535\n",
      "\n",
      "Train set: Avg. loss: 0.000071559, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.48999 104.90289\n",
      "l2 norm: 917.7506280430531\n",
      "l1 norm: 769.3583102431248\n",
      "Rbeta: 918.0053990668018\n",
      "\n",
      "Train set: Avg. loss: 0.000071383, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.517334 104.93152\n",
      "l2 norm: 917.6878471832824\n",
      "l1 norm: 769.3067665497688\n",
      "Rbeta: 917.9426000217311\n",
      "\n",
      "Train set: Avg. loss: 0.000071208, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.54469 104.96005\n",
      "l2 norm: 917.6242164060968\n",
      "l1 norm: 769.254418865452\n",
      "Rbeta: 917.8789531944756\n",
      "\n",
      "Train set: Avg. loss: 0.000071033, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.57204 104.988556\n",
      "l2 norm: 917.5555786103248\n",
      "l1 norm: 769.1978443336957\n",
      "Rbeta: 917.8102602534406\n",
      "\n",
      "Train set: Avg. loss: 0.000070859, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.599396 105.017\n",
      "l2 norm: 917.4878299271755\n",
      "l1 norm: 769.1420625087626\n",
      "Rbeta: 917.7425092757376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000070686, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.626755 105.045395\n",
      "l2 norm: 917.430965171254\n",
      "l1 norm: 769.0953690720498\n",
      "Rbeta: 917.6855832715888\n",
      "\n",
      "Train set: Avg. loss: 0.000070513, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.65412 105.07382\n",
      "l2 norm: 917.3761202210422\n",
      "l1 norm: 769.0503743070826\n",
      "Rbeta: 917.6307220036142\n",
      "\n",
      "Train set: Avg. loss: 0.000070341, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.68149 105.10214\n",
      "l2 norm: 917.323362405295\n",
      "l1 norm: 769.0071956866684\n",
      "Rbeta: 917.5779274822779\n",
      "\n",
      "Train set: Avg. loss: 0.000070169, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.70886 105.13043\n",
      "l2 norm: 917.2645637851655\n",
      "l1 norm: 768.9589507502396\n",
      "Rbeta: 917.5191200833507\n",
      "\n",
      "Train set: Avg. loss: 0.000069998, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.736244 105.158676\n",
      "l2 norm: 917.2077611918618\n",
      "l1 norm: 768.9123213829256\n",
      "Rbeta: 917.4623481791054\n",
      "\n",
      "Train set: Avg. loss: 0.000069827, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.763626 105.18685\n",
      "l2 norm: 917.1410697062704\n",
      "l1 norm: 768.8573650319483\n",
      "Rbeta: 917.3955239845617\n",
      "\n",
      "Train set: Avg. loss: 0.000069658, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.79057 105.21498\n",
      "l2 norm: 917.0757446372274\n",
      "l1 norm: 768.8035255043005\n",
      "Rbeta: 917.3302375226508\n",
      "\n",
      "Train set: Avg. loss: 0.000069491, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.81732 105.24303\n",
      "l2 norm: 917.0099268866578\n",
      "l1 norm: 768.7493506892504\n",
      "Rbeta: 917.2643739826397\n",
      "\n",
      "Train set: Avg. loss: 0.000069324, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.84408 105.27103\n",
      "l2 norm: 916.9357234157582\n",
      "l1 norm: 768.6882139927269\n",
      "Rbeta: 917.1902086637987\n",
      "\n",
      "Train set: Avg. loss: 0.000069158, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.870834 105.298935\n",
      "l2 norm: 916.8632362745982\n",
      "l1 norm: 768.6285466753567\n",
      "Rbeta: 917.1176865105442\n",
      "\n",
      "Train set: Avg. loss: 0.000068992, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.89758 105.32678\n",
      "l2 norm: 916.7870154527975\n",
      "l1 norm: 768.5657248242233\n",
      "Rbeta: 917.0414184245276\n",
      "\n",
      "Train set: Avg. loss: 0.000068827, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.924355 105.35457\n",
      "l2 norm: 916.7155929301837\n",
      "l1 norm: 768.5069499780842\n",
      "Rbeta: 916.9700483870027\n",
      "\n",
      "Train set: Avg. loss: 0.000068663, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.951126 105.38226\n",
      "l2 norm: 916.6541960409871\n",
      "l1 norm: 768.4566174262577\n",
      "Rbeta: 916.9085741431346\n",
      "\n",
      "Train set: Avg. loss: 0.000068499, Accuracy: 512/512 (100%)\n",
      "\n",
      "100.97789 105.40988\n",
      "l2 norm: 916.5909622720433\n",
      "l1 norm: 768.40476879681\n",
      "Rbeta: 916.8452236692241\n",
      "\n",
      "Train set: Avg. loss: 0.000068336, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.00467 105.43744\n",
      "l2 norm: 916.5263482705932\n",
      "l1 norm: 768.3518106036034\n",
      "Rbeta: 916.7806200237168\n",
      "\n",
      "Train set: Avg. loss: 0.000068174, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.03111 105.46502\n",
      "l2 norm: 916.463599752935\n",
      "l1 norm: 768.3003699128232\n",
      "Rbeta: 916.7178510027572\n",
      "\n",
      "Train set: Avg. loss: 0.000068013, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.05725 105.49256\n",
      "l2 norm: 916.404715477112\n",
      "l1 norm: 768.2521766883413\n",
      "Rbeta: 916.6589680833406\n",
      "\n",
      "Train set: Avg. loss: 0.000067853, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.083405 105.52002\n",
      "l2 norm: 916.3382116552492\n",
      "l1 norm: 768.197534268468\n",
      "Rbeta: 916.5925205095767\n",
      "\n",
      "Train set: Avg. loss: 0.000067694, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.10955 105.54741\n",
      "l2 norm: 916.2721742431694\n",
      "l1 norm: 768.143337704674\n",
      "Rbeta: 916.5264378484181\n",
      "\n",
      "Train set: Avg. loss: 0.000067535, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.135704 105.57471\n",
      "l2 norm: 916.2088939415562\n",
      "l1 norm: 768.0915115253151\n",
      "Rbeta: 916.4632161823283\n",
      "\n",
      "Train set: Avg. loss: 0.000067376, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.161865 105.60197\n",
      "l2 norm: 916.1508333258681\n",
      "l1 norm: 768.044066835081\n",
      "Rbeta: 916.4050914074705\n",
      "\n",
      "Train set: Avg. loss: 0.000067219, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.18802 105.629196\n",
      "l2 norm: 916.0979274793201\n",
      "l1 norm: 768.0009205225631\n",
      "Rbeta: 916.3521999158073\n",
      "\n",
      "Train set: Avg. loss: 0.000067062, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.21419 105.65628\n",
      "l2 norm: 916.0460375975039\n",
      "l1 norm: 767.9585715008509\n",
      "Rbeta: 916.3003059737282\n",
      "\n",
      "Train set: Avg. loss: 0.000066905, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.24036 105.68328\n",
      "l2 norm: 915.9927609032809\n",
      "l1 norm: 767.9149367674963\n",
      "Rbeta: 916.247018590304\n",
      "\n",
      "Train set: Avg. loss: 0.000066749, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.266525 105.71025\n",
      "l2 norm: 915.9392407639712\n",
      "l1 norm: 767.871075716085\n",
      "Rbeta: 916.1934412838392\n",
      "\n",
      "Train set: Avg. loss: 0.000066594, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.29271 105.73719\n",
      "l2 norm: 915.8790031280898\n",
      "l1 norm: 767.8214885990732\n",
      "Rbeta: 916.1332526585489\n",
      "\n",
      "Train set: Avg. loss: 0.000066439, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.31888 105.764015\n",
      "l2 norm: 915.8287240391817\n",
      "l1 norm: 767.7802717751046\n",
      "Rbeta: 916.0828143090647\n",
      "\n",
      "Train set: Avg. loss: 0.000066284, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.34506 105.79077\n",
      "l2 norm: 915.7720259877425\n",
      "l1 norm: 767.7336970385763\n",
      "Rbeta: 916.0261151437072\n",
      "\n",
      "Train set: Avg. loss: 0.000066131, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.37125 105.817474\n",
      "l2 norm: 915.7182784614542\n",
      "l1 norm: 767.6896585991933\n",
      "Rbeta: 915.9723199037452\n",
      "\n",
      "Train set: Avg. loss: 0.000065977, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.39744 105.8441\n",
      "l2 norm: 915.6605056130797\n",
      "l1 norm: 767.6422482094376\n",
      "Rbeta: 915.9145488991965\n",
      "\n",
      "Train set: Avg. loss: 0.000065825, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.42363 105.87067\n",
      "l2 norm: 915.6028525315035\n",
      "l1 norm: 767.5949279426127\n",
      "Rbeta: 915.8567790217986\n",
      "\n",
      "Train set: Avg. loss: 0.000065672, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.44983 105.89717\n",
      "l2 norm: 915.5498659172458\n",
      "l1 norm: 767.5515172273363\n",
      "Rbeta: 915.8037257729295\n",
      "\n",
      "Train set: Avg. loss: 0.000065521, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.47603 105.92365\n",
      "l2 norm: 915.4939953648321\n",
      "l1 norm: 767.50557586482\n",
      "Rbeta: 915.74780645281\n",
      "\n",
      "Train set: Avg. loss: 0.000065369, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.50223 105.950134\n",
      "l2 norm: 915.4419256738208\n",
      "l1 norm: 767.4628113448654\n",
      "Rbeta: 915.6956494056943\n",
      "\n",
      "Train set: Avg. loss: 0.000065218, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.52843 105.97658\n",
      "l2 norm: 915.3880772556648\n",
      "l1 norm: 767.4186154596116\n",
      "Rbeta: 915.6417043290218\n",
      "\n",
      "Train set: Avg. loss: 0.000065068, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.55464 106.003006\n",
      "l2 norm: 915.334272079702\n",
      "l1 norm: 767.3744900082081\n",
      "Rbeta: 915.5877929672864\n",
      "\n",
      "Train set: Avg. loss: 0.000064918, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.58086 106.02941\n",
      "l2 norm: 915.2801161343068\n",
      "l1 norm: 767.3300563619091\n",
      "Rbeta: 915.5335400637014\n",
      "\n",
      "Train set: Avg. loss: 0.000064768, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.60707 106.05573\n",
      "l2 norm: 915.2277029539997\n",
      "l1 norm: 767.2870883091942\n",
      "Rbeta: 915.4810967419919\n",
      "\n",
      "Train set: Avg. loss: 0.000064619, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.633286 106.08208\n",
      "l2 norm: 915.172754455914\n",
      "l1 norm: 767.2420092885892\n",
      "Rbeta: 915.4260407993968\n",
      "\n",
      "Train set: Avg. loss: 0.000064470, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.659515 106.10837\n",
      "l2 norm: 915.1116872198406\n",
      "l1 norm: 767.1918301909991\n",
      "Rbeta: 915.3648754451903\n",
      "\n",
      "Train set: Avg. loss: 0.000064321, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.685745 106.1346\n",
      "l2 norm: 915.0615636083329\n",
      "l1 norm: 767.1508046750758\n",
      "Rbeta: 915.3146536323717\n",
      "\n",
      "Train set: Avg. loss: 0.000064174, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.711975 106.16077\n",
      "l2 norm: 915.0057186648368\n",
      "l1 norm: 767.1049207542338\n",
      "Rbeta: 915.2586693379337\n",
      "\n",
      "Train set: Avg. loss: 0.000064026, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.738205 106.18692\n",
      "l2 norm: 914.951155552618\n",
      "l1 norm: 767.0601631113907\n",
      "Rbeta: 915.2040335666181\n",
      "\n",
      "Train set: Avg. loss: 0.000063879, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.76445 106.21302\n",
      "l2 norm: 914.896075362406\n",
      "l1 norm: 767.0149960432991\n",
      "Rbeta: 915.1489375206459\n",
      "\n",
      "Train set: Avg. loss: 0.000063733, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.790695 106.23902\n",
      "l2 norm: 914.8425489770652\n",
      "l1 norm: 766.9711636258216\n",
      "Rbeta: 915.0952654507031\n",
      "\n",
      "Train set: Avg. loss: 0.000063587, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.81693 106.264984\n",
      "l2 norm: 914.7856658216435\n",
      "l1 norm: 766.9245316164786\n",
      "Rbeta: 915.0381803111636\n",
      "\n",
      "Train set: Avg. loss: 0.000063442, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.843185 106.29086\n",
      "l2 norm: 914.7229241500266\n",
      "l1 norm: 766.8729245097913\n",
      "Rbeta: 914.9753554906587\n",
      "\n",
      "Train set: Avg. loss: 0.000063297, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.86943 106.31668\n",
      "l2 norm: 914.6690156903458\n",
      "l1 norm: 766.8286619331074\n",
      "Rbeta: 914.9212825513978\n",
      "\n",
      "Train set: Avg. loss: 0.000063152, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.89569 106.34244\n",
      "l2 norm: 914.6168503539524\n",
      "l1 norm: 766.7858514287252\n",
      "Rbeta: 914.869018396072\n",
      "\n",
      "Train set: Avg. loss: 0.000063008, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.92195 106.368164\n",
      "l2 norm: 914.5614975436063\n",
      "l1 norm: 766.7403819360798\n",
      "Rbeta: 914.8135389738308\n",
      "\n",
      "Train set: Avg. loss: 0.000062865, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.94821 106.39386\n",
      "l2 norm: 914.5027034483085\n",
      "l1 norm: 766.6920319705836\n",
      "Rbeta: 914.7546084486961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000062722, Accuracy: 512/512 (100%)\n",
      "\n",
      "101.97448 106.41945\n",
      "l2 norm: 914.444283492927\n",
      "l1 norm: 766.6439583890813\n",
      "Rbeta: 914.6959639800855\n",
      "\n",
      "Train set: Avg. loss: 0.000062579, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.00075 106.444984\n",
      "l2 norm: 914.3836366762016\n",
      "l1 norm: 766.5940103917413\n",
      "Rbeta: 914.6352487884736\n",
      "\n",
      "Train set: Avg. loss: 0.000062437, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.02702 106.47046\n",
      "l2 norm: 914.3315489572417\n",
      "l1 norm: 766.5513370279081\n",
      "Rbeta: 914.5829651565011\n",
      "\n",
      "Train set: Avg. loss: 0.000062296, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.0533 106.49588\n",
      "l2 norm: 914.2894098993216\n",
      "l1 norm: 766.5170533910022\n",
      "Rbeta: 914.5406719553267\n",
      "\n",
      "Train set: Avg. loss: 0.000062154, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.079575 106.521286\n",
      "l2 norm: 914.2394697138548\n",
      "l1 norm: 766.4761661600753\n",
      "Rbeta: 914.4905705143866\n",
      "\n",
      "Train set: Avg. loss: 0.000062014, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.105865 106.546585\n",
      "l2 norm: 914.1939691890502\n",
      "l1 norm: 766.4390000084318\n",
      "Rbeta: 914.4448756247432\n",
      "\n",
      "Train set: Avg. loss: 0.000061874, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.13215 106.57187\n",
      "l2 norm: 914.1443308938761\n",
      "l1 norm: 766.398408609238\n",
      "Rbeta: 914.3950744229868\n",
      "\n",
      "Train set: Avg. loss: 0.000061734, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.15843 106.59708\n",
      "l2 norm: 914.0925402147071\n",
      "l1 norm: 766.3559513792422\n",
      "Rbeta: 914.3430665928719\n",
      "\n",
      "Train set: Avg. loss: 0.000061595, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.1844 106.62223\n",
      "l2 norm: 914.0421891911561\n",
      "l1 norm: 766.3146971273924\n",
      "Rbeta: 914.2926485209337\n",
      "\n",
      "Train set: Avg. loss: 0.000061458, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.2103 106.64734\n",
      "l2 norm: 913.9901886237989\n",
      "l1 norm: 766.2720743750526\n",
      "Rbeta: 914.2404072486704\n",
      "\n",
      "Train set: Avg. loss: 0.000061320, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.2362 106.67241\n",
      "l2 norm: 913.9330695296899\n",
      "l1 norm: 766.225138735426\n",
      "Rbeta: 914.1831161176495\n",
      "\n",
      "Train set: Avg. loss: 0.000061183, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.26211 106.6974\n",
      "l2 norm: 913.8835907141258\n",
      "l1 norm: 766.1846040876223\n",
      "Rbeta: 914.1334589810114\n",
      "\n",
      "Train set: Avg. loss: 0.000061047, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.288025 106.72234\n",
      "l2 norm: 913.831243024013\n",
      "l1 norm: 766.1416688917368\n",
      "Rbeta: 914.0810769050203\n",
      "\n",
      "Train set: Avg. loss: 0.000060911, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.313934 106.74719\n",
      "l2 norm: 913.7780300388588\n",
      "l1 norm: 766.0979909292781\n",
      "Rbeta: 914.0275971438003\n",
      "\n",
      "Train set: Avg. loss: 0.000060776, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.33985 106.77196\n",
      "l2 norm: 913.7208115603471\n",
      "l1 norm: 766.0510380181763\n",
      "Rbeta: 913.9702517335354\n",
      "\n",
      "Train set: Avg. loss: 0.000060641, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.36577 106.79665\n",
      "l2 norm: 913.6683294015272\n",
      "l1 norm: 766.0080818585832\n",
      "Rbeta: 913.9174801322033\n",
      "\n",
      "Train set: Avg. loss: 0.000060506, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.39169 106.82126\n",
      "l2 norm: 913.613221488104\n",
      "l1 norm: 765.9628720834618\n",
      "Rbeta: 913.8622131773576\n",
      "\n",
      "Train set: Avg. loss: 0.000060372, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.41762 106.845825\n",
      "l2 norm: 913.5570354216993\n",
      "l1 norm: 765.9167089637641\n",
      "Rbeta: 913.805768856742\n",
      "\n",
      "Train set: Avg. loss: 0.000060239, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.44354 106.87034\n",
      "l2 norm: 913.511777251084\n",
      "l1 norm: 765.8796739226511\n",
      "Rbeta: 913.7602670736261\n",
      "\n",
      "Train set: Avg. loss: 0.000060106, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.46948 106.894806\n",
      "l2 norm: 913.4626029540757\n",
      "l1 norm: 765.8393382349248\n",
      "Rbeta: 913.7109816485219\n",
      "\n",
      "Train set: Avg. loss: 0.000059973, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.49542 106.919235\n",
      "l2 norm: 913.4133882267532\n",
      "l1 norm: 765.7989686638846\n",
      "Rbeta: 913.6615255470106\n",
      "\n",
      "Train set: Avg. loss: 0.000059840, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.52136 106.943665\n",
      "l2 norm: 913.3654796186518\n",
      "l1 norm: 765.7596984595693\n",
      "Rbeta: 913.6134049182269\n",
      "\n",
      "Train set: Avg. loss: 0.000059708, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.54731 106.96803\n",
      "l2 norm: 913.3194023779979\n",
      "l1 norm: 765.7219369277965\n",
      "Rbeta: 913.5671310846433\n",
      "\n",
      "Train set: Avg. loss: 0.000059576, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.57326 106.99239\n",
      "l2 norm: 913.2721604565807\n",
      "l1 norm: 765.6832328991097\n",
      "Rbeta: 913.5196236980829\n",
      "\n",
      "Train set: Avg. loss: 0.000059445, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.5992 107.016754\n",
      "l2 norm: 913.2244492861212\n",
      "l1 norm: 765.6441332656158\n",
      "Rbeta: 913.4716766791634\n",
      "\n",
      "Train set: Avg. loss: 0.000059314, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.62516 107.04108\n",
      "l2 norm: 913.1737578923882\n",
      "l1 norm: 765.6025516308637\n",
      "Rbeta: 913.4207191805945\n",
      "\n",
      "Train set: Avg. loss: 0.000059183, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.65111 107.06535\n",
      "l2 norm: 913.1196564916409\n",
      "l1 norm: 765.5580827931894\n",
      "Rbeta: 913.3664507979494\n",
      "\n",
      "Train set: Avg. loss: 0.000059053, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.67707 107.089584\n",
      "l2 norm: 913.060001468925\n",
      "l1 norm: 765.508941813224\n",
      "Rbeta: 913.3066305873456\n",
      "\n",
      "Train set: Avg. loss: 0.000058923, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.70303 107.113785\n",
      "l2 norm: 912.9968795295974\n",
      "l1 norm: 765.456938982222\n",
      "Rbeta: 913.2432121462671\n",
      "\n",
      "Train set: Avg. loss: 0.000058794, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.729004 107.13797\n",
      "l2 norm: 912.9359979416778\n",
      "l1 norm: 765.4068024816247\n",
      "Rbeta: 913.1820827703185\n",
      "\n",
      "Train set: Avg. loss: 0.000058664, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.754974 107.162125\n",
      "l2 norm: 912.8807566982113\n",
      "l1 norm: 765.3614562940188\n",
      "Rbeta: 913.1265992422253\n",
      "\n",
      "Train set: Avg. loss: 0.000058536, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.780945 107.18625\n",
      "l2 norm: 912.8211228886519\n",
      "l1 norm: 765.3124779335853\n",
      "Rbeta: 913.0667562454016\n",
      "\n",
      "Train set: Avg. loss: 0.000058407, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.80693 107.2103\n",
      "l2 norm: 912.7640131496581\n",
      "l1 norm: 765.2656195611227\n",
      "Rbeta: 913.0093457992359\n",
      "\n",
      "Train set: Avg. loss: 0.000058279, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.83291 107.23433\n",
      "l2 norm: 912.7122116921316\n",
      "l1 norm: 765.2232578940591\n",
      "Rbeta: 912.957276647421\n",
      "\n",
      "Train set: Avg. loss: 0.000058151, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.85889 107.258286\n",
      "l2 norm: 912.6614591934698\n",
      "l1 norm: 765.1817786898303\n",
      "Rbeta: 912.9062732989546\n",
      "\n",
      "Train set: Avg. loss: 0.000058024, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.88488 107.28218\n",
      "l2 norm: 912.6161842408102\n",
      "l1 norm: 765.1448094962265\n",
      "Rbeta: 912.8607658217979\n",
      "\n",
      "Train set: Avg. loss: 0.000057898, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.91087 107.30597\n",
      "l2 norm: 912.5708232065641\n",
      "l1 norm: 765.1077405816931\n",
      "Rbeta: 912.815123182754\n",
      "\n",
      "Train set: Avg. loss: 0.000057771, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.936874 107.329704\n",
      "l2 norm: 912.5213549069854\n",
      "l1 norm: 765.0672430957101\n",
      "Rbeta: 912.7654039564432\n",
      "\n",
      "Train set: Avg. loss: 0.000057645, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.96286 107.35339\n",
      "l2 norm: 912.4672263549063\n",
      "l1 norm: 765.0228641519689\n",
      "Rbeta: 912.7109702814585\n",
      "\n",
      "Train set: Avg. loss: 0.000057520, Accuracy: 512/512 (100%)\n",
      "\n",
      "102.98886 107.37702\n",
      "l2 norm: 912.410067470465\n",
      "l1 norm: 764.975920366316\n",
      "Rbeta: 912.6534719700078\n",
      "\n",
      "Train set: Avg. loss: 0.000057395, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.01486 107.4006\n",
      "l2 norm: 912.3547250456083\n",
      "l1 norm: 764.9303955597632\n",
      "Rbeta: 912.5979399379285\n",
      "\n",
      "Train set: Avg. loss: 0.000057270, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.04088 107.4241\n",
      "l2 norm: 912.2971596236467\n",
      "l1 norm: 764.8829753216132\n",
      "Rbeta: 912.5401054468648\n",
      "\n",
      "Train set: Avg. loss: 0.000057146, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.06689 107.447556\n",
      "l2 norm: 912.2457985013137\n",
      "l1 norm: 764.840791877562\n",
      "Rbeta: 912.4884061515043\n",
      "\n",
      "Train set: Avg. loss: 0.000057022, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.09291 107.47095\n",
      "l2 norm: 912.1974744341231\n",
      "l1 norm: 764.8011942687681\n",
      "Rbeta: 912.439845206742\n",
      "\n",
      "Train set: Avg. loss: 0.000056899, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.11891 107.49427\n",
      "l2 norm: 912.1506429954945\n",
      "l1 norm: 764.7628645535767\n",
      "Rbeta: 912.3926236339702\n",
      "\n",
      "Train set: Avg. loss: 0.000056776, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.144936 107.5176\n",
      "l2 norm: 912.1092411521569\n",
      "l1 norm: 764.729102443834\n",
      "Rbeta: 912.350911933144\n",
      "\n",
      "Train set: Avg. loss: 0.000056653, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.17097 107.540924\n",
      "l2 norm: 912.0714899164715\n",
      "l1 norm: 764.6983157825193\n",
      "Rbeta: 912.3128728725887\n",
      "\n",
      "Train set: Avg. loss: 0.000056530, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.19699 107.56422\n",
      "l2 norm: 912.0347056062657\n",
      "l1 norm: 764.6683301180644\n",
      "Rbeta: 912.2757229993967\n",
      "\n",
      "Train set: Avg. loss: 0.000056408, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.22299 107.58746\n",
      "l2 norm: 911.9937374624296\n",
      "l1 norm: 764.6348129478974\n",
      "Rbeta: 912.2345059576945\n",
      "\n",
      "Train set: Avg. loss: 0.000056287, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.24862 107.610664\n",
      "l2 norm: 911.9409251690411\n",
      "l1 norm: 764.5914122454751\n",
      "Rbeta: 912.1813495965021\n",
      "\n",
      "Train set: Avg. loss: 0.000056167, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.27426 107.63379\n",
      "l2 norm: 911.8944946988571\n",
      "l1 norm: 764.5533777747667\n",
      "Rbeta: 912.134661182353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000056047, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.299904 107.65686\n",
      "l2 norm: 911.8413083027959\n",
      "l1 norm: 764.5096930877257\n",
      "Rbeta: 912.0811538927387\n",
      "\n",
      "Train set: Avg. loss: 0.000055927, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.32555 107.67992\n",
      "l2 norm: 911.784172944469\n",
      "l1 norm: 764.4627109325432\n",
      "Rbeta: 912.0237419379438\n",
      "\n",
      "Train set: Avg. loss: 0.000055808, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.3512 107.702934\n",
      "l2 norm: 911.7330462897786\n",
      "l1 norm: 764.4207565756907\n",
      "Rbeta: 911.9723387259511\n",
      "\n",
      "Train set: Avg. loss: 0.000055689, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.37685 107.7259\n",
      "l2 norm: 911.6859118984405\n",
      "l1 norm: 764.3821691448107\n",
      "Rbeta: 911.9249277910449\n",
      "\n",
      "Train set: Avg. loss: 0.000055570, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.4025 107.7488\n",
      "l2 norm: 911.6382918474445\n",
      "l1 norm: 764.3431632064605\n",
      "Rbeta: 911.8769279399601\n",
      "\n",
      "Train set: Avg. loss: 0.000055452, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.428154 107.77165\n",
      "l2 norm: 911.5912167536321\n",
      "l1 norm: 764.3045865326901\n",
      "Rbeta: 911.829512957856\n",
      "\n",
      "Train set: Avg. loss: 0.000055334, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.45381 107.794426\n",
      "l2 norm: 911.5511262049188\n",
      "l1 norm: 764.2718684015591\n",
      "Rbeta: 911.7890742593409\n",
      "\n",
      "Train set: Avg. loss: 0.000055217, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.47948 107.81717\n",
      "l2 norm: 911.5111410947571\n",
      "l1 norm: 764.2392468877479\n",
      "Rbeta: 911.7488263383228\n",
      "\n",
      "Train set: Avg. loss: 0.000055100, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.50514 107.83983\n",
      "l2 norm: 911.4729185566243\n",
      "l1 norm: 764.2080960801234\n",
      "Rbeta: 911.7102566690301\n",
      "\n",
      "Train set: Avg. loss: 0.000054983, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.53081 107.86246\n",
      "l2 norm: 911.4348511094151\n",
      "l1 norm: 764.1770322932608\n",
      "Rbeta: 911.6718106326138\n",
      "\n",
      "Train set: Avg. loss: 0.000054867, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.55649 107.8851\n",
      "l2 norm: 911.3890038064618\n",
      "l1 norm: 764.1394357940821\n",
      "Rbeta: 911.6256687529063\n",
      "\n",
      "Train set: Avg. loss: 0.000054750, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.58216 107.90776\n",
      "l2 norm: 911.3376028843368\n",
      "l1 norm: 764.0971952098289\n",
      "Rbeta: 911.5739391768888\n",
      "\n",
      "Train set: Avg. loss: 0.000054634, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.60785 107.930405\n",
      "l2 norm: 911.2800525751586\n",
      "l1 norm: 764.0497376317164\n",
      "Rbeta: 911.5160864596562\n",
      "\n",
      "Train set: Avg. loss: 0.000054519, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.63353 107.95299\n",
      "l2 norm: 911.2166132117043\n",
      "l1 norm: 763.9973587512934\n",
      "Rbeta: 911.4522709646791\n",
      "\n",
      "Train set: Avg. loss: 0.000054404, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.65921 107.975525\n",
      "l2 norm: 911.1501040874107\n",
      "l1 norm: 763.9424178374945\n",
      "Rbeta: 911.3854306458034\n",
      "\n",
      "Train set: Avg. loss: 0.000054289, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.68477 107.99809\n",
      "l2 norm: 911.0879168781943\n",
      "l1 norm: 763.8910048232096\n",
      "Rbeta: 911.3229718843916\n",
      "\n",
      "Train set: Avg. loss: 0.000054175, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.71005 108.02059\n",
      "l2 norm: 911.0306908041043\n",
      "l1 norm: 763.8437483284813\n",
      "Rbeta: 911.2653585596457\n",
      "\n",
      "Train set: Avg. loss: 0.000054061, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.73535 108.04309\n",
      "l2 norm: 910.9726611573436\n",
      "l1 norm: 763.7958301867301\n",
      "Rbeta: 911.2070190367123\n",
      "\n",
      "Train set: Avg. loss: 0.000053948, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.76065 108.06555\n",
      "l2 norm: 910.9238826403625\n",
      "l1 norm: 763.7556940078426\n",
      "Rbeta: 911.1579262588098\n",
      "\n",
      "Train set: Avg. loss: 0.000053835, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.78595 108.08795\n",
      "l2 norm: 910.8807974919625\n",
      "l1 norm: 763.7203565047682\n",
      "Rbeta: 911.1145534912154\n",
      "\n",
      "Train set: Avg. loss: 0.000053723, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.81125 108.11029\n",
      "l2 norm: 910.8415402172545\n",
      "l1 norm: 763.6882490375068\n",
      "Rbeta: 911.0749107875449\n",
      "\n",
      "Train set: Avg. loss: 0.000053611, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.83649 108.13258\n",
      "l2 norm: 910.8031384289387\n",
      "l1 norm: 763.6568268164419\n",
      "Rbeta: 911.0361722579723\n",
      "\n",
      "Train set: Avg. loss: 0.000053500, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.8614 108.15481\n",
      "l2 norm: 910.7597818795884\n",
      "l1 norm: 763.6212614608496\n",
      "Rbeta: 910.992483311579\n",
      "\n",
      "Train set: Avg. loss: 0.000053390, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.88631 108.17699\n",
      "l2 norm: 910.7088331924067\n",
      "l1 norm: 763.5793582601477\n",
      "Rbeta: 910.9412160360379\n",
      "\n",
      "Train set: Avg. loss: 0.000053279, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.911224 108.19914\n",
      "l2 norm: 910.6557968235844\n",
      "l1 norm: 763.5357561121169\n",
      "Rbeta: 910.8878716353604\n",
      "\n",
      "Train set: Avg. loss: 0.000053170, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.93614 108.22128\n",
      "l2 norm: 910.6035933512117\n",
      "l1 norm: 763.49287025525\n",
      "Rbeta: 910.8354239527433\n",
      "\n",
      "Train set: Avg. loss: 0.000053060, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.96106 108.24333\n",
      "l2 norm: 910.5518084159949\n",
      "l1 norm: 763.450362707782\n",
      "Rbeta: 910.7832533713727\n",
      "\n",
      "Train set: Avg. loss: 0.000052951, Accuracy: 512/512 (100%)\n",
      "\n",
      "103.98598 108.265335\n",
      "l2 norm: 910.499951273327\n",
      "l1 norm: 763.4077310725295\n",
      "Rbeta: 910.7310318934852\n",
      "\n",
      "Train set: Avg. loss: 0.000052842, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.01077 108.28729\n",
      "l2 norm: 910.4466170239342\n",
      "l1 norm: 763.3638885705814\n",
      "Rbeta: 910.6774209549076\n",
      "\n",
      "Train set: Avg. loss: 0.000052735, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.0353 108.309204\n",
      "l2 norm: 910.4026409809998\n",
      "l1 norm: 763.3279301054399\n",
      "Rbeta: 910.6331220949612\n",
      "\n",
      "Train set: Avg. loss: 0.000052627, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.05983 108.331085\n",
      "l2 norm: 910.3590700383265\n",
      "l1 norm: 763.2923108235319\n",
      "Rbeta: 910.5892323990155\n",
      "\n",
      "Train set: Avg. loss: 0.000052520, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.084366 108.352905\n",
      "l2 norm: 910.3151740959509\n",
      "l1 norm: 763.2564299746533\n",
      "Rbeta: 910.5449954311238\n",
      "\n",
      "Train set: Avg. loss: 0.000052414, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.108894 108.37472\n",
      "l2 norm: 910.2724186610519\n",
      "l1 norm: 763.2214770991798\n",
      "Rbeta: 910.5019375616612\n",
      "\n",
      "Train set: Avg. loss: 0.000052307, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.13344 108.396484\n",
      "l2 norm: 910.2259457692325\n",
      "l1 norm: 763.1833470900558\n",
      "Rbeta: 910.4551684199258\n",
      "\n",
      "Train set: Avg. loss: 0.000052201, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.157974 108.41822\n",
      "l2 norm: 910.1816450185025\n",
      "l1 norm: 763.1470189819389\n",
      "Rbeta: 910.4105015725696\n",
      "\n",
      "Train set: Avg. loss: 0.000052095, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.18252 108.43996\n",
      "l2 norm: 910.1408790028275\n",
      "l1 norm: 763.1136884689834\n",
      "Rbeta: 910.3693362433468\n",
      "\n",
      "Train set: Avg. loss: 0.000051990, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.20707 108.46165\n",
      "l2 norm: 910.0977796573862\n",
      "l1 norm: 763.0784051288688\n",
      "Rbeta: 910.3259788394481\n",
      "\n",
      "Train set: Avg. loss: 0.000051884, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.23162 108.48329\n",
      "l2 norm: 910.0569874749946\n",
      "l1 norm: 763.0450120992248\n",
      "Rbeta: 910.2848211761204\n",
      "\n",
      "Train set: Avg. loss: 0.000051779, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.25618 108.5049\n",
      "l2 norm: 910.0171151217934\n",
      "l1 norm: 763.0124181658285\n",
      "Rbeta: 910.2446643395901\n",
      "\n",
      "Train set: Avg. loss: 0.000051675, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.28073 108.526436\n",
      "l2 norm: 909.975350170306\n",
      "l1 norm: 762.9782616221054\n",
      "Rbeta: 910.2025125769701\n",
      "\n",
      "Train set: Avg. loss: 0.000051570, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.30528 108.54794\n",
      "l2 norm: 909.9343819745259\n",
      "l1 norm: 762.9447997893133\n",
      "Rbeta: 910.1612456093916\n",
      "\n",
      "Train set: Avg. loss: 0.000051466, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.32985 108.56938\n",
      "l2 norm: 909.8945347610545\n",
      "l1 norm: 762.9122462563624\n",
      "Rbeta: 910.121014779869\n",
      "\n",
      "Train set: Avg. loss: 0.000051362, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.354416 108.59083\n",
      "l2 norm: 909.8525707196771\n",
      "l1 norm: 762.8778815224496\n",
      "Rbeta: 910.0787836231449\n",
      "\n",
      "Train set: Avg. loss: 0.000051259, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.37898 108.61223\n",
      "l2 norm: 909.8121042198154\n",
      "l1 norm: 762.8447738844536\n",
      "Rbeta: 910.0378782240355\n",
      "\n",
      "Train set: Avg. loss: 0.000051156, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.40355 108.633606\n",
      "l2 norm: 909.7714427061925\n",
      "l1 norm: 762.8114830917136\n",
      "Rbeta: 909.9968710482726\n",
      "\n",
      "Train set: Avg. loss: 0.000051053, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.42812 108.65496\n",
      "l2 norm: 909.7311178517186\n",
      "l1 norm: 762.7784444114959\n",
      "Rbeta: 909.9561675143717\n",
      "\n",
      "Train set: Avg. loss: 0.000050950, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.4527 108.67625\n",
      "l2 norm: 909.6932855021385\n",
      "l1 norm: 762.7474571475643\n",
      "Rbeta: 909.9179380138146\n",
      "\n",
      "Train set: Avg. loss: 0.000050848, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.47728 108.697495\n",
      "l2 norm: 909.6536364705544\n",
      "l1 norm: 762.7149841936266\n",
      "Rbeta: 909.8779848190817\n",
      "\n",
      "Train set: Avg. loss: 0.000050745, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.50186 108.718735\n",
      "l2 norm: 909.61296786727\n",
      "l1 norm: 762.6816647440407\n",
      "Rbeta: 909.8369458608881\n",
      "\n",
      "Train set: Avg. loss: 0.000050643, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.52644 108.73999\n",
      "l2 norm: 909.5704280871713\n",
      "l1 norm: 762.6467798114988\n",
      "Rbeta: 909.7940269687298\n",
      "\n",
      "Train set: Avg. loss: 0.000050542, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.55103 108.761215\n",
      "l2 norm: 909.5297125103733\n",
      "l1 norm: 762.6133864116517\n",
      "Rbeta: 909.7529734293779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000050440, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.57562 108.78239\n",
      "l2 norm: 909.4769931495381\n",
      "l1 norm: 762.5699652394853\n",
      "Rbeta: 909.6998251277741\n",
      "\n",
      "Train set: Avg. loss: 0.000050339, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.60022 108.80351\n",
      "l2 norm: 909.4231963807223\n",
      "l1 norm: 762.5256529706783\n",
      "Rbeta: 909.6456585489542\n",
      "\n",
      "Train set: Avg. loss: 0.000050238, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.62481 108.824585\n",
      "l2 norm: 909.3658126882548\n",
      "l1 norm: 762.4782865807055\n",
      "Rbeta: 909.5879115366532\n",
      "\n",
      "Train set: Avg. loss: 0.000050138, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.64941 108.84561\n",
      "l2 norm: 909.318870216895\n",
      "l1 norm: 762.4395900795882\n",
      "Rbeta: 909.540567831178\n",
      "\n",
      "Train set: Avg. loss: 0.000050038, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.67401 108.86662\n",
      "l2 norm: 909.2711324868728\n",
      "l1 norm: 762.4001753381785\n",
      "Rbeta: 909.4924339117994\n",
      "\n",
      "Train set: Avg. loss: 0.000049938, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.69861 108.88759\n",
      "l2 norm: 909.2246345328494\n",
      "l1 norm: 762.3618157269234\n",
      "Rbeta: 909.4455156016866\n",
      "\n",
      "Train set: Avg. loss: 0.000049838, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.72323 108.90852\n",
      "l2 norm: 909.1760238279406\n",
      "l1 norm: 762.3217207737177\n",
      "Rbeta: 909.3964805327872\n",
      "\n",
      "Train set: Avg. loss: 0.000049738, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.74783 108.92944\n",
      "l2 norm: 909.1239143175835\n",
      "l1 norm: 762.2787178119916\n",
      "Rbeta: 909.3440044217806\n",
      "\n",
      "Train set: Avg. loss: 0.000049639, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.772446 108.95029\n",
      "l2 norm: 909.0765348568033\n",
      "l1 norm: 762.2396983439946\n",
      "Rbeta: 909.2962474282647\n",
      "\n",
      "Train set: Avg. loss: 0.000049540, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.79706 108.97107\n",
      "l2 norm: 909.0346524119955\n",
      "l1 norm: 762.2052463008699\n",
      "Rbeta: 909.2539038392757\n",
      "\n",
      "Train set: Avg. loss: 0.000049442, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.821686 108.9918\n",
      "l2 norm: 908.9955951338823\n",
      "l1 norm: 762.1731600613405\n",
      "Rbeta: 909.2144985854334\n",
      "\n",
      "Train set: Avg. loss: 0.000049344, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.846306 109.01247\n",
      "l2 norm: 908.9543755566208\n",
      "l1 norm: 762.1393420527996\n",
      "Rbeta: 909.1728381437791\n",
      "\n",
      "Train set: Avg. loss: 0.000049246, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.870926 109.03309\n",
      "l2 norm: 908.912543726232\n",
      "l1 norm: 762.1050852655262\n",
      "Rbeta: 909.1306088316576\n",
      "\n",
      "Train set: Avg. loss: 0.000049148, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.89555 109.053665\n",
      "l2 norm: 908.8689140379496\n",
      "l1 norm: 762.069347654257\n",
      "Rbeta: 909.0865302827941\n",
      "\n",
      "Train set: Avg. loss: 0.000049051, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.92019 109.07422\n",
      "l2 norm: 908.8263358603182\n",
      "l1 norm: 762.0344962407914\n",
      "Rbeta: 909.0435679264193\n",
      "\n",
      "Train set: Avg. loss: 0.000048953, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.944824 109.09477\n",
      "l2 norm: 908.7819939101323\n",
      "l1 norm: 761.9981647402401\n",
      "Rbeta: 908.998779707645\n",
      "\n",
      "Train set: Avg. loss: 0.000048856, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.96947 109.115295\n",
      "l2 norm: 908.7344587774064\n",
      "l1 norm: 761.9591861411162\n",
      "Rbeta: 908.9509021759802\n",
      "\n",
      "Train set: Avg. loss: 0.000048760, Accuracy: 512/512 (100%)\n",
      "\n",
      "104.9941 109.13575\n",
      "l2 norm: 908.6840248305369\n",
      "l1 norm: 761.917818575215\n",
      "Rbeta: 908.9000241108156\n",
      "\n",
      "Train set: Avg. loss: 0.000048664, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.01874 109.15616\n",
      "l2 norm: 908.630766464677\n",
      "l1 norm: 761.8740269206861\n",
      "Rbeta: 908.8462778650073\n",
      "\n",
      "Train set: Avg. loss: 0.000048567, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.04338 109.17656\n",
      "l2 norm: 908.5797357447232\n",
      "l1 norm: 761.8321019062547\n",
      "Rbeta: 908.7948011276445\n",
      "\n",
      "Train set: Avg. loss: 0.000048472, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.06768 109.19695\n",
      "l2 norm: 908.5325329774746\n",
      "l1 norm: 761.7934022616448\n",
      "Rbeta: 908.7472045657244\n",
      "\n",
      "Train set: Avg. loss: 0.000048377, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.091934 109.21733\n",
      "l2 norm: 908.4866436764977\n",
      "l1 norm: 761.7557621056993\n",
      "Rbeta: 908.7009242280587\n",
      "\n",
      "Train set: Avg. loss: 0.000048283, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.11618 109.23769\n",
      "l2 norm: 908.4390300133364\n",
      "l1 norm: 761.7166192888628\n",
      "Rbeta: 908.6529386917979\n",
      "\n",
      "Train set: Avg. loss: 0.000048188, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.14043 109.25801\n",
      "l2 norm: 908.3926769105568\n",
      "l1 norm: 761.678493342303\n",
      "Rbeta: 908.6060966131078\n",
      "\n",
      "Train set: Avg. loss: 0.000048094, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.16469 109.278275\n",
      "l2 norm: 908.3469388478787\n",
      "l1 norm: 761.6409146224487\n",
      "Rbeta: 908.559952578943\n",
      "\n",
      "Train set: Avg. loss: 0.000048001, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.18863 109.29848\n",
      "l2 norm: 908.3020585164966\n",
      "l1 norm: 761.6040320308953\n",
      "Rbeta: 908.5147396781479\n",
      "\n",
      "Train set: Avg. loss: 0.000047909, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.21248 109.3186\n",
      "l2 norm: 908.2585328088269\n",
      "l1 norm: 761.5682570416009\n",
      "Rbeta: 908.4707308599079\n",
      "\n",
      "Train set: Avg. loss: 0.000047816, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.23634 109.33873\n",
      "l2 norm: 908.2186644357114\n",
      "l1 norm: 761.5355808103672\n",
      "Rbeta: 908.4305340230982\n",
      "\n",
      "Train set: Avg. loss: 0.000047724, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.26021 109.35878\n",
      "l2 norm: 908.1839438642907\n",
      "l1 norm: 761.5072487692739\n",
      "Rbeta: 908.3953625205934\n",
      "\n",
      "Train set: Avg. loss: 0.000047632, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.284065 109.378845\n",
      "l2 norm: 908.1495575768653\n",
      "l1 norm: 761.4791928819002\n",
      "Rbeta: 908.3605517916114\n",
      "\n",
      "Train set: Avg. loss: 0.000047541, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.30794 109.398865\n",
      "l2 norm: 908.1120871796576\n",
      "l1 norm: 761.4485493427264\n",
      "Rbeta: 908.3227158633921\n",
      "\n",
      "Train set: Avg. loss: 0.000047449, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.3318 109.4189\n",
      "l2 norm: 908.0754814407198\n",
      "l1 norm: 761.4185886539228\n",
      "Rbeta: 908.2856999701958\n",
      "\n",
      "Train set: Avg. loss: 0.000047358, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.355675 109.43888\n",
      "l2 norm: 908.0398071775109\n",
      "l1 norm: 761.3894308103179\n",
      "Rbeta: 908.2495959867853\n",
      "\n",
      "Train set: Avg. loss: 0.000047267, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.37955 109.4588\n",
      "l2 norm: 908.0042607574962\n",
      "l1 norm: 761.3603716104294\n",
      "Rbeta: 908.2135948591693\n",
      "\n",
      "Train set: Avg. loss: 0.000047176, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.40343 109.478714\n",
      "l2 norm: 907.9607942711089\n",
      "l1 norm: 761.3246642885063\n",
      "Rbeta: 908.1697755506062\n",
      "\n",
      "Train set: Avg. loss: 0.000047086, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.42731 109.498566\n",
      "l2 norm: 907.9166024005402\n",
      "l1 norm: 761.2882767268634\n",
      "Rbeta: 908.1251486606735\n",
      "\n",
      "Train set: Avg. loss: 0.000046996, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.45119 109.51836\n",
      "l2 norm: 907.873971607729\n",
      "l1 norm: 761.2531307693291\n",
      "Rbeta: 908.0820584456311\n",
      "\n",
      "Train set: Avg. loss: 0.000046905, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.475075 109.53815\n",
      "l2 norm: 907.8297781529355\n",
      "l1 norm: 761.2166938153575\n",
      "Rbeta: 908.0373898164057\n",
      "\n",
      "Train set: Avg. loss: 0.000046816, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.49896 109.55792\n",
      "l2 norm: 907.7973693444483\n",
      "l1 norm: 761.1902079776689\n",
      "Rbeta: 908.0046260207502\n",
      "\n",
      "Train set: Avg. loss: 0.000046727, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.52264 109.57765\n",
      "l2 norm: 907.7640031199091\n",
      "l1 norm: 761.1629640398535\n",
      "Rbeta: 907.9708327407624\n",
      "\n",
      "Train set: Avg. loss: 0.000046638, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.54612 109.59738\n",
      "l2 norm: 907.7294125619933\n",
      "l1 norm: 761.1346603338363\n",
      "Rbeta: 907.9358261114106\n",
      "\n",
      "Train set: Avg. loss: 0.000046550, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.56961 109.6171\n",
      "l2 norm: 907.697872874389\n",
      "l1 norm: 761.1088791457613\n",
      "Rbeta: 907.9038614724014\n",
      "\n",
      "Train set: Avg. loss: 0.000046461, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.59311 109.6368\n",
      "l2 norm: 907.6653345370703\n",
      "l1 norm: 761.0822645128278\n",
      "Rbeta: 907.870998885702\n",
      "\n",
      "Train set: Avg. loss: 0.000046374, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.61661 109.65648\n",
      "l2 norm: 907.6275147127847\n",
      "l1 norm: 761.0512531972876\n",
      "Rbeta: 907.8327103584834\n",
      "\n",
      "Train set: Avg. loss: 0.000046286, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.64011 109.67613\n",
      "l2 norm: 907.5874951971606\n",
      "l1 norm: 761.0184236275102\n",
      "Rbeta: 907.7923229325927\n",
      "\n",
      "Train set: Avg. loss: 0.000046198, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.663605 109.69573\n",
      "l2 norm: 907.545972599448\n",
      "l1 norm: 760.9843079775102\n",
      "Rbeta: 907.7503939703226\n",
      "\n",
      "Train set: Avg. loss: 0.000046111, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.68712 109.71528\n",
      "l2 norm: 907.5102011333229\n",
      "l1 norm: 760.9549856852107\n",
      "Rbeta: 907.7142206237361\n",
      "\n",
      "Train set: Avg. loss: 0.000046024, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.710625 109.73479\n",
      "l2 norm: 907.4684732973836\n",
      "l1 norm: 760.9207147209472\n",
      "Rbeta: 907.6719952328422\n",
      "\n",
      "Train set: Avg. loss: 0.000045937, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.73413 109.75429\n",
      "l2 norm: 907.4223812217845\n",
      "l1 norm: 760.8827726248032\n",
      "Rbeta: 907.6255255136138\n",
      "\n",
      "Train set: Avg. loss: 0.000045851, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.757645 109.77374\n",
      "l2 norm: 907.3682085445242\n",
      "l1 norm: 760.8380119633881\n",
      "Rbeta: 907.5709136908049\n",
      "\n",
      "Train set: Avg. loss: 0.000045765, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.78116 109.79314\n",
      "l2 norm: 907.3149974225167\n",
      "l1 norm: 760.7940614298273\n",
      "Rbeta: 907.5172687686995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000045679, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.80467 109.81247\n",
      "l2 norm: 907.2656687788303\n",
      "l1 norm: 760.7533609509283\n",
      "Rbeta: 907.4674922569553\n",
      "\n",
      "Train set: Avg. loss: 0.000045593, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.8282 109.83174\n",
      "l2 norm: 907.2230169193359\n",
      "l1 norm: 760.7182276819226\n",
      "Rbeta: 907.4244325734206\n",
      "\n",
      "Train set: Avg. loss: 0.000045507, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.851715 109.85095\n",
      "l2 norm: 907.1787473901036\n",
      "l1 norm: 760.6817190840427\n",
      "Rbeta: 907.3797080525684\n",
      "\n",
      "Train set: Avg. loss: 0.000045422, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.875244 109.870094\n",
      "l2 norm: 907.1398020649098\n",
      "l1 norm: 760.6497476275879\n",
      "Rbeta: 907.340264504689\n",
      "\n",
      "Train set: Avg. loss: 0.000045337, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.89877 109.88922\n",
      "l2 norm: 907.0969293717915\n",
      "l1 norm: 760.6144803459761\n",
      "Rbeta: 907.2969745087248\n",
      "\n",
      "Train set: Avg. loss: 0.000045253, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.9223 109.90831\n",
      "l2 norm: 907.0528440031601\n",
      "l1 norm: 760.5781229345679\n",
      "Rbeta: 907.2524335966118\n",
      "\n",
      "Train set: Avg. loss: 0.000045168, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.94584 109.92737\n",
      "l2 norm: 907.0109682555697\n",
      "l1 norm: 760.5435852629967\n",
      "Rbeta: 907.2100588499634\n",
      "\n",
      "Train set: Avg. loss: 0.000045084, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.969376 109.94641\n",
      "l2 norm: 906.9701362734003\n",
      "l1 norm: 760.509929714685\n",
      "Rbeta: 907.1688406786112\n",
      "\n",
      "Train set: Avg. loss: 0.000045000, Accuracy: 512/512 (100%)\n",
      "\n",
      "105.99291 109.965416\n",
      "l2 norm: 906.9260168970395\n",
      "l1 norm: 760.4735822168891\n",
      "Rbeta: 907.1242706185916\n",
      "\n",
      "Train set: Avg. loss: 0.000044916, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.01646 109.98438\n",
      "l2 norm: 906.8828262223693\n",
      "l1 norm: 760.4380458154972\n",
      "Rbeta: 907.0806160471166\n",
      "\n",
      "Train set: Avg. loss: 0.000044832, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.03999 110.00335\n",
      "l2 norm: 906.8408402357792\n",
      "l1 norm: 760.4035188068189\n",
      "Rbeta: 907.0381104428398\n",
      "\n",
      "Train set: Avg. loss: 0.000044748, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.063545 110.02229\n",
      "l2 norm: 906.7948275466522\n",
      "l1 norm: 760.3655949416709\n",
      "Rbeta: 906.991718580599\n",
      "\n",
      "Train set: Avg. loss: 0.000044665, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.0871 110.04118\n",
      "l2 norm: 906.7493664296177\n",
      "l1 norm: 760.3281170018677\n",
      "Rbeta: 906.945766805367\n",
      "\n",
      "Train set: Avg. loss: 0.000044583, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.11042 110.06008\n",
      "l2 norm: 906.70629937201\n",
      "l1 norm: 760.292658875267\n",
      "Rbeta: 906.9022865045824\n",
      "\n",
      "Train set: Avg. loss: 0.000044500, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.13357 110.07897\n",
      "l2 norm: 906.6623058637906\n",
      "l1 norm: 760.2564121172387\n",
      "Rbeta: 906.8578326403567\n",
      "\n",
      "Train set: Avg. loss: 0.000044418, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.15672 110.09782\n",
      "l2 norm: 906.6192139526885\n",
      "l1 norm: 760.2209565288397\n",
      "Rbeta: 906.8143490219343\n",
      "\n",
      "Train set: Avg. loss: 0.000044337, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.17987 110.11661\n",
      "l2 norm: 906.5736771176821\n",
      "l1 norm: 760.1834950109678\n",
      "Rbeta: 906.7683008024188\n",
      "\n",
      "Train set: Avg. loss: 0.000044255, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.20303 110.135345\n",
      "l2 norm: 906.5257125817978\n",
      "l1 norm: 760.1440756492264\n",
      "Rbeta: 906.7198820670199\n",
      "\n",
      "Train set: Avg. loss: 0.000044174, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.22619 110.15407\n",
      "l2 norm: 906.4828648502715\n",
      "l1 norm: 760.1089688313191\n",
      "Rbeta: 906.6765676365709\n",
      "\n",
      "Train set: Avg. loss: 0.000044093, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.24935 110.17276\n",
      "l2 norm: 906.4396472697714\n",
      "l1 norm: 760.0736016699628\n",
      "Rbeta: 906.6329911241515\n",
      "\n",
      "Train set: Avg. loss: 0.000044012, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.272514 110.19141\n",
      "l2 norm: 906.3975807392087\n",
      "l1 norm: 760.0391855592172\n",
      "Rbeta: 906.590424770395\n",
      "\n",
      "Train set: Avg. loss: 0.000043931, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.295685 110.21004\n",
      "l2 norm: 906.353933739524\n",
      "l1 norm: 760.0034556724887\n",
      "Rbeta: 906.5463443250028\n",
      "\n",
      "Train set: Avg. loss: 0.000043851, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.31885 110.228615\n",
      "l2 norm: 906.3107255355474\n",
      "l1 norm: 759.9680660082072\n",
      "Rbeta: 906.502651686055\n",
      "\n",
      "Train set: Avg. loss: 0.000043771, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.34201 110.24716\n",
      "l2 norm: 906.2691660745334\n",
      "l1 norm: 759.9340109751508\n",
      "Rbeta: 906.460668669292\n",
      "\n",
      "Train set: Avg. loss: 0.000043691, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.36517 110.26566\n",
      "l2 norm: 906.2268129057472\n",
      "l1 norm: 759.8992921923232\n",
      "Rbeta: 906.4178286721558\n",
      "\n",
      "Train set: Avg. loss: 0.000043611, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.388336 110.28415\n",
      "l2 norm: 906.1806081733213\n",
      "l1 norm: 759.8613992303742\n",
      "Rbeta: 906.3711673543313\n",
      "\n",
      "Train set: Avg. loss: 0.000043531, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.411514 110.30263\n",
      "l2 norm: 906.1337790471292\n",
      "l1 norm: 759.8229489306577\n",
      "Rbeta: 906.3239074883347\n",
      "\n",
      "Train set: Avg. loss: 0.000043452, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.43468 110.32109\n",
      "l2 norm: 906.0879858325419\n",
      "l1 norm: 759.7853501073698\n",
      "Rbeta: 906.2776159312401\n",
      "\n",
      "Train set: Avg. loss: 0.000043372, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.457855 110.33951\n",
      "l2 norm: 906.0523578958\n",
      "l1 norm: 759.7562912488412\n",
      "Rbeta: 906.2415493072674\n",
      "\n",
      "Train set: Avg. loss: 0.000043293, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.48103 110.357864\n",
      "l2 norm: 906.0197390295608\n",
      "l1 norm: 759.7297889513108\n",
      "Rbeta: 906.2084045936491\n",
      "\n",
      "Train set: Avg. loss: 0.000043215, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.50422 110.376205\n",
      "l2 norm: 905.9923521851074\n",
      "l1 norm: 759.7076610176424\n",
      "Rbeta: 906.1806348195748\n",
      "\n",
      "Train set: Avg. loss: 0.000043136, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.52739 110.39454\n",
      "l2 norm: 905.9631036107094\n",
      "l1 norm: 759.6839762680434\n",
      "Rbeta: 906.1508885297662\n",
      "\n",
      "Train set: Avg. loss: 0.000043057, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.55058 110.412834\n",
      "l2 norm: 905.9338262991249\n",
      "l1 norm: 759.6602744580455\n",
      "Rbeta: 906.1210845098769\n",
      "\n",
      "Train set: Avg. loss: 0.000042980, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.57349 110.43109\n",
      "l2 norm: 905.9065678288899\n",
      "l1 norm: 759.638274764881\n",
      "Rbeta: 906.0934021676411\n",
      "\n",
      "Train set: Avg. loss: 0.000042902, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.59627 110.44934\n",
      "l2 norm: 905.8756085562122\n",
      "l1 norm: 759.61315310965\n",
      "Rbeta: 906.0620024590979\n",
      "\n",
      "Train set: Avg. loss: 0.000042825, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.61905 110.46758\n",
      "l2 norm: 905.8465774123266\n",
      "l1 norm: 759.5896199826476\n",
      "Rbeta: 906.0325889532398\n",
      "\n",
      "Train set: Avg. loss: 0.000042748, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.641846 110.48577\n",
      "l2 norm: 905.8140258390149\n",
      "l1 norm: 759.5631424739272\n",
      "Rbeta: 905.999617484205\n",
      "\n",
      "Train set: Avg. loss: 0.000042672, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.664635 110.50394\n",
      "l2 norm: 905.7783818607538\n",
      "l1 norm: 759.5339662936838\n",
      "Rbeta: 905.9634923844716\n",
      "\n",
      "Train set: Avg. loss: 0.000042595, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.68742 110.52207\n",
      "l2 norm: 905.741486256085\n",
      "l1 norm: 759.5037623674249\n",
      "Rbeta: 905.9261304559511\n",
      "\n",
      "Train set: Avg. loss: 0.000042519, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.71022 110.54017\n",
      "l2 norm: 905.698233352246\n",
      "l1 norm: 759.468196170273\n",
      "Rbeta: 905.8824106018004\n",
      "\n",
      "Train set: Avg. loss: 0.000042442, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.733025 110.55821\n",
      "l2 norm: 905.6599709312484\n",
      "l1 norm: 759.4368430129351\n",
      "Rbeta: 905.8436338783863\n",
      "\n",
      "Train set: Avg. loss: 0.000042366, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.75583 110.57623\n",
      "l2 norm: 905.6292195361989\n",
      "l1 norm: 759.4117967999949\n",
      "Rbeta: 905.8124133541629\n",
      "\n",
      "Train set: Avg. loss: 0.000042291, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.778625 110.59428\n",
      "l2 norm: 905.6018029423\n",
      "l1 norm: 759.3895195785874\n",
      "Rbeta: 905.784525004584\n",
      "\n",
      "Train set: Avg. loss: 0.000042215, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.80144 110.61226\n",
      "l2 norm: 905.5713414592259\n",
      "l1 norm: 759.3646397465429\n",
      "Rbeta: 905.7536540667604\n",
      "\n",
      "Train set: Avg. loss: 0.000042140, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.82425 110.630226\n",
      "l2 norm: 905.5399766782816\n",
      "l1 norm: 759.3389946203772\n",
      "Rbeta: 905.7218265028134\n",
      "\n",
      "Train set: Avg. loss: 0.000042064, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.84706 110.64815\n",
      "l2 norm: 905.5104214695585\n",
      "l1 norm: 759.314794052495\n",
      "Rbeta: 905.6917968001264\n",
      "\n",
      "Train set: Avg. loss: 0.000041989, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.86982 110.66603\n",
      "l2 norm: 905.4745493762852\n",
      "l1 norm: 759.285237866665\n",
      "Rbeta: 905.6554997820323\n",
      "\n",
      "Train set: Avg. loss: 0.000041915, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.89223 110.683846\n",
      "l2 norm: 905.438561484074\n",
      "l1 norm: 759.2556192094114\n",
      "Rbeta: 905.6190698174644\n",
      "\n",
      "Train set: Avg. loss: 0.000041841, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.91464 110.701614\n",
      "l2 norm: 905.4040620766798\n",
      "l1 norm: 759.2272563617859\n",
      "Rbeta: 905.5840942404053\n",
      "\n",
      "Train set: Avg. loss: 0.000041768, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.93706 110.719376\n",
      "l2 norm: 905.3725200132936\n",
      "l1 norm: 759.201347399199\n",
      "Rbeta: 905.5521097067209\n",
      "\n",
      "Train set: Avg. loss: 0.000041694, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.95947 110.73711\n",
      "l2 norm: 905.3385569554864\n",
      "l1 norm: 759.1734322358411\n",
      "Rbeta: 905.5176550085591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000041621, Accuracy: 512/512 (100%)\n",
      "\n",
      "106.98189 110.75482\n",
      "l2 norm: 905.3060554124751\n",
      "l1 norm: 759.1467589494493\n",
      "Rbeta: 905.4846671843911\n",
      "\n",
      "Train set: Avg. loss: 0.000041547, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.00432 110.772484\n",
      "l2 norm: 905.2766049707537\n",
      "l1 norm: 759.122647941936\n",
      "Rbeta: 905.4548133247778\n",
      "\n",
      "Train set: Avg. loss: 0.000041475, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.02673 110.790115\n",
      "l2 norm: 905.2454690919784\n",
      "l1 norm: 759.0971796572467\n",
      "Rbeta: 905.4232191521984\n",
      "\n",
      "Train set: Avg. loss: 0.000041402, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.049164 110.807724\n",
      "l2 norm: 905.2106157374064\n",
      "l1 norm: 759.068593715872\n",
      "Rbeta: 905.3879309255758\n",
      "\n",
      "Train set: Avg. loss: 0.000041329, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.071594 110.82535\n",
      "l2 norm: 905.1733472707258\n",
      "l1 norm: 759.0380354900175\n",
      "Rbeta: 905.3501994049119\n",
      "\n",
      "Train set: Avg. loss: 0.000041256, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.094025 110.84297\n",
      "l2 norm: 905.1326201232941\n",
      "l1 norm: 759.0045517466344\n",
      "Rbeta: 905.309010525487\n",
      "\n",
      "Train set: Avg. loss: 0.000041184, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.116455 110.860565\n",
      "l2 norm: 905.0820689597956\n",
      "l1 norm: 758.9628405872586\n",
      "Rbeta: 905.257965956109\n",
      "\n",
      "Train set: Avg. loss: 0.000041112, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.1389 110.87816\n",
      "l2 norm: 905.0362015315383\n",
      "l1 norm: 758.9250330319758\n",
      "Rbeta: 905.211713665353\n",
      "\n",
      "Train set: Avg. loss: 0.000041040, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.16134 110.895706\n",
      "l2 norm: 904.9991941844559\n",
      "l1 norm: 758.8946264380227\n",
      "Rbeta: 905.1741881146327\n",
      "\n",
      "Train set: Avg. loss: 0.000040968, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.18378 110.91321\n",
      "l2 norm: 904.9617484408001\n",
      "l1 norm: 758.8638209439472\n",
      "Rbeta: 905.136328875464\n",
      "\n",
      "Train set: Avg. loss: 0.000040896, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.20622 110.93068\n",
      "l2 norm: 904.9215737984247\n",
      "l1 norm: 758.8307337268755\n",
      "Rbeta: 905.0956739416833\n",
      "\n",
      "Train set: Avg. loss: 0.000040824, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.22867 110.948135\n",
      "l2 norm: 904.8856757518204\n",
      "l1 norm: 758.801284429267\n",
      "Rbeta: 905.0593379694019\n",
      "\n",
      "Train set: Avg. loss: 0.000040753, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.251114 110.96555\n",
      "l2 norm: 904.8520793856303\n",
      "l1 norm: 758.7737924664355\n",
      "Rbeta: 905.0252966661959\n",
      "\n",
      "Train set: Avg. loss: 0.000040682, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.273575 110.98294\n",
      "l2 norm: 904.8130216954569\n",
      "l1 norm: 758.7417123882027\n",
      "Rbeta: 904.985775848167\n",
      "\n",
      "Train set: Avg. loss: 0.000040611, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.29603 111.0003\n",
      "l2 norm: 904.7743420632855\n",
      "l1 norm: 758.7099448925966\n",
      "Rbeta: 904.9466495037366\n",
      "\n",
      "Train set: Avg. loss: 0.000040540, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.31848 111.01761\n",
      "l2 norm: 904.731765854734\n",
      "l1 norm: 758.6748580100591\n",
      "Rbeta: 904.9035508554852\n",
      "\n",
      "Train set: Avg. loss: 0.000040469, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.34094 111.034874\n",
      "l2 norm: 904.684033880604\n",
      "l1 norm: 758.6354129130705\n",
      "Rbeta: 904.8554290201487\n",
      "\n",
      "Train set: Avg. loss: 0.000040399, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.3634 111.052124\n",
      "l2 norm: 904.6346145565142\n",
      "l1 norm: 758.594543922896\n",
      "Rbeta: 904.8055277910564\n",
      "\n",
      "Train set: Avg. loss: 0.000040328, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.385864 111.06935\n",
      "l2 norm: 904.5856171934676\n",
      "l1 norm: 758.554037940052\n",
      "Rbeta: 904.7560336997258\n",
      "\n",
      "Train set: Avg. loss: 0.000040258, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.40832 111.08655\n",
      "l2 norm: 904.5370718542496\n",
      "l1 norm: 758.5139121379378\n",
      "Rbeta: 904.7069948444519\n",
      "\n",
      "Train set: Avg. loss: 0.000040188, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.43072 111.103714\n",
      "l2 norm: 904.4879893209506\n",
      "l1 norm: 758.4733273165987\n",
      "Rbeta: 904.6574811326603\n",
      "\n",
      "Train set: Avg. loss: 0.000040120, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.452545 111.1208\n",
      "l2 norm: 904.4425566898127\n",
      "l1 norm: 758.4357768497553\n",
      "Rbeta: 904.6115867086395\n",
      "\n",
      "Train set: Avg. loss: 0.000040052, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.47399 111.137856\n",
      "l2 norm: 904.4077993375497\n",
      "l1 norm: 758.407193383991\n",
      "Rbeta: 904.5764535132325\n",
      "\n",
      "Train set: Avg. loss: 0.000039984, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.49544 111.15489\n",
      "l2 norm: 904.3756690564521\n",
      "l1 norm: 758.3808233516453\n",
      "Rbeta: 904.5438856850483\n",
      "\n",
      "Train set: Avg. loss: 0.000039917, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.51689 111.17192\n",
      "l2 norm: 904.34898526439\n",
      "l1 norm: 758.3590344719692\n",
      "Rbeta: 904.516850650131\n",
      "\n",
      "Train set: Avg. loss: 0.000039850, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.53834 111.18889\n",
      "l2 norm: 904.3194136301529\n",
      "l1 norm: 758.3348213925908\n",
      "Rbeta: 904.4868433644128\n",
      "\n",
      "Train set: Avg. loss: 0.000039783, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.5598 111.20583\n",
      "l2 norm: 904.2890764969709\n",
      "l1 norm: 758.309956924833\n",
      "Rbeta: 904.4560537901107\n",
      "\n",
      "Train set: Avg. loss: 0.000039716, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.58125 111.22274\n",
      "l2 norm: 904.259334990653\n",
      "l1 norm: 758.2856370959514\n",
      "Rbeta: 904.4258751634087\n",
      "\n",
      "Train set: Avg. loss: 0.000039649, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.60272 111.239655\n",
      "l2 norm: 904.2296705903909\n",
      "l1 norm: 758.2614067243871\n",
      "Rbeta: 904.3958909411466\n",
      "\n",
      "Train set: Avg. loss: 0.000039583, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.62398 111.25656\n",
      "l2 norm: 904.1974548469726\n",
      "l1 norm: 758.2350175344944\n",
      "Rbeta: 904.3632474546355\n",
      "\n",
      "Train set: Avg. loss: 0.000039517, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.64502 111.27345\n",
      "l2 norm: 904.1573511122808\n",
      "l1 norm: 758.2020401136992\n",
      "Rbeta: 904.3227140004171\n",
      "\n",
      "Train set: Avg. loss: 0.000039451, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.66608 111.29034\n",
      "l2 norm: 904.1223132643003\n",
      "l1 norm: 758.1733002023337\n",
      "Rbeta: 904.287381131599\n",
      "\n",
      "Train set: Avg. loss: 0.000039385, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.687126 111.30718\n",
      "l2 norm: 904.0881676481633\n",
      "l1 norm: 758.1452626292196\n",
      "Rbeta: 904.2528127191957\n",
      "\n",
      "Train set: Avg. loss: 0.000039320, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.708176 111.324\n",
      "l2 norm: 904.0525240174206\n",
      "l1 norm: 758.1159923299876\n",
      "Rbeta: 904.2167423369183\n",
      "\n",
      "Train set: Avg. loss: 0.000039255, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.729225 111.3408\n",
      "l2 norm: 904.0125516247855\n",
      "l1 norm: 758.0830651150472\n",
      "Rbeta: 904.1764053995694\n",
      "\n",
      "Train set: Avg. loss: 0.000039190, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.750275 111.35757\n",
      "l2 norm: 903.9731389294274\n",
      "l1 norm: 758.0505971201667\n",
      "Rbeta: 904.1366271998552\n",
      "\n",
      "Train set: Avg. loss: 0.000039125, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.77132 111.374306\n",
      "l2 norm: 903.9384286520168\n",
      "l1 norm: 758.0221192759359\n",
      "Rbeta: 904.1015128467299\n",
      "\n",
      "Train set: Avg. loss: 0.000039060, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.79237 111.39101\n",
      "l2 norm: 903.9047960935601\n",
      "l1 norm: 757.9945478679846\n",
      "Rbeta: 904.0674543470186\n",
      "\n",
      "Train set: Avg. loss: 0.000038995, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.81343 111.4077\n",
      "l2 norm: 903.8689036827494\n",
      "l1 norm: 757.9650540454438\n",
      "Rbeta: 904.031188031626\n",
      "\n",
      "Train set: Avg. loss: 0.000038931, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.83449 111.42437\n",
      "l2 norm: 903.83168294198\n",
      "l1 norm: 757.9344230131462\n",
      "Rbeta: 903.993533903876\n",
      "\n",
      "Train set: Avg. loss: 0.000038866, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.85555 111.44103\n",
      "l2 norm: 903.793779617609\n",
      "l1 norm: 757.9031831588305\n",
      "Rbeta: 903.9552924714949\n",
      "\n",
      "Train set: Avg. loss: 0.000038802, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.87662 111.4577\n",
      "l2 norm: 903.7595778704381\n",
      "l1 norm: 757.8750273231467\n",
      "Rbeta: 903.9207311014484\n",
      "\n",
      "Train set: Avg. loss: 0.000038738, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.89768 111.47433\n",
      "l2 norm: 903.7263475003305\n",
      "l1 norm: 757.8477129900768\n",
      "Rbeta: 903.8871055818126\n",
      "\n",
      "Train set: Avg. loss: 0.000038674, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.91875 111.49092\n",
      "l2 norm: 903.6920009431967\n",
      "l1 norm: 757.8194695614197\n",
      "Rbeta: 903.8523697043211\n",
      "\n",
      "Train set: Avg. loss: 0.000038610, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.93982 111.50749\n",
      "l2 norm: 903.6565924600217\n",
      "l1 norm: 757.7903130158735\n",
      "Rbeta: 903.8165871285768\n",
      "\n",
      "Train set: Avg. loss: 0.000038546, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.96089 111.52403\n",
      "l2 norm: 903.624985914289\n",
      "l1 norm: 757.7643817244264\n",
      "Rbeta: 903.784575869231\n",
      "\n",
      "Train set: Avg. loss: 0.000038483, Accuracy: 512/512 (100%)\n",
      "\n",
      "107.981964 111.54062\n",
      "l2 norm: 903.5960672844125\n",
      "l1 norm: 757.7407681681916\n",
      "Rbeta: 903.7552612069059\n",
      "\n",
      "Train set: Avg. loss: 0.000038419, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.003044 111.5572\n",
      "l2 norm: 903.5662534030467\n",
      "l1 norm: 757.7164334340081\n",
      "Rbeta: 903.7250716322185\n",
      "\n",
      "Train set: Avg. loss: 0.000038356, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.024124 111.57376\n",
      "l2 norm: 903.5380096557357\n",
      "l1 norm: 757.6933802574035\n",
      "Rbeta: 903.6964387026575\n",
      "\n",
      "Train set: Avg. loss: 0.000038293, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.0452 111.59029\n",
      "l2 norm: 903.51119625764\n",
      "l1 norm: 757.6715273416117\n",
      "Rbeta: 903.6691803521514\n",
      "\n",
      "Train set: Avg. loss: 0.000038229, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.066284 111.60683\n",
      "l2 norm: 903.4826531022092\n",
      "l1 norm: 757.6482245575331\n",
      "Rbeta: 903.6403414068151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000038166, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.08737 111.62334\n",
      "l2 norm: 903.4572369392547\n",
      "l1 norm: 757.6275060255149\n",
      "Rbeta: 903.6144898116022\n",
      "\n",
      "Train set: Avg. loss: 0.000038104, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.10846 111.63986\n",
      "l2 norm: 903.4363516857708\n",
      "l1 norm: 757.6105666805099\n",
      "Rbeta: 903.5932512453844\n",
      "\n",
      "Train set: Avg. loss: 0.000038041, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.12955 111.65636\n",
      "l2 norm: 903.4131131813915\n",
      "l1 norm: 757.5916286350929\n",
      "Rbeta: 903.5696259930588\n",
      "\n",
      "Train set: Avg. loss: 0.000037978, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.15048 111.672806\n",
      "l2 norm: 903.3880118295201\n",
      "l1 norm: 757.5711475776354\n",
      "Rbeta: 903.5441432363414\n",
      "\n",
      "Train set: Avg. loss: 0.000037917, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.17117 111.689224\n",
      "l2 norm: 903.3659659819701\n",
      "l1 norm: 757.5532248037028\n",
      "Rbeta: 903.5218021616308\n",
      "\n",
      "Train set: Avg. loss: 0.000037855, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.19186 111.7056\n",
      "l2 norm: 903.3477435370438\n",
      "l1 norm: 757.53851867345\n",
      "Rbeta: 903.5031538278429\n",
      "\n",
      "Train set: Avg. loss: 0.000037794, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.21254 111.72197\n",
      "l2 norm: 903.3289854329073\n",
      "l1 norm: 757.523311075995\n",
      "Rbeta: 903.484007422504\n",
      "\n",
      "Train set: Avg. loss: 0.000037732, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.23324 111.7383\n",
      "l2 norm: 903.3085531005703\n",
      "l1 norm: 757.5066917558772\n",
      "Rbeta: 903.4631947137323\n",
      "\n",
      "Train set: Avg. loss: 0.000037671, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.25392 111.75461\n",
      "l2 norm: 903.2880917984927\n",
      "l1 norm: 757.4900829432169\n",
      "Rbeta: 903.4423825901481\n",
      "\n",
      "Train set: Avg. loss: 0.000037610, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.27461 111.770905\n",
      "l2 norm: 903.2647162310668\n",
      "l1 norm: 757.4710363236005\n",
      "Rbeta: 903.418641753602\n",
      "\n",
      "Train set: Avg. loss: 0.000037549, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.2953 111.78717\n",
      "l2 norm: 903.2407051615227\n",
      "l1 norm: 757.4514226734638\n",
      "Rbeta: 903.3942875597594\n",
      "\n",
      "Train set: Avg. loss: 0.000037489, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.31555 111.80339\n",
      "l2 norm: 903.2161343428031\n",
      "l1 norm: 757.4313526234419\n",
      "Rbeta: 903.369430564592\n",
      "\n",
      "Train set: Avg. loss: 0.000037429, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.33559 111.81955\n",
      "l2 norm: 903.192506145874\n",
      "l1 norm: 757.4120891380899\n",
      "Rbeta: 903.3454445758471\n",
      "\n",
      "Train set: Avg. loss: 0.000037370, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.35562 111.8357\n",
      "l2 norm: 903.1641075725785\n",
      "l1 norm: 757.3888353964844\n",
      "Rbeta: 903.3167113838359\n",
      "\n",
      "Train set: Avg. loss: 0.000037311, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.375656 111.851814\n",
      "l2 norm: 903.1321711052165\n",
      "l1 norm: 757.3626191211533\n",
      "Rbeta: 903.2845039983766\n",
      "\n",
      "Train set: Avg. loss: 0.000037252, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.3957 111.867935\n",
      "l2 norm: 903.0957592599793\n",
      "l1 norm: 757.3326520340346\n",
      "Rbeta: 903.2477835613975\n",
      "\n",
      "Train set: Avg. loss: 0.000037193, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.41574 111.884026\n",
      "l2 norm: 903.0575107267746\n",
      "l1 norm: 757.3011773474856\n",
      "Rbeta: 903.2092424568718\n",
      "\n",
      "Train set: Avg. loss: 0.000037134, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.43579 111.900085\n",
      "l2 norm: 903.0225908755972\n",
      "l1 norm: 757.2724796715509\n",
      "Rbeta: 903.174012515258\n",
      "\n",
      "Train set: Avg. loss: 0.000037075, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.45583 111.91611\n",
      "l2 norm: 902.9878082604617\n",
      "l1 norm: 757.2438880102181\n",
      "Rbeta: 903.1388924088183\n",
      "\n",
      "Train set: Avg. loss: 0.000037017, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.47583 111.93209\n",
      "l2 norm: 902.9545022052969\n",
      "l1 norm: 757.2165202655705\n",
      "Rbeta: 903.1052861665219\n",
      "\n",
      "Train set: Avg. loss: 0.000036959, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.495255 111.94803\n",
      "l2 norm: 902.9260380578654\n",
      "l1 norm: 757.1932406114635\n",
      "Rbeta: 903.0764624807847\n",
      "\n",
      "Train set: Avg. loss: 0.000036902, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.514694 111.96393\n",
      "l2 norm: 902.8983858842422\n",
      "l1 norm: 757.1706248436939\n",
      "Rbeta: 903.0485243471926\n",
      "\n",
      "Train set: Avg. loss: 0.000036845, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.534134 111.979836\n",
      "l2 norm: 902.8671248528138\n",
      "l1 norm: 757.1449245098057\n",
      "Rbeta: 903.0170225640109\n",
      "\n",
      "Train set: Avg. loss: 0.000036788, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.55358 111.99573\n",
      "l2 norm: 902.838616125707\n",
      "l1 norm: 757.1215706079695\n",
      "Rbeta: 902.9882251321112\n",
      "\n",
      "Train set: Avg. loss: 0.000036731, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.572716 112.01162\n",
      "l2 norm: 902.8071499606815\n",
      "l1 norm: 757.0957717354813\n",
      "Rbeta: 902.9564747232938\n",
      "\n",
      "Train set: Avg. loss: 0.000036675, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.59175 112.0275\n",
      "l2 norm: 902.775975373051\n",
      "l1 norm: 757.0701811528402\n",
      "Rbeta: 902.9250153318186\n",
      "\n",
      "Train set: Avg. loss: 0.000036619, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.61079 112.04339\n",
      "l2 norm: 902.7411320340236\n",
      "l1 norm: 757.0415310088326\n",
      "Rbeta: 902.8899419865425\n",
      "\n",
      "Train set: Avg. loss: 0.000036563, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.62983 112.05925\n",
      "l2 norm: 902.7090095713455\n",
      "l1 norm: 757.0151728127079\n",
      "Rbeta: 902.8575720289242\n",
      "\n",
      "Train set: Avg. loss: 0.000036507, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.64888 112.0751\n",
      "l2 norm: 902.6744604224306\n",
      "l1 norm: 756.9867378066704\n",
      "Rbeta: 902.8227860522704\n",
      "\n",
      "Train set: Avg. loss: 0.000036451, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.66792 112.09099\n",
      "l2 norm: 902.6380624590817\n",
      "l1 norm: 756.9567340777445\n",
      "Rbeta: 902.7861580521744\n",
      "\n",
      "Train set: Avg. loss: 0.000036395, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.686966 112.106865\n",
      "l2 norm: 902.6000533446083\n",
      "l1 norm: 756.9253972942047\n",
      "Rbeta: 902.7478322070631\n",
      "\n",
      "Train set: Avg. loss: 0.000036339, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.706024 112.12274\n",
      "l2 norm: 902.5626894388623\n",
      "l1 norm: 756.8945643536225\n",
      "Rbeta: 902.7103063286346\n",
      "\n",
      "Train set: Avg. loss: 0.000036284, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.72507 112.13859\n",
      "l2 norm: 902.524933109528\n",
      "l1 norm: 756.8634527974237\n",
      "Rbeta: 902.6722161476645\n",
      "\n",
      "Train set: Avg. loss: 0.000036228, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.744125 112.1544\n",
      "l2 norm: 902.4839186076202\n",
      "l1 norm: 756.8296183559412\n",
      "Rbeta: 902.6310084704261\n",
      "\n",
      "Train set: Avg. loss: 0.000036173, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.76318 112.17019\n",
      "l2 norm: 902.4377263845466\n",
      "l1 norm: 756.7914307216267\n",
      "Rbeta: 902.5845829011386\n",
      "\n",
      "Train set: Avg. loss: 0.000036118, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.782135 112.18595\n",
      "l2 norm: 902.3974003137007\n",
      "l1 norm: 756.7581831526102\n",
      "Rbeta: 902.5439256030368\n",
      "\n",
      "Train set: Avg. loss: 0.000036064, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.80078 112.20166\n",
      "l2 norm: 902.357880960601\n",
      "l1 norm: 756.725595280365\n",
      "Rbeta: 902.5041655378689\n",
      "\n",
      "Train set: Avg. loss: 0.000036009, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.81944 112.21736\n",
      "l2 norm: 902.3200278839224\n",
      "l1 norm: 756.694364507925\n",
      "Rbeta: 902.4661783151917\n",
      "\n",
      "Train set: Avg. loss: 0.000035955, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.83809 112.232994\n",
      "l2 norm: 902.2871893116933\n",
      "l1 norm: 756.667305998644\n",
      "Rbeta: 902.4330471925741\n",
      "\n",
      "Train set: Avg. loss: 0.000035901, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.85675 112.2486\n",
      "l2 norm: 902.2544460866436\n",
      "l1 norm: 756.6403140831235\n",
      "Rbeta: 902.4000934138379\n",
      "\n",
      "Train set: Avg. loss: 0.000035847, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.87541 112.26421\n",
      "l2 norm: 902.2201263535142\n",
      "l1 norm: 756.6119877770022\n",
      "Rbeta: 902.3655610818784\n",
      "\n",
      "Train set: Avg. loss: 0.000035794, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.89407 112.27981\n",
      "l2 norm: 902.1844155270337\n",
      "l1 norm: 756.5825142250246\n",
      "Rbeta: 902.3296707144681\n",
      "\n",
      "Train set: Avg. loss: 0.000035740, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.912735 112.29539\n",
      "l2 norm: 902.1535770276718\n",
      "l1 norm: 756.5571361490929\n",
      "Rbeta: 902.2986056669722\n",
      "\n",
      "Train set: Avg. loss: 0.000035686, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.9314 112.31094\n",
      "l2 norm: 902.118446013922\n",
      "l1 norm: 756.5281570392135\n",
      "Rbeta: 902.2632608157578\n",
      "\n",
      "Train set: Avg. loss: 0.000035633, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.95006 112.32649\n",
      "l2 norm: 902.0764674081795\n",
      "l1 norm: 756.4934530642645\n",
      "Rbeta: 902.2210227403217\n",
      "\n",
      "Train set: Avg. loss: 0.000035579, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.968735 112.342026\n",
      "l2 norm: 902.0402912127745\n",
      "l1 norm: 756.4636074910024\n",
      "Rbeta: 902.1846108109582\n",
      "\n",
      "Train set: Avg. loss: 0.000035526, Accuracy: 512/512 (100%)\n",
      "\n",
      "108.9874 112.35757\n",
      "l2 norm: 902.0031808088459\n",
      "l1 norm: 756.432990121127\n",
      "Rbeta: 902.1473364253945\n",
      "\n",
      "Train set: Avg. loss: 0.000035473, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.00607 112.37306\n",
      "l2 norm: 901.9660128103613\n",
      "l1 norm: 756.4023219012028\n",
      "Rbeta: 902.1098901018829\n",
      "\n",
      "Train set: Avg. loss: 0.000035420, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.024734 112.38855\n",
      "l2 norm: 901.9280167378377\n",
      "l1 norm: 756.3709605694334\n",
      "Rbeta: 902.0716295722838\n",
      "\n",
      "Train set: Avg. loss: 0.000035367, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.04342 112.40399\n",
      "l2 norm: 901.8925642429477\n",
      "l1 norm: 756.3416996498686\n",
      "Rbeta: 902.036004755313\n",
      "\n",
      "Train set: Avg. loss: 0.000035314, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.061966 112.41939\n",
      "l2 norm: 901.8591138440743\n",
      "l1 norm: 756.3140923363424\n",
      "Rbeta: 902.0023228284958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000035263, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.08003 112.43477\n",
      "l2 norm: 901.8281105793961\n",
      "l1 norm: 756.2885322610862\n",
      "Rbeta: 901.971153148391\n",
      "\n",
      "Train set: Avg. loss: 0.000035211, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.0981 112.450134\n",
      "l2 norm: 901.798621508188\n",
      "l1 norm: 756.2641966176304\n",
      "Rbeta: 901.9414649380373\n",
      "\n",
      "Train set: Avg. loss: 0.000035159, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.116165 112.4655\n",
      "l2 norm: 901.7679698824405\n",
      "l1 norm: 756.2389238463201\n",
      "Rbeta: 901.9106388695559\n",
      "\n",
      "Train set: Avg. loss: 0.000035108, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.13423 112.48085\n",
      "l2 norm: 901.733768269548\n",
      "l1 norm: 756.2107049643291\n",
      "Rbeta: 901.8762618826423\n",
      "\n",
      "Train set: Avg. loss: 0.000035056, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.1523 112.49617\n",
      "l2 norm: 901.7014155031586\n",
      "l1 norm: 756.1840588930819\n",
      "Rbeta: 901.8436874602875\n",
      "\n",
      "Train set: Avg. loss: 0.000035006, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.1698 112.5115\n",
      "l2 norm: 901.6698375051334\n",
      "l1 norm: 756.1580634003494\n",
      "Rbeta: 901.8120065519182\n",
      "\n",
      "Train set: Avg. loss: 0.000034956, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.187256 112.52681\n",
      "l2 norm: 901.6370965073418\n",
      "l1 norm: 756.1310714580932\n",
      "Rbeta: 901.7790978111386\n",
      "\n",
      "Train set: Avg. loss: 0.000034906, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.20471 112.54208\n",
      "l2 norm: 901.6046266799557\n",
      "l1 norm: 756.1043052915883\n",
      "Rbeta: 901.7464049489847\n",
      "\n",
      "Train set: Avg. loss: 0.000034856, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.222176 112.55736\n",
      "l2 norm: 901.573021161948\n",
      "l1 norm: 756.0782785834599\n",
      "Rbeta: 901.714714866451\n",
      "\n",
      "Train set: Avg. loss: 0.000034806, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.23964 112.5726\n",
      "l2 norm: 901.542107698983\n",
      "l1 norm: 756.0528653170047\n",
      "Rbeta: 901.6836695654323\n",
      "\n",
      "Train set: Avg. loss: 0.000034757, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.256485 112.5878\n",
      "l2 norm: 901.5141266331362\n",
      "l1 norm: 756.0299553965292\n",
      "Rbeta: 901.6555387699848\n",
      "\n",
      "Train set: Avg. loss: 0.000034708, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.27333 112.602974\n",
      "l2 norm: 901.4926688701893\n",
      "l1 norm: 756.012570173275\n",
      "Rbeta: 901.6340166476185\n",
      "\n",
      "Train set: Avg. loss: 0.000034659, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.290184 112.61809\n",
      "l2 norm: 901.4728917380758\n",
      "l1 norm: 755.9966133002664\n",
      "Rbeta: 901.614047731745\n",
      "\n",
      "Train set: Avg. loss: 0.000034611, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.30704 112.633194\n",
      "l2 norm: 901.4538671731203\n",
      "l1 norm: 755.9813073687905\n",
      "Rbeta: 901.5949660852165\n",
      "\n",
      "Train set: Avg. loss: 0.000034562, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.3239 112.648285\n",
      "l2 norm: 901.4290340509609\n",
      "l1 norm: 755.9611082060264\n",
      "Rbeta: 901.5700447543979\n",
      "\n",
      "Train set: Avg. loss: 0.000034514, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.34075 112.66336\n",
      "l2 norm: 901.4029726457251\n",
      "l1 norm: 755.9398209368796\n",
      "Rbeta: 901.5438152477266\n",
      "\n",
      "Train set: Avg. loss: 0.000034465, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.357605 112.67848\n",
      "l2 norm: 901.3780372816749\n",
      "l1 norm: 755.9194802167301\n",
      "Rbeta: 901.5188095969596\n",
      "\n",
      "Train set: Avg. loss: 0.000034417, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.374466 112.69356\n",
      "l2 norm: 901.3542994776033\n",
      "l1 norm: 755.9001779300932\n",
      "Rbeta: 901.4948883724492\n",
      "\n",
      "Train set: Avg. loss: 0.000034369, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.39133 112.70863\n",
      "l2 norm: 901.3291577576341\n",
      "l1 norm: 755.8797307416071\n",
      "Rbeta: 901.4696667600905\n",
      "\n",
      "Train set: Avg. loss: 0.000034321, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.40819 112.72366\n",
      "l2 norm: 901.2962284151506\n",
      "l1 norm: 755.8527461793956\n",
      "Rbeta: 901.4366365172208\n",
      "\n",
      "Train set: Avg. loss: 0.000034273, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.42505 112.73868\n",
      "l2 norm: 901.2651216563848\n",
      "l1 norm: 755.8272755789843\n",
      "Rbeta: 901.4053547237072\n",
      "\n",
      "Train set: Avg. loss: 0.000034225, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.441925 112.753685\n",
      "l2 norm: 901.236182436009\n",
      "l1 norm: 755.8036534548045\n",
      "Rbeta: 901.3763799577665\n",
      "\n",
      "Train set: Avg. loss: 0.000034177, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.45877 112.76866\n",
      "l2 norm: 901.2041479648491\n",
      "l1 norm: 755.7774346631855\n",
      "Rbeta: 901.3442186961909\n",
      "\n",
      "Train set: Avg. loss: 0.000034131, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.474976 112.783585\n",
      "l2 norm: 901.1719347902657\n",
      "l1 norm: 755.7510380023912\n",
      "Rbeta: 901.3119531737766\n",
      "\n",
      "Train set: Avg. loss: 0.000034084, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.49118 112.79846\n",
      "l2 norm: 901.1426112157565\n",
      "l1 norm: 755.7270336115597\n",
      "Rbeta: 901.282562760059\n",
      "\n",
      "Train set: Avg. loss: 0.000034038, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.507385 112.81329\n",
      "l2 norm: 901.1168450479697\n",
      "l1 norm: 755.7060034984697\n",
      "Rbeta: 901.2566989780792\n",
      "\n",
      "Train set: Avg. loss: 0.000033991, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.52359 112.828094\n",
      "l2 norm: 901.0871350708982\n",
      "l1 norm: 755.6816444607032\n",
      "Rbeta: 901.2268555749221\n",
      "\n",
      "Train set: Avg. loss: 0.000033945, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.539795 112.842896\n",
      "l2 norm: 901.0623917911164\n",
      "l1 norm: 755.661418713955\n",
      "Rbeta: 901.2020291380318\n",
      "\n",
      "Train set: Avg. loss: 0.000033899, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.556015 112.857666\n",
      "l2 norm: 901.0403618071648\n",
      "l1 norm: 755.6434710190831\n",
      "Rbeta: 901.1799240846212\n",
      "\n",
      "Train set: Avg. loss: 0.000033853, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.57222 112.87243\n",
      "l2 norm: 901.016044292103\n",
      "l1 norm: 755.6236322380822\n",
      "Rbeta: 901.1555266438987\n",
      "\n",
      "Train set: Avg. loss: 0.000033807, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.58844 112.88714\n",
      "l2 norm: 900.993592789756\n",
      "l1 norm: 755.6053957987281\n",
      "Rbeta: 901.1330879182874\n",
      "\n",
      "Train set: Avg. loss: 0.000033761, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.60465 112.90183\n",
      "l2 norm: 900.966502215085\n",
      "l1 norm: 755.5832925927529\n",
      "Rbeta: 901.1058489492892\n",
      "\n",
      "Train set: Avg. loss: 0.000033715, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.620865 112.916534\n",
      "l2 norm: 900.9390480697291\n",
      "l1 norm: 755.5609316901823\n",
      "Rbeta: 901.0783003998978\n",
      "\n",
      "Train set: Avg. loss: 0.000033670, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.637085 112.93123\n",
      "l2 norm: 900.9104490665873\n",
      "l1 norm: 755.5375850828857\n",
      "Rbeta: 901.049619691994\n",
      "\n",
      "Train set: Avg. loss: 0.000033624, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.653305 112.94591\n",
      "l2 norm: 900.8792812658837\n",
      "l1 norm: 755.5120667808217\n",
      "Rbeta: 901.018387110783\n",
      "\n",
      "Train set: Avg. loss: 0.000033578, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.669525 112.96057\n",
      "l2 norm: 900.8466778300542\n",
      "l1 norm: 755.4853573959\n",
      "Rbeta: 900.9856909235137\n",
      "\n",
      "Train set: Avg. loss: 0.000033533, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.685745 112.975235\n",
      "l2 norm: 900.8143916835543\n",
      "l1 norm: 755.4589330334178\n",
      "Rbeta: 900.9533321137749\n",
      "\n",
      "Train set: Avg. loss: 0.000033488, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.70197 112.98987\n",
      "l2 norm: 900.7887610235277\n",
      "l1 norm: 755.4380929348745\n",
      "Rbeta: 900.9275957755029\n",
      "\n",
      "Train set: Avg. loss: 0.000033442, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.7182 113.004486\n",
      "l2 norm: 900.7630004684243\n",
      "l1 norm: 755.4171163055643\n",
      "Rbeta: 900.9018137359761\n",
      "\n",
      "Train set: Avg. loss: 0.000033397, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.73442 113.01909\n",
      "l2 norm: 900.736178150456\n",
      "l1 norm: 755.3952189955039\n",
      "Rbeta: 900.8748853697892\n",
      "\n",
      "Train set: Avg. loss: 0.000033352, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.750656 113.03368\n",
      "l2 norm: 900.7090680607954\n",
      "l1 norm: 755.3730783616346\n",
      "Rbeta: 900.847694664834\n",
      "\n",
      "Train set: Avg. loss: 0.000033307, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.766884 113.04826\n",
      "l2 norm: 900.6760075497605\n",
      "l1 norm: 755.3459328532497\n",
      "Rbeta: 900.8145978397374\n",
      "\n",
      "Train set: Avg. loss: 0.000033262, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.78311 113.06285\n",
      "l2 norm: 900.641790769653\n",
      "l1 norm: 755.3177994394713\n",
      "Rbeta: 900.7802572742441\n",
      "\n",
      "Train set: Avg. loss: 0.000033217, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.79933 113.07741\n",
      "l2 norm: 900.6114744961428\n",
      "l1 norm: 755.292949154321\n",
      "Rbeta: 900.749873878798\n",
      "\n",
      "Train set: Avg. loss: 0.000033172, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.81556 113.0919\n",
      "l2 norm: 900.5816215475546\n",
      "l1 norm: 755.2684792330033\n",
      "Rbeta: 900.7199667628056\n",
      "\n",
      "Train set: Avg. loss: 0.000033127, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.83178 113.10637\n",
      "l2 norm: 900.5480240109869\n",
      "l1 norm: 755.2408516547613\n",
      "Rbeta: 900.6861903406505\n",
      "\n",
      "Train set: Avg. loss: 0.000033083, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.848015 113.12082\n",
      "l2 norm: 900.5123441455713\n",
      "l1 norm: 755.2114723824361\n",
      "Rbeta: 900.6505431975545\n",
      "\n",
      "Train set: Avg. loss: 0.000033038, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.86424 113.13524\n",
      "l2 norm: 900.4768470586894\n",
      "l1 norm: 755.1822677509111\n",
      "Rbeta: 900.6149331879694\n",
      "\n",
      "Train set: Avg. loss: 0.000032994, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.88047 113.14966\n",
      "l2 norm: 900.4471998288359\n",
      "l1 norm: 755.1579647311673\n",
      "Rbeta: 900.585188084497\n",
      "\n",
      "Train set: Avg. loss: 0.000032950, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.8967 113.164055\n",
      "l2 norm: 900.4221083184901\n",
      "l1 norm: 755.1374903436084\n",
      "Rbeta: 900.5600582158685\n",
      "\n",
      "Train set: Avg. loss: 0.000032906, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.91293 113.17842\n",
      "l2 norm: 900.3964859033629\n",
      "l1 norm: 755.1165940619096\n",
      "Rbeta: 900.5343185307308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000032861, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.92917 113.19276\n",
      "l2 norm: 900.3728021578714\n",
      "l1 norm: 755.097350847527\n",
      "Rbeta: 900.5105221730621\n",
      "\n",
      "Train set: Avg. loss: 0.000032817, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.945404 113.20706\n",
      "l2 norm: 900.3494272463865\n",
      "l1 norm: 755.0783397878254\n",
      "Rbeta: 900.4870510450286\n",
      "\n",
      "Train set: Avg. loss: 0.000032774, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.96165 113.22138\n",
      "l2 norm: 900.3260289638853\n",
      "l1 norm: 755.0592457420901\n",
      "Rbeta: 900.4636491289331\n",
      "\n",
      "Train set: Avg. loss: 0.000032730, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.977875 113.23568\n",
      "l2 norm: 900.3011563444882\n",
      "l1 norm: 755.0388822709327\n",
      "Rbeta: 900.4386703190888\n",
      "\n",
      "Train set: Avg. loss: 0.000032686, Accuracy: 512/512 (100%)\n",
      "\n",
      "109.994125 113.24997\n",
      "l2 norm: 900.270636574188\n",
      "l1 norm: 755.0137893247705\n",
      "Rbeta: 900.4080717159987\n",
      "\n",
      "Train set: Avg. loss: 0.000032642, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.01037 113.26422\n",
      "l2 norm: 900.2420148944268\n",
      "l1 norm: 754.9902604980688\n",
      "Rbeta: 900.3793623501085\n",
      "\n",
      "Train set: Avg. loss: 0.000032599, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.02606 113.27844\n",
      "l2 norm: 900.2115316200318\n",
      "l1 norm: 754.9651461101254\n",
      "Rbeta: 900.3488042181306\n",
      "\n",
      "Train set: Avg. loss: 0.000032557, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.04164 113.29266\n",
      "l2 norm: 900.1803859676925\n",
      "l1 norm: 754.9394775899875\n",
      "Rbeta: 900.3176635117497\n",
      "\n",
      "Train set: Avg. loss: 0.000032515, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.05702 113.306854\n",
      "l2 norm: 900.1477498645653\n",
      "l1 norm: 754.9125412618421\n",
      "Rbeta: 900.2849618851617\n",
      "\n",
      "Train set: Avg. loss: 0.000032473, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.07193 113.32104\n",
      "l2 norm: 900.1134807781018\n",
      "l1 norm: 754.8842485154507\n",
      "Rbeta: 900.2507434808957\n",
      "\n",
      "Train set: Avg. loss: 0.000032432, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.08684 113.335175\n",
      "l2 norm: 900.0855995887815\n",
      "l1 norm: 754.8613008094184\n",
      "Rbeta: 900.222776595981\n",
      "\n",
      "Train set: Avg. loss: 0.000032391, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.10176 113.34927\n",
      "l2 norm: 900.0606968493718\n",
      "l1 norm: 754.8408605779517\n",
      "Rbeta: 900.1978915424335\n",
      "\n",
      "Train set: Avg. loss: 0.000032349, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.11667 113.36337\n",
      "l2 norm: 900.0364027199649\n",
      "l1 norm: 754.820970054443\n",
      "Rbeta: 900.1735471935187\n",
      "\n",
      "Train set: Avg. loss: 0.000032308, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.13159 113.37747\n",
      "l2 norm: 900.0097504040942\n",
      "l1 norm: 754.7991201452744\n",
      "Rbeta: 900.1468950612735\n",
      "\n",
      "Train set: Avg. loss: 0.000032267, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.14651 113.39158\n",
      "l2 norm: 899.9802987668448\n",
      "l1 norm: 754.7749167697274\n",
      "Rbeta: 900.117496954284\n",
      "\n",
      "Train set: Avg. loss: 0.000032226, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.16143 113.40565\n",
      "l2 norm: 899.9498365497969\n",
      "l1 norm: 754.7498415925058\n",
      "Rbeta: 900.0870135615382\n",
      "\n",
      "Train set: Avg. loss: 0.000032185, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.176346 113.41969\n",
      "l2 norm: 899.9212165211815\n",
      "l1 norm: 754.7263061916105\n",
      "Rbeta: 900.0583608574373\n",
      "\n",
      "Train set: Avg. loss: 0.000032145, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.19127 113.433716\n",
      "l2 norm: 899.8987902769121\n",
      "l1 norm: 754.7079708245999\n",
      "Rbeta: 900.0359174289202\n",
      "\n",
      "Train set: Avg. loss: 0.000032104, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.20619 113.44775\n",
      "l2 norm: 899.8726224009835\n",
      "l1 norm: 754.6865138472618\n",
      "Rbeta: 900.0097571067117\n",
      "\n",
      "Train set: Avg. loss: 0.000032063, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.221115 113.46178\n",
      "l2 norm: 899.851304409148\n",
      "l1 norm: 754.6691241369709\n",
      "Rbeta: 899.9884687646346\n",
      "\n",
      "Train set: Avg. loss: 0.000032022, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.23605 113.4758\n",
      "l2 norm: 899.8293638537723\n",
      "l1 norm: 754.6512326876955\n",
      "Rbeta: 899.9665395519725\n",
      "\n",
      "Train set: Avg. loss: 0.000031982, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.25098 113.48984\n",
      "l2 norm: 899.8054161377994\n",
      "l1 norm: 754.6316471998286\n",
      "Rbeta: 899.9425673903911\n",
      "\n",
      "Train set: Avg. loss: 0.000031941, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.2659 113.503876\n",
      "l2 norm: 899.7823355792507\n",
      "l1 norm: 754.6127773712576\n",
      "Rbeta: 899.9194655808539\n",
      "\n",
      "Train set: Avg. loss: 0.000031900, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.28083 113.51794\n",
      "l2 norm: 899.7575234586684\n",
      "l1 norm: 754.5924630514388\n",
      "Rbeta: 899.8947143004882\n",
      "\n",
      "Train set: Avg. loss: 0.000031860, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.29576 113.53197\n",
      "l2 norm: 899.732785115333\n",
      "l1 norm: 754.5722313041265\n",
      "Rbeta: 899.8699559201752\n",
      "\n",
      "Train set: Avg. loss: 0.000031820, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.31069 113.545975\n",
      "l2 norm: 899.7084908504429\n",
      "l1 norm: 754.5523520496251\n",
      "Rbeta: 899.8456216315162\n",
      "\n",
      "Train set: Avg. loss: 0.000031779, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.32562 113.55994\n",
      "l2 norm: 899.6842034076717\n",
      "l1 norm: 754.5325003348819\n",
      "Rbeta: 899.8213339632106\n",
      "\n",
      "Train set: Avg. loss: 0.000031739, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.34056 113.57388\n",
      "l2 norm: 899.6627355398015\n",
      "l1 norm: 754.5150066984103\n",
      "Rbeta: 899.7998482034565\n",
      "\n",
      "Train set: Avg. loss: 0.000031699, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.3555 113.5878\n",
      "l2 norm: 899.6446456679688\n",
      "l1 norm: 754.5003436677998\n",
      "Rbeta: 899.7818018793864\n",
      "\n",
      "Train set: Avg. loss: 0.000031659, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.37007 113.601685\n",
      "l2 norm: 899.6255035795645\n",
      "l1 norm: 754.4847992993083\n",
      "Rbeta: 899.762651908566\n",
      "\n",
      "Train set: Avg. loss: 0.000031620, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.38434 113.615555\n",
      "l2 norm: 899.6061156184488\n",
      "l1 norm: 754.4690347521464\n",
      "Rbeta: 899.7432975839221\n",
      "\n",
      "Train set: Avg. loss: 0.000031581, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.398605 113.6294\n",
      "l2 norm: 899.5832061945355\n",
      "l1 norm: 754.4503273510238\n",
      "Rbeta: 899.7204416774936\n",
      "\n",
      "Train set: Avg. loss: 0.000031542, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.41287 113.64322\n",
      "l2 norm: 899.5595892402947\n",
      "l1 norm: 754.4310242862452\n",
      "Rbeta: 899.6968166810601\n",
      "\n",
      "Train set: Avg. loss: 0.000031504, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.42714 113.65701\n",
      "l2 norm: 899.5382341903608\n",
      "l1 norm: 754.4135881650614\n",
      "Rbeta: 899.6755058584673\n",
      "\n",
      "Train set: Avg. loss: 0.000031465, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.44141 113.67079\n",
      "l2 norm: 899.513305424745\n",
      "l1 norm: 754.3931861326873\n",
      "Rbeta: 899.6506421472199\n",
      "\n",
      "Train set: Avg. loss: 0.000031426, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.45567 113.68455\n",
      "l2 norm: 899.4917195335247\n",
      "l1 norm: 754.3755930572252\n",
      "Rbeta: 899.6290607211795\n",
      "\n",
      "Train set: Avg. loss: 0.000031388, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.46994 113.69829\n",
      "l2 norm: 899.4701622591687\n",
      "l1 norm: 754.3580156751588\n",
      "Rbeta: 899.6075246463041\n",
      "\n",
      "Train set: Avg. loss: 0.000031349, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.48421 113.712006\n",
      "l2 norm: 899.446801741982\n",
      "l1 norm: 754.3389111073317\n",
      "Rbeta: 899.5842125755635\n",
      "\n",
      "Train set: Avg. loss: 0.000031310, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.498474 113.72571\n",
      "l2 norm: 899.4241072090111\n",
      "l1 norm: 754.3203642199754\n",
      "Rbeta: 899.5615454201329\n",
      "\n",
      "Train set: Avg. loss: 0.000031272, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.51274 113.739365\n",
      "l2 norm: 899.4010913164045\n",
      "l1 norm: 754.3015462187341\n",
      "Rbeta: 899.5385401350518\n",
      "\n",
      "Train set: Avg. loss: 0.000031234, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.52701 113.752975\n",
      "l2 norm: 899.3820465021223\n",
      "l1 norm: 754.2860373043143\n",
      "Rbeta: 899.519443821238\n",
      "\n",
      "Train set: Avg. loss: 0.000031196, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.54128 113.76659\n",
      "l2 norm: 899.3632086915201\n",
      "l1 norm: 754.2707042556156\n",
      "Rbeta: 899.5006218145895\n",
      "\n",
      "Train set: Avg. loss: 0.000031158, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.55553 113.7802\n",
      "l2 norm: 899.3421229194541\n",
      "l1 norm: 754.2535156090389\n",
      "Rbeta: 899.47959445461\n",
      "\n",
      "Train set: Avg. loss: 0.000031120, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.56918 113.79379\n",
      "l2 norm: 899.3241965565362\n",
      "l1 norm: 754.2389723102401\n",
      "Rbeta: 899.4617145511564\n",
      "\n",
      "Train set: Avg. loss: 0.000031083, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.58268 113.8074\n",
      "l2 norm: 899.2996303180794\n",
      "l1 norm: 754.2188635427098\n",
      "Rbeta: 899.4372648047255\n",
      "\n",
      "Train set: Avg. loss: 0.000031047, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.59572 113.82097\n",
      "l2 norm: 899.2736021683303\n",
      "l1 norm: 754.1975137975038\n",
      "Rbeta: 899.4113487913439\n",
      "\n",
      "Train set: Avg. loss: 0.000031011, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.60875 113.834526\n",
      "l2 norm: 899.2466559824404\n",
      "l1 norm: 754.1753734746857\n",
      "Rbeta: 899.3844051629536\n",
      "\n",
      "Train set: Avg. loss: 0.000030975, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.62178 113.84808\n",
      "l2 norm: 899.2184965454728\n",
      "l1 norm: 754.1522031217321\n",
      "Rbeta: 899.3564030896381\n",
      "\n",
      "Train set: Avg. loss: 0.000030939, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.63481 113.8616\n",
      "l2 norm: 899.1865927907693\n",
      "l1 norm: 754.1258718952507\n",
      "Rbeta: 899.3245754985095\n",
      "\n",
      "Train set: Avg. loss: 0.000030902, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.647835 113.8751\n",
      "l2 norm: 899.1534829292808\n",
      "l1 norm: 754.0985539512235\n",
      "Rbeta: 899.2915083068867\n",
      "\n",
      "Train set: Avg. loss: 0.000030867, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.660866 113.88859\n",
      "l2 norm: 899.1237614517336\n",
      "l1 norm: 754.074095551824\n",
      "Rbeta: 899.2618858192733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000030831, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.67389 113.902054\n",
      "l2 norm: 899.0965868166016\n",
      "l1 norm: 754.0517963728396\n",
      "Rbeta: 899.2347791262521\n",
      "\n",
      "Train set: Avg. loss: 0.000030795, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.68691 113.91554\n",
      "l2 norm: 899.0709299820105\n",
      "l1 norm: 754.0307580376598\n",
      "Rbeta: 899.209189955484\n",
      "\n",
      "Train set: Avg. loss: 0.000030759, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.699936 113.92904\n",
      "l2 norm: 899.0451213420398\n",
      "l1 norm: 754.009610020252\n",
      "Rbeta: 899.1835084536557\n",
      "\n",
      "Train set: Avg. loss: 0.000030723, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.71295 113.9425\n",
      "l2 norm: 899.0191732363717\n",
      "l1 norm: 753.9883210500478\n",
      "Rbeta: 899.1576587123756\n",
      "\n",
      "Train set: Avg. loss: 0.000030688, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.72598 113.95596\n",
      "l2 norm: 898.9925944743244\n",
      "l1 norm: 753.9664854072578\n",
      "Rbeta: 899.1311731541355\n",
      "\n",
      "Train set: Avg. loss: 0.000030652, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.73901 113.96942\n",
      "l2 norm: 898.9692211645225\n",
      "l1 norm: 753.9473173709487\n",
      "Rbeta: 899.1079118297649\n",
      "\n",
      "Train set: Avg. loss: 0.000030616, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.75204 113.982864\n",
      "l2 norm: 898.9450039697573\n",
      "l1 norm: 753.9274485366686\n",
      "Rbeta: 899.0837311289749\n",
      "\n",
      "Train set: Avg. loss: 0.000030581, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.76506 113.99626\n",
      "l2 norm: 898.918360400353\n",
      "l1 norm: 753.9055360774036\n",
      "Rbeta: 899.0571846184125\n",
      "\n",
      "Train set: Avg. loss: 0.000030545, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.77809 114.0096\n",
      "l2 norm: 898.8900350209867\n",
      "l1 norm: 753.8822139088957\n",
      "Rbeta: 899.0289015935526\n",
      "\n",
      "Train set: Avg. loss: 0.000030510, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.79112 114.02294\n",
      "l2 norm: 898.8628709252098\n",
      "l1 norm: 753.8598768952761\n",
      "Rbeta: 899.0018055097722\n",
      "\n",
      "Train set: Avg. loss: 0.000030475, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.804016 114.036255\n",
      "l2 norm: 898.8363266997793\n",
      "l1 norm: 753.8380399379454\n",
      "Rbeta: 898.9753927661699\n",
      "\n",
      "Train set: Avg. loss: 0.000030441, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.81642 114.049545\n",
      "l2 norm: 898.8105148113871\n",
      "l1 norm: 753.8168261093991\n",
      "Rbeta: 898.9496983703636\n",
      "\n",
      "Train set: Avg. loss: 0.000030406, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.828835 114.062805\n",
      "l2 norm: 898.7836538076423\n",
      "l1 norm: 753.7947237360509\n",
      "Rbeta: 898.9229255806164\n",
      "\n",
      "Train set: Avg. loss: 0.000030372, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.84125 114.07604\n",
      "l2 norm: 898.7571285563241\n",
      "l1 norm: 753.7728985466117\n",
      "Rbeta: 898.8965887540678\n",
      "\n",
      "Train set: Avg. loss: 0.000030338, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.85367 114.089264\n",
      "l2 norm: 898.7318591873847\n",
      "l1 norm: 753.752128281928\n",
      "Rbeta: 898.8714220895295\n",
      "\n",
      "Train set: Avg. loss: 0.000030304, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.86607 114.10245\n",
      "l2 norm: 898.7111638215634\n",
      "l1 norm: 753.7351965818445\n",
      "Rbeta: 898.8507889468143\n",
      "\n",
      "Train set: Avg. loss: 0.000030270, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.878494 114.11565\n",
      "l2 norm: 898.6898301010864\n",
      "l1 norm: 753.717741004491\n",
      "Rbeta: 898.8296412849053\n",
      "\n",
      "Train set: Avg. loss: 0.000030236, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.8909 114.128815\n",
      "l2 norm: 898.6676249075116\n",
      "l1 norm: 753.6995473130305\n",
      "Rbeta: 898.8074606266265\n",
      "\n",
      "Train set: Avg. loss: 0.000030202, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.90332 114.14198\n",
      "l2 norm: 898.641967163083\n",
      "l1 norm: 753.67845343213\n",
      "Rbeta: 898.781923779897\n",
      "\n",
      "Train set: Avg. loss: 0.000030168, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.91574 114.155136\n",
      "l2 norm: 898.6113567083061\n",
      "l1 norm: 753.653208838941\n",
      "Rbeta: 898.7514132012394\n",
      "\n",
      "Train set: Avg. loss: 0.000030134, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.92816 114.16829\n",
      "l2 norm: 898.5817776903224\n",
      "l1 norm: 753.6288318411935\n",
      "Rbeta: 898.7219810050716\n",
      "\n",
      "Train set: Avg. loss: 0.000030100, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.940575 114.181465\n",
      "l2 norm: 898.5524201452635\n",
      "l1 norm: 753.6046506021999\n",
      "Rbeta: 898.6927252551852\n",
      "\n",
      "Train set: Avg. loss: 0.000030067, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.952995 114.19465\n",
      "l2 norm: 898.5217637765821\n",
      "l1 norm: 753.5793905402221\n",
      "Rbeta: 898.66214926851\n",
      "\n",
      "Train set: Avg. loss: 0.000030033, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.965416 114.207855\n",
      "l2 norm: 898.4940074475662\n",
      "l1 norm: 753.5565706874397\n",
      "Rbeta: 898.6345898772381\n",
      "\n",
      "Train set: Avg. loss: 0.000029999, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.97784 114.22102\n",
      "l2 norm: 898.4684383142802\n",
      "l1 norm: 753.5355975006154\n",
      "Rbeta: 898.6090390102626\n",
      "\n",
      "Train set: Avg. loss: 0.000029965, Accuracy: 512/512 (100%)\n",
      "\n",
      "110.99026 114.23418\n",
      "l2 norm: 898.4417053763382\n",
      "l1 norm: 753.5135913931015\n",
      "Rbeta: 898.5824269561493\n",
      "\n",
      "Train set: Avg. loss: 0.000029932, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.00268 114.247314\n",
      "l2 norm: 898.4197546759503\n",
      "l1 norm: 753.4955999734354\n",
      "Rbeta: 898.5606296992135\n",
      "\n",
      "Train set: Avg. loss: 0.000029898, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.01511 114.26044\n",
      "l2 norm: 898.3998782926984\n",
      "l1 norm: 753.4793312703896\n",
      "Rbeta: 898.5408529192204\n",
      "\n",
      "Train set: Avg. loss: 0.000029865, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.0272 114.273544\n",
      "l2 norm: 898.3831287551402\n",
      "l1 norm: 753.4656602054235\n",
      "Rbeta: 898.5242280460353\n",
      "\n",
      "Train set: Avg. loss: 0.000029833, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.039 114.28664\n",
      "l2 norm: 898.3615684788465\n",
      "l1 norm: 753.4479541703902\n",
      "Rbeta: 898.5028060855543\n",
      "\n",
      "Train set: Avg. loss: 0.000029800, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.05081 114.2997\n",
      "l2 norm: 898.3401540986243\n",
      "l1 norm: 753.4303827942563\n",
      "Rbeta: 898.4815425609339\n",
      "\n",
      "Train set: Avg. loss: 0.000029768, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.06261 114.31276\n",
      "l2 norm: 898.3202283819458\n",
      "l1 norm: 753.4140322434436\n",
      "Rbeta: 898.4617183175599\n",
      "\n",
      "Train set: Avg. loss: 0.000029735, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.07442 114.325806\n",
      "l2 norm: 898.2984332421007\n",
      "l1 norm: 753.3961056421064\n",
      "Rbeta: 898.4400849595871\n",
      "\n",
      "Train set: Avg. loss: 0.000029703, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.08623 114.33885\n",
      "l2 norm: 898.2765178209997\n",
      "l1 norm: 753.3780899632718\n",
      "Rbeta: 898.4183507038392\n",
      "\n",
      "Train set: Avg. loss: 0.000029670, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.09804 114.35188\n",
      "l2 norm: 898.2493455476651\n",
      "l1 norm: 753.3556796537052\n",
      "Rbeta: 898.3913335573124\n",
      "\n",
      "Train set: Avg. loss: 0.000029638, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.10984 114.3649\n",
      "l2 norm: 898.2176808186266\n",
      "l1 norm: 753.3295539341113\n",
      "Rbeta: 898.3597221259097\n",
      "\n",
      "Train set: Avg. loss: 0.000029606, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.12165 114.37793\n",
      "l2 norm: 898.1849938812428\n",
      "l1 norm: 753.3025882980496\n",
      "Rbeta: 898.327174915875\n",
      "\n",
      "Train set: Avg. loss: 0.000029574, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.13346 114.390945\n",
      "l2 norm: 898.1548069951267\n",
      "l1 norm: 753.2777336123714\n",
      "Rbeta: 898.2971964476196\n",
      "\n",
      "Train set: Avg. loss: 0.000029541, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.14527 114.40395\n",
      "l2 norm: 898.1264449613105\n",
      "l1 norm: 753.2544060683567\n",
      "Rbeta: 898.2689175090709\n",
      "\n",
      "Train set: Avg. loss: 0.000029509, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.15708 114.41696\n",
      "l2 norm: 898.0938939499426\n",
      "l1 norm: 753.2275533547256\n",
      "Rbeta: 898.2364892937429\n",
      "\n",
      "Train set: Avg. loss: 0.000029477, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.16889 114.42998\n",
      "l2 norm: 898.0595575289226\n",
      "l1 norm: 753.1991937196046\n",
      "Rbeta: 898.2023802503367\n",
      "\n",
      "Train set: Avg. loss: 0.000029445, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.18071 114.44299\n",
      "l2 norm: 898.02911077906\n",
      "l1 norm: 753.1741079885492\n",
      "Rbeta: 898.1720566222655\n",
      "\n",
      "Train set: Avg. loss: 0.000029413, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.19252 114.45599\n",
      "l2 norm: 897.9999057102601\n",
      "l1 norm: 753.1500841990172\n",
      "Rbeta: 898.1429594453547\n",
      "\n",
      "Train set: Avg. loss: 0.000029381, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.20433 114.46898\n",
      "l2 norm: 897.9740230347181\n",
      "l1 norm: 753.1288164928903\n",
      "Rbeta: 898.1172184901847\n",
      "\n",
      "Train set: Avg. loss: 0.000029349, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.21615 114.48192\n",
      "l2 norm: 897.9517558156132\n",
      "l1 norm: 753.1105774923938\n",
      "Rbeta: 898.0950301465215\n",
      "\n",
      "Train set: Avg. loss: 0.000029317, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.22797 114.49486\n",
      "l2 norm: 897.9277323195143\n",
      "l1 norm: 753.0908856942597\n",
      "Rbeta: 898.071204680404\n",
      "\n",
      "Train set: Avg. loss: 0.000029285, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.23978 114.50778\n",
      "l2 norm: 897.9040172203154\n",
      "l1 norm: 753.0714415970606\n",
      "Rbeta: 898.0475837120531\n",
      "\n",
      "Train set: Avg. loss: 0.000029254, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.2516 114.52069\n",
      "l2 norm: 897.8828686086584\n",
      "l1 norm: 753.054144651624\n",
      "Rbeta: 898.0266304784624\n",
      "\n",
      "Train set: Avg. loss: 0.000029222, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.26341 114.53357\n",
      "l2 norm: 897.859181408419\n",
      "l1 norm: 753.0347036406597\n",
      "Rbeta: 898.0030527286696\n",
      "\n",
      "Train set: Avg. loss: 0.000029190, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.27524 114.54642\n",
      "l2 norm: 897.8373036174461\n",
      "l1 norm: 753.0168110723738\n",
      "Rbeta: 897.9813362331729\n",
      "\n",
      "Train set: Avg. loss: 0.000029159, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.28704 114.55926\n",
      "l2 norm: 897.8144996820597\n",
      "l1 norm: 752.9981112496898\n",
      "Rbeta: 897.9586529786468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000029127, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.29886 114.57208\n",
      "l2 norm: 897.7917546630441\n",
      "l1 norm: 752.9794536179652\n",
      "Rbeta: 897.9360822042939\n",
      "\n",
      "Train set: Avg. loss: 0.000029096, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.31048 114.58488\n",
      "l2 norm: 897.7721854427826\n",
      "l1 norm: 752.9634662506978\n",
      "Rbeta: 897.9166497068659\n",
      "\n",
      "Train set: Avg. loss: 0.000029066, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.321625 114.59766\n",
      "l2 norm: 897.7529981219801\n",
      "l1 norm: 752.9477637353789\n",
      "Rbeta: 897.8976213852528\n",
      "\n",
      "Train set: Avg. loss: 0.000029035, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.33276 114.61043\n",
      "l2 norm: 897.7362141111154\n",
      "l1 norm: 752.9340911687104\n",
      "Rbeta: 897.8809830295015\n",
      "\n",
      "Train set: Avg. loss: 0.000029005, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.3439 114.62319\n",
      "l2 norm: 897.7249968955113\n",
      "l1 norm: 752.9250594779105\n",
      "Rbeta: 897.869983954338\n",
      "\n",
      "Train set: Avg. loss: 0.000028974, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.35504 114.63593\n",
      "l2 norm: 897.7147710770583\n",
      "l1 norm: 752.9168381526038\n",
      "Rbeta: 897.8598687109095\n",
      "\n",
      "Train set: Avg. loss: 0.000028944, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.36619 114.64865\n",
      "l2 norm: 897.7044775838842\n",
      "l1 norm: 752.9085596209115\n",
      "Rbeta: 897.8497558314423\n",
      "\n",
      "Train set: Avg. loss: 0.000028914, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.377335 114.66133\n",
      "l2 norm: 897.6926661066406\n",
      "l1 norm: 752.8989888352162\n",
      "Rbeta: 897.8381424173235\n",
      "\n",
      "Train set: Avg. loss: 0.000028883, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.38847 114.67401\n",
      "l2 norm: 897.6792253854508\n",
      "l1 norm: 752.8880751016\n",
      "Rbeta: 897.8248771192615\n",
      "\n",
      "Train set: Avg. loss: 0.000028853, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.39963 114.68669\n",
      "l2 norm: 897.6642239833285\n",
      "l1 norm: 752.8758258413959\n",
      "Rbeta: 897.8100817913696\n",
      "\n",
      "Train set: Avg. loss: 0.000028823, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.41077 114.699356\n",
      "l2 norm: 897.6507532588378\n",
      "l1 norm: 752.8648228057139\n",
      "Rbeta: 897.7967624869182\n",
      "\n",
      "Train set: Avg. loss: 0.000028793, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.42191 114.711975\n",
      "l2 norm: 897.6383518259242\n",
      "l1 norm: 752.8547359484962\n",
      "Rbeta: 897.7844331215256\n",
      "\n",
      "Train set: Avg. loss: 0.000028763, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.43306 114.72461\n",
      "l2 norm: 897.6270048603914\n",
      "l1 norm: 752.8455771390106\n",
      "Rbeta: 897.7732899451322\n",
      "\n",
      "Train set: Avg. loss: 0.000028733, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.44421 114.73724\n",
      "l2 norm: 897.6150175164373\n",
      "l1 norm: 752.8359039080722\n",
      "Rbeta: 897.7615122026118\n",
      "\n",
      "Train set: Avg. loss: 0.000028703, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.45536 114.74988\n",
      "l2 norm: 897.601350402819\n",
      "l1 norm: 752.8248646756667\n",
      "Rbeta: 897.7479777515413\n",
      "\n",
      "Train set: Avg. loss: 0.000028673, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.46651 114.7625\n",
      "l2 norm: 897.5882658598337\n",
      "l1 norm: 752.8143118602701\n",
      "Rbeta: 897.7350835672187\n",
      "\n",
      "Train set: Avg. loss: 0.000028643, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.47766 114.7751\n",
      "l2 norm: 897.5668650934816\n",
      "l1 norm: 752.7967828105791\n",
      "Rbeta: 897.7138519127102\n",
      "\n",
      "Train set: Avg. loss: 0.000028613, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.48881 114.787704\n",
      "l2 norm: 897.5464958831357\n",
      "l1 norm: 752.7801090270316\n",
      "Rbeta: 897.693687089687\n",
      "\n",
      "Train set: Avg. loss: 0.000028583, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.499954 114.80031\n",
      "l2 norm: 897.5267925177367\n",
      "l1 norm: 752.7640167893362\n",
      "Rbeta: 897.6741036058156\n",
      "\n",
      "Train set: Avg. loss: 0.000028554, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.51111 114.81291\n",
      "l2 norm: 897.5062272684152\n",
      "l1 norm: 752.7471941743909\n",
      "Rbeta: 897.6537367615655\n",
      "\n",
      "Train set: Avg. loss: 0.000028524, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.52226 114.825485\n",
      "l2 norm: 897.4867783292971\n",
      "l1 norm: 752.7313218044369\n",
      "Rbeta: 897.6344210767046\n",
      "\n",
      "Train set: Avg. loss: 0.000028494, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.53342 114.83806\n",
      "l2 norm: 897.470665992573\n",
      "l1 norm: 752.7182341027917\n",
      "Rbeta: 897.6185046510737\n",
      "\n",
      "Train set: Avg. loss: 0.000028465, Accuracy: 512/512 (100%)\n",
      "\n",
      "111.54457 114.8506\n",
      "l2 norm: 897.4577959741276\n",
      "l1 norm: 752.7078372116998\n",
      "Rbeta: 897.6057589986126\n",
      "After training:\n",
      "\n",
      "Train set: Avg. loss: 0.000028465, Accuracy: 512/512 (100%)\n",
      "\n",
      "tensor(49.) tensor(1.3789e+22)\n",
      "111.54457 114.8506\n",
      "l2 norm: 897.4577959741276\n",
      "l1 norm: 752.7078372116998\n",
      "Rbeta: 897.6057589986126\n",
      "Time domain:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency domain:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scale the initialization \n",
    "\n",
    "# Train and extract info about beta\n",
    "import seaborn as sns\n",
    "n_epochs = 100000\n",
    "learning_rate_start = 0.001\n",
    "momentum = 0.3\n",
    "initialization_scale = 0.01\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "def experiment(ker_size1, ker_size2, output_channels):\n",
    "  # print(class1, class2)\n",
    "    network = Net(ker_size1, ker_size2, output_channels)\n",
    "    network.initialize(initialization_scale, ker_size1)\n",
    "    optimizer =  optim.SGD(network.parameters(), lr=learning_rate_start, momentum=momentum)\n",
    "    print(\"Before training:\")\n",
    "    train_eval(network)\n",
    "    extract_info(network, False)\n",
    "    # test()\n",
    "    \n",
    "    print(\"Start training:\")\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "        train_minibatch(network, optimizer)\n",
    "        if epoch % 100 == 0:\n",
    "            loss = train_eval(network)\n",
    "#             if loss <= 1:\n",
    "            if loss <= 0.000001: # stop at 10^-6 loss \n",
    "                break\n",
    "            extract_info(network, False)\n",
    "        # After enough epochs, change the learning rate to be higher to expedite convergence\n",
    "\n",
    "        if epoch == 200 == 0:\n",
    "            optimizer =  optim.SGD(network.parameters(), lr=0.005, momentum=momentum)\n",
    "            print(\"Learning rate change\")\n",
    "              # optimizer =  optim.SGD(network.parameters(), lr=0.001, momentum=momentum)\n",
    "\n",
    "        if epoch == 500:\n",
    "            optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=momentum)\n",
    "            print(\"Learning rate change\")\n",
    "              # optimizer =  optim.SGD(network.parameters(), lr=0.001, momentum=momentum)\n",
    "\n",
    "#         if epoch == 1000:\n",
    "#             optimizer = optim.SGD(network.parameters(), lr=0.05, momentum=momentum)\n",
    "#               # print(\"Learning rate change\")\n",
    "#               # optimizer =  optim.SGD(network.parameters(), lr=0.005, momentum=momentum)\n",
    "\n",
    "#         if epoch == 1200:\n",
    "#             optimizer = optim.SGD(network.parameters(), lr=0.1, momentum=momentum)\n",
    "#               # optimizer =  optim.SGD(network.parameters(), lr=0.007, momentum=momentum)\n",
    "\n",
    "#         if epoch == 1500:\n",
    "#             optimizer = optim.SGD(network.parameters(), lr=0.5, momentum=momentum)\n",
    "\n",
    "#         if epoch == 2000:\n",
    "#             optimizer = optim.SGD(network.parameters(), lr=1, momentum=momentum)\n",
    "#               # optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=momentum)\n",
    "\n",
    "#         if epoch == 3000:\n",
    "#             optimizer = optim.SGD(network.parameters(), lr=2, momentum=momentum)\n",
    "#               # optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=momentum)\n",
    "\n",
    "#         if epoch == 4000:\n",
    "#             optimizer = optim.SGD(network.parameters(), lr=4, momentum=momentum)\n",
    "#               # optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=momentum)\n",
    "\n",
    "#         if epoch == 4500:\n",
    "#             optimizer = optim.SGD(network.parameters(), lr=10, momentum=momentum)\n",
    "\n",
    "#         if epoch == 5000:\n",
    "#             optimizer = optim.SGD(network.parameters(), lr=20, momentum=momentum)\n",
    "\n",
    "\n",
    "#         if epoch % 500 == 0:\n",
    "#             print(test(network))\n",
    "\n",
    "    print(\"After training:\")\n",
    "    train_eval(network)\n",
    "    (accuracy, losses) = test(network)\n",
    "    print(accuracy, losses)\n",
    "\n",
    "    (rk, beta) = extract_info(network, True)\n",
    "\n",
    "    return (rk, beta)\n",
    "\n",
    "\n",
    "k = 1\n",
    "Cout = [1, 2, 3, 4]\n",
    "pairs = []\n",
    "for c in Cout:\n",
    "    pairs.append((k, c))\n",
    "\n",
    "betas = []\n",
    "rbetas = []\n",
    "\n",
    "\n",
    "for (k, output_channels) in pairs:\n",
    "    print(k, output_channels)\n",
    "    (Rbeta, beta) = experiment(k, k, output_channels)\n",
    "    rbetas.append(Rbeta)\n",
    "    betas.append(beta)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "nearby-worship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 1)\n",
      "(28, 2)\n",
      "(28, 3)\n",
      "(28, 4)\n"
     ]
    }
   ],
   "source": [
    "# Write data to a CSV\n",
    "import pandas as pd\n",
    "    \n",
    "# Write rbetas \n",
    "name =  \"experiments-data/\"+ str(k) + \"rbeta\" + str(Cout) + \".csv\"\n",
    "pd.DataFrame(rbetas).to_csv(name, header=False, index=False)\n",
    "\n",
    "# Write betas\n",
    "for i in range(len(pairs)):\n",
    "    beta = betas[i]\n",
    "    name = \"experiments-data/\"+ str(pairs[i]) + \".csv\"\n",
    "    print(str(pairs[i]))\n",
    "    pd.DataFrame(beta).to_csv(name, header=False, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "enhanced-protest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "private-football",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[897.4652174016205, 897.3379004473398, 897.486905565276, 897.6057589986126]\n"
     ]
    }
   ],
   "source": [
    "print(rbetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-valuation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
