{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "geographic-argentina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Loading training and test data\n",
    "import torch\n",
    "\n",
    "data_tr = torch.load(\"training-test-data/training_data.txt\")\n",
    "target_tr = torch.load(\"training-test-data/training_targets.txt\")\n",
    "data_test = torch.load(\"training-test-data/test_data.txt\")\n",
    "target_test = torch.load(\"training-test-data/test_targets.txt\")\n",
    "\n",
    "num_samples = 128\n",
    "num_samples_test = 100 # number of test samples\n",
    "new_dim1 = 28 * 1 # first dimension\n",
    "new_dim2 = 28 * 1 # second dimension\n",
    "old_dim = 28 # MNIST original dimension\n",
    "\n",
    "print(data_tr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "every-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Two-layer linear convolutional neural network\n",
    "output_channels = 1\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, ker_size1, ker_size2, output_channels):\n",
    "        super(Net, self).__init__()\n",
    "        self.ker_size1 = ker_size1\n",
    "        self.ker_size2 = ker_size2\n",
    "        self.output_channels = output_channels\n",
    "        self.conv1 = nn.Conv2d(1, output_channels, kernel_size=(self.ker_size1, self.ker_size2), bias=True) \n",
    "        self.fc1 = nn.Linear(int(new_dim1 * new_dim2 * output_channels), 1, bias=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = F.pad(x, (0,self.ker_size2-1,0,self.ker_size1-1), mode='circular') # Circular padding \n",
    "        y1 = self.conv1(y1)\n",
    "#         print(y1.shape)\n",
    "        y1 = F.relu(y1) # ReLU activations\n",
    "        y1 = y1.reshape(y1.size(0), -1)\n",
    "        y1 = self.fc1(y1) \n",
    "        return y1\n",
    "\n",
    "    def initialize(self, initialization_scale):\n",
    "        print(\"random initialization with bias\")\n",
    "        self.fc1.weight.data.mul_(initialization_scale)\n",
    "        self.conv1.weight.data.mul_(initialization_scale)\n",
    "        self.fc1.bias.data.mul_(initialization_scale)\n",
    "        self.conv1.bias.data.mul_(initialization_scale)\n",
    "#         nn.init.normal_(self.fc1.weight, mean=0.0, std=initialization_scale/np.sqrt(new_dim1 * new_dim1 * self.output_channels))\n",
    "#         nn.init.normal_(self.conv1.weight, mean=0.0, std=initialization_scale/np.sqrt(self.ker_size1 * self.ker_size1  * self.output_channels))\n",
    "#         print(self.conv1.weight.shape)\n",
    "#         print(self.conv1.weight.data)\n",
    "        \n",
    "    def initialize_nonrandom(self, Uinit, Vinit):\n",
    "        print(\"nonrandom\")\n",
    "        print(self.conv1.weight.data.shape)\n",
    "        print(self.fc1.weight.data.shape)\n",
    "        self.conv1.weight = torch.nn.Parameter(Uinit)\n",
    "        self.fc1.weight = torch.nn.Parameter(Vinit)\n",
    "#         for c in range(self.output_channels):\n",
    "#             for i in range(self.ker_size1):\n",
    "#                 for j in range(self.ker_size2):\n",
    "#                     self.conv1.weight.data[c][0][i][j] = Uinit[i][j]\n",
    "#             for i in range(new_dim1):\n",
    "#                 self.fc1.weight.data[0][i + c * new_dim1] = Vinit[i]\n",
    "#         nn.init.normal_(self.fc1.weight, mean=0.0, std=initialization_scale/np.sqrt(new_dim1))\n",
    "#         nn.init.normal_(self.conv1.weight, mean=0.0, std=initialization_scale/np.sqrt(ker_size1))\n",
    "\n",
    "output = torch.zeros((num_samples, 1))\n",
    "output = output.float()\n",
    "output_test = torch.zeros((num_samples_test, 1))\n",
    "output_test = output.float()\n",
    "\n",
    "\n",
    "# Batch gradient descent\n",
    "def train_minibatch(network, optimizer):\n",
    "    minibatch_size = 32\n",
    "    num_batch = int(num_samples/minibatch_size)\n",
    "    for i in range(num_batch):\n",
    "        network.train()\n",
    "        optimizer.zero_grad()\n",
    "        start_index = i * minibatch_size\n",
    "        end_index = start_index + minibatch_size\n",
    "        output = network(data_tr[start_index:end_index])\n",
    "        loss = torch.sum(torch.exp(-1 * torch.mul(output.flatten(), target_tr[start_index:end_index]))) / minibatch_size\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate training data loss\n",
    "def train_eval(network):\n",
    "    network.eval()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        output = network(data_tr)\n",
    "        train_loss = torch.sum(torch.exp(-1 * torch.mul(output.flatten(), target_tr)))\n",
    "        pred = output.apply_(lambda x: 1 if x > 0 else -1)\n",
    "        correct += pred.eq(target_tr.data.view_as(pred)).sum()\n",
    "    train_loss /= num_samples\n",
    "    print('\\nTrain set: Avg. loss: {:.9f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    train_loss, correct, num_samples,\n",
    "    100. * correct / num_samples))\n",
    "    return train_loss\n",
    "\n",
    "def test(network):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        output_test = network(data_test)\n",
    "        test_loss = torch.sum(torch.exp(-1 * torch.mul(output_test.flatten(), target_test)))\n",
    "        pred = output_test.apply_(lambda x: 1 if x > 0 else -1)\n",
    "        correct += pred.eq(target_test.data.view_as(pred)).sum()\n",
    "    test_loss /= num_samples_test\n",
    "    accuracy = 100. * correct / num_samples_test\n",
    "    losses = test_loss\n",
    "    return (accuracy, losses)\n",
    "\n",
    "\n",
    "# Get the information about beta\n",
    "def extract_info(network, show_photo): \n",
    "\n",
    "  # Compute beta for linear CNNs\n",
    "    beta_test = np.zeros((new_dim1,new_dim2))\n",
    "    for i in range(new_dim1):\n",
    "        for j in range(new_dim2):\n",
    "            tempimg = torch.zeros((1,1,new_dim1, new_dim2))\n",
    "            tempimg[0,0,i,j]=1\n",
    "            beta_test[i,j] = network(tempimg)  \n",
    "\n",
    "  # Compute margin\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        output_np = np.ndarray.flatten(network(data_tr).data.numpy())\n",
    "        target_np = np.ndarray.flatten(target_tr.data.numpy())\n",
    "        margins = [target_np[i] * output_np[i] for i in range(num_samples)]\n",
    "        min_margin = min(margins) # get the minimum margin for any datapoint \n",
    "\n",
    "\n",
    "    # Compute R(beta)\n",
    "    w1 = network.conv1.weight.detach().numpy()\n",
    "    w2 = network.fc1.weight.detach().numpy()\n",
    "    w1_norm_sq = np.sum(np.square(w1))\n",
    "    w2_norm_sq = np.sum(np.square(w2))\n",
    "    print(w1_norm_sq, w2_norm_sq)\n",
    "    Rbeta = (np.sum(np.square(w1)) + np.sum(np.square(w2))) * np.sqrt(new_dim1 * new_dim2)\n",
    "    Rbeta2 = 2 * new_dim1 * np.linalg.norm(w1) * np.linalg.norm(w2)\n",
    "\n",
    "    # Normalize by margin \n",
    "    beta_test = beta_test / min_margin # normalize to have margin 1\n",
    "    hat_beta = np.absolute(np.fft.fft2(beta_test,norm='ortho'))\n",
    "    Rbeta = Rbeta / min_margin\n",
    "    Rbeta2 = Rbeta2 / min_margin\n",
    "    ell1 = 2 * np.sum(hat_beta)\n",
    "    \n",
    "    print(\"l2 norm: \" + str(2 * np.sqrt(new_dim1 * new_dim2)* np.linalg.norm(beta_test, ord=\"fro\")))\n",
    "    print(\"l1 norm: \" + str(2 * np.sum(hat_beta)))\n",
    "    print(\"Rbeta: \" + str(Rbeta))\n",
    "    print(\"Rbeta2: \" + str(Rbeta2))\n",
    "\n",
    "    if show_photo:\n",
    "        print(\"Time domain:\")\n",
    "        plt.imshow(np.absolute(beta_test), cmap='gray')\n",
    "        plt.show()\n",
    "        print(\"Frequency domain:\")\n",
    "        plt.imshow(np.absolute(hat_beta), cmap='gray', norm=LogNorm(vmin=0.0001, vmax=0.08))\n",
    "        plt.show()\n",
    "  \n",
    "    return (Rbeta, beta_test, ell1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "external-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and extract info about beta\n",
    "import seaborn as sns\n",
    "n_epochs = 100000\n",
    "learning_rate_start = 0.0001\n",
    "momentum = 0.3\n",
    "initialization_scale = 0.1\n",
    "wd = 1e-5\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "def experiment(ker_size1, ker_size2, output_channels, initialization):\n",
    "  # print(class1, class2)\n",
    "    network = Net(ker_size1, ker_size2, output_channels)\n",
    "    network.initialize(initialization_scale)\n",
    "    (Uinit, Vinit) = initialization\n",
    "#     print(Uinit)\n",
    "#     network.initialize_nonrandom(Uinit, Vinit)\n",
    "    optimizer =  optim.SGD(network.parameters(), lr=learning_rate_start, momentum=momentum, weight_decay=1e-5)\n",
    "    print(\"Before training:\")\n",
    "    train_eval(network)\n",
    "    extract_info(network, False)\n",
    "    # test()\n",
    "    lossarray = []\n",
    "    rbetavals = []\n",
    "    ell1s = []\n",
    "    print(\"Start training:\")\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "        train_minibatch(network, optimizer)\n",
    "        if epoch % 100 == 0:\n",
    "            lossv = train_eval(network)\n",
    "            loss = np.ndarray.flatten(lossv.detach().numpy())[0]\n",
    "            lossarray.append(loss)\n",
    "            (Rbeta, beta_test, ell1) = extract_info(network, False)\n",
    "            rbetavals.append(Rbeta)\n",
    "            ell1s.append(ell1)\n",
    "#             if loss <= 1:\n",
    "            if loss <= 0.000001: # stop at 10^-6 loss \n",
    "                break\n",
    "#         # After enough epochs, change the learning rate to be higher to expedite convergence\n",
    "\n",
    "        if epoch == 200 == 0:\n",
    "            optimizer =  optim.SGD(network.parameters(), lr=0.005, momentum=momentum, weight_decay=wd)\n",
    "            print(\"Learning rate change\")\n",
    "              # optimizer =  optim.SGD(network.parameters(), lr=0.001, momentum=momentum)\n",
    "\n",
    "        if epoch == 500:\n",
    "            optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=momentum, weight_decay=wd)\n",
    "            print(\"Learning rate change\")\n",
    "              # optimizer =  optim.SGD(network.parameters(), lr=0.001, momentum=momentum)\n",
    "\n",
    "        if epoch == 1000:\n",
    "            optimizer = optim.SGD(network.parameters(), lr=0.05, momentum=momentum, weight_decay=wd)\n",
    "              # print(\"Learning rate change\")\n",
    "              # optimizer =  optim.SGD(network.parameters(), lr=0.005, momentum=momentum)\n",
    "\n",
    "        if epoch == 1200:\n",
    "            optimizer = optim.SGD(network.parameters(), lr=0.1, momentum=momentum, weight_decay=wd)\n",
    "              # optimizer =  optim.SGD(network.parameters(), lr=0.007, momentum=momentum)\n",
    "\n",
    "        if epoch == 1500:\n",
    "            optimizer = optim.SGD(network.parameters(), lr=0.5, momentum=momentum, weight_decay=wd)\n",
    "\n",
    "        if epoch == 2000:\n",
    "            optimizer = optim.SGD(network.parameters(), lr=1, momentum=momentum, weight_decay=wd)\n",
    "              # optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=momentum)\n",
    "\n",
    "        if epoch == 3000:\n",
    "            optimizer = optim.SGD(network.parameters(), lr=2, momentum=momentum, weight_decay=wd)\n",
    "              # optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=momentum)\n",
    "\n",
    "#         if epoch == 4000:\n",
    "#             optimizer = optim.SGD(network.parameters(), lr=4, momentum=momentum, weight_decay=wd)\n",
    "#               # optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=momentum)\n",
    "\n",
    "#         if epoch == 4500:\n",
    "#             optimizer = optim.SGD(network.parameters(), lr=10, momentum=momentum, weight_decay=wd)\n",
    "\n",
    "#         if epoch == 5000:\n",
    "#             optimizer = optim.SGD(network.parameters(), lr=20, momentum=momentum, weight_decay=wd)\n",
    "        \n",
    "#         if epoch == 7000:\n",
    "#             optimizer = optim.SGD(network.parameters(), lr=40, momentum=momentum, weight_decay=wd)\n",
    "        \n",
    "#         if epoch == 8000:\n",
    "#             optimizer = optim.SGD(network.parameters(), lr=50, momentum=momentum, weight_decay=wd)\n",
    "            \n",
    "\n",
    "\n",
    "#         if epoch % 500 == 0:\n",
    "#             print(test(network))\n",
    "\n",
    "    print(\"After training:\")\n",
    "    train_eval(network)\n",
    "    (accuracy, losses) = test(network)\n",
    "    print(accuracy, losses)\n",
    "\n",
    "    (rk, beta, ell1) = extract_info(network, True)\n",
    "\n",
    "    return (rk, beta, lossarray, rbetavals, ell1s)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "twelve-sheriff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 1\n",
      "random initialization with bias\n",
      "Before training:\n",
      "\n",
      "Train set: Avg. loss: 0.999999881, Accuracy: 64/128 (50%)\n",
      "\n",
      "3.3946372e-07 3.336561e-07\n",
      "l2 norm: 1562.9221146571585\n",
      "l1 norm: 56.130754975446095\n",
      "Rbeta: -0.5686848696773349\n",
      "Rbeta2: -0.5686637181495952\n",
      "Start training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-478e19dfb2ce>:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm(range(1, n_epochs + 1)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561daaf403d44edc867bcefe7be822fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-331f8a4fe9b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUinit_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVinit_torch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mRbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrbetavals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mell1s\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mrbetas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mbetas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-478e19dfb2ce>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(ker_size1, ker_size2, output_channels, initialization)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start training:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mlossv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f258a9e1a11c>\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(network, optimizer)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "k = 28\n",
    "# Cout = [1,3,5,7]\n",
    "Cout = [1,2, 4,8]\n",
    "# Cout = [1, 2, 3, 4]\n",
    "# Cout = [3]\n",
    "pairs = []\n",
    "for c in Cout:\n",
    "    pairs.append((k, c))\n",
    "\n",
    "betas = []\n",
    "rbetas = []\n",
    "losses_all = []\n",
    "rbetavals_all = []\n",
    "ell1s_all = []\n",
    "\n",
    "Uinitial = np.random.normal(0, initialization_scale/np.sqrt(k), (k, k))\n",
    "Vinitial = np.random.normal(0, initialization_scale/np.sqrt(new_dim1), new_dim1 * new_dim1)\n",
    "\n",
    "\n",
    "for (k, output_channels) in pairs:\n",
    "    print(k, output_channels)\n",
    "    Uinit_torch = torch.zeros((output_channels, 1, k, k))\n",
    "    Vinit_torch = torch.zeros((1, new_dim1 * new_dim1 * output_channels))\n",
    "    for c in range(output_channels):\n",
    "        for i in range(k):\n",
    "            for j in range(k):\n",
    "                Uinit_torch.data[c][0][i][j] = Uinitial[i][j] / np.sqrt(output_channels) # Initialize all channels in the same way\n",
    "        for i in range(new_dim1 * new_dim1):\n",
    "            Vinit_torch.data[0][i + c * new_dim1 * new_dim1] = Vinitial[i] / np.sqrt(output_channels)\n",
    "                \n",
    "    init = (Uinit_torch, Vinit_torch)\n",
    "    (Rbeta, beta, losses, rbetavals, ell1s) = experiment(k, k, output_channels, init)\n",
    "    rbetas.append(Rbeta)\n",
    "    betas.append(beta)\n",
    "    rbetavals_all.append(rbetavals)\n",
    "    ell1s_all.append(ell1s)\n",
    "    losses_all.append(losses)\n",
    "    \n",
    "# Write data to a CSV\n",
    "import pandas as pd\n",
    "    \n",
    "# Write rbetas \n",
    "name =  \"experiments-data/\" + str(k) + \"rbeta-2-nonlinear-bias2-wd\" + str(Cout) + \".csv\"\n",
    "pd.DataFrame(rbetas).to_csv(name, header=False, index=False)\n",
    "\n",
    "# Write betas, losses, and ell1s\n",
    "for i in range(len(pairs)):\n",
    "    beta = betas[i]\n",
    "    losses = losses_all[i]\n",
    "    rbetavals = rbetavals_all[i]\n",
    "    ell1s = ell1s_all[i]\n",
    "    print(str(pairs[i]))\n",
    "    name = \"experiments-data/\" + \"-iid-nonlinear-bias2-wd\" + str(pairs[i]) + \".csv\"\n",
    "    pd.DataFrame(beta).to_csv(name, header=False, index=False)\n",
    "    name = \"experiments-data/\" + \"loss-iid-nonlinear-bias2-wd\" + str(pairs[i]) + \".csv\"\n",
    "    pd.DataFrame(losses).to_csv(name, header=False, index=False)\n",
    "    name = \"experiments-data/\" + \"rbetavals-iid-nonlinear-bias2-wd\" + str(pairs[i]) + \".csv\"\n",
    "    pd.DataFrame(rbetavals).to_csv(name, header=False, index=False)\n",
    "    name = \"experiments-data/\" + \"ell1s-iid-nonlinear-bias2-wd\" + str(pairs[i]) + \".csv\"\n",
    "    pd.DataFrame(ell1s).to_csv(name, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "forward-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(k, T, Cout):\n",
    "    pairs = []\n",
    "    for c in Cout:\n",
    "        pairs.append((k, c))\n",
    "\n",
    "    betas = []\n",
    "    rbetas = []\n",
    "    losses_all = []\n",
    "    rbetavals_all = []\n",
    "    ell1s_all = []\n",
    "\n",
    "    Uinitial = np.random.normal(0, initialization_scale/np.sqrt(k), (k, k))\n",
    "    Vinitial = np.random.normal(0, initialization_scale/np.sqrt(new_dim1), new_dim1 * new_dim1)\n",
    "\n",
    "\n",
    "    for (k, output_channels) in pairs:\n",
    "        for t in range(T):\n",
    "            print(k, output_channels, t)\n",
    "            Uinit_torch = torch.zeros((output_channels, 1, k, k))\n",
    "            Vinit_torch = torch.zeros((1, new_dim1 * new_dim1 * output_channels))\n",
    "            for c in range(output_channels):\n",
    "                for i in range(k):\n",
    "                    for j in range(k):\n",
    "                        Uinit_torch.data[c][0][i][j] = Uinitial[i][j] / np.sqrt(output_channels) # Initialize all channels in the same way\n",
    "                for i in range(new_dim1 * new_dim1):\n",
    "                    Vinit_torch.data[0][i + c * new_dim1 * new_dim1] = Vinitial[i] / np.sqrt(output_channels)\n",
    "\n",
    "            init = (Uinit_torch, Vinit_torch)\n",
    "            (Rbeta, beta, losses, rbetavals, ell1s) = experiment(k, k, output_channels, init)\n",
    "            rbetas.append(Rbeta)\n",
    "            betas.append(beta)\n",
    "            rbetavals_all.append(rbetavals)\n",
    "            ell1s_all.append(ell1s)\n",
    "            losses_all.append(losses)\n",
    "\n",
    "    # Write data to a CSV\n",
    "    import pandas as pd\n",
    "\n",
    "\n",
    "    # Write betas, losses, and ell1s\n",
    "    for i in range(len(pairs)):\n",
    "        # Write rbetas \n",
    "        rbetas_to_write = []\n",
    "        for t in range(T):\n",
    "            index = i *T + t\n",
    "            rbetas_to_write.append(rbetas[index])\n",
    "            beta = betas[index]\n",
    "            losses = losses_all[index]\n",
    "            rbetavals = rbetavals_all[index]\n",
    "            ell1s = ell1s_all[index]\n",
    "            print(str(pairs[i]))\n",
    "            name = \"experiments-data/\" + \"-iid-nonlinear-bias-wd\" + str(pairs[i]) + str(t) +  \".csv\"\n",
    "            pd.DataFrame(beta).to_csv(name, header=False, index=False)\n",
    "            name = \"experiments-data/\" + \"loss-iid-nonlinear-bias-wd\" + str(pairs[i]) + str(t) + \".csv\"\n",
    "            pd.DataFrame(losses).to_csv(name, header=False, index=False)\n",
    "            name = \"experiments-data/\" + \"rbetavals-iid-nonlinear-bias-wd\" + str(pairs[i]) +  str(t) + \".csv\"\n",
    "            pd.DataFrame(rbetavals).to_csv(name, header=False, index=False)\n",
    "            name = \"experiments-data/\" + \"ell1s-iid-nonlinear-bias-wd\" + str(pairs[i]) + str(t)+  \".csv\"\n",
    "            pd.DataFrame(ell1s).to_csv(name, header=False, index=False)\n",
    "        name =  \"experiments-data/\" + str(pairs[i]) + \"rbeta-2-nonlinear-bias-wd\" + \".csv\"\n",
    "        pd.DataFrame(rbetas_to_write).to_csv(name, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "endangered-gregory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0\n",
      "random initialization with bias\n",
      "Before training:\n",
      "\n",
      "Train set: Avg. loss: 1.000117540, Accuracy: 66/128 (52%)\n",
      "\n",
      "0.008403225 0.0032239256\n",
      "l2 norm: 561.6109811688538\n",
      "l1 norm: 46.01073171600378\n",
      "Rbeta: -46.771090835368774\n",
      "Rbeta2: -41.87450529238547\n",
      "Start training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-46bf24234d01>:26: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for epoch in tqdm(range(1, n_epochs + 1)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0ee8e75f0f4c9ba6350d6104b14287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.987675369, Accuracy: 64/128 (50%)\n",
      "\n",
      "0.00880862 0.003906818\n",
      "l2 norm: 960.6017460145578\n",
      "l1 norm: 45.444760806582394\n",
      "Rbeta: -19.946986241372226\n",
      "Rbeta2: -18.40523978950929\n",
      "\n",
      "Train set: Avg. loss: 0.974147975, Accuracy: 64/128 (50%)\n",
      "\n",
      "0.009980234 0.005957\n",
      "l2 norm: 1018.4164765765922\n",
      "l1 norm: 44.34365896044611\n",
      "Rbeta: -14.963995729438585\n",
      "Rbeta2: -14.47933928887067\n",
      "\n",
      "Train set: Avg. loss: 0.957000136, Accuracy: 64/128 (50%)\n",
      "\n",
      "0.011933016 0.009569131\n",
      "l2 norm: 1019.0049052962811\n",
      "l1 norm: 44.6972568999044\n",
      "Rbeta: -16.721490721634126\n",
      "Rbeta2: -16.62013407314884\n",
      "\n",
      "Train set: Avg. loss: 0.933597684, Accuracy: 96/128 (75%)\n",
      "\n",
      "0.0147699425 0.015219541\n",
      "l2 norm: 972.9938584077661\n",
      "l1 norm: 46.37026034544471\n",
      "Rbeta: -24.977344653102328\n",
      "Rbeta2: -24.97453610552876\n",
      "\n",
      "Train set: Avg. loss: 0.900919139, Accuracy: 124/128 (97%)\n",
      "\n",
      "0.018657994 0.023679383\n",
      "l2 norm: 693.5760546223748\n",
      "l1 norm: 53.80304472927342\n",
      "Rbeta: -65.6792059115765\n",
      "Rbeta2: -65.21561868573492\n",
      "Learning rate change\n",
      "\n",
      "Train set: Avg. loss: 0.014894344, Accuracy: 128/128 (100%)\n",
      "\n",
      "0.47423983 1.4611621\n",
      "l2 norm: 998.7444170669246\n",
      "l1 norm: 45.17909039683872\n",
      "Rbeta: 30.985133529949792\n",
      "Rbeta2: 26.65387904550508\n",
      "\n",
      "Train set: Avg. loss: 0.006031114, Accuracy: 128/128 (100%)\n",
      "\n",
      "0.5895488 1.8385816\n",
      "l2 norm: 797.8702121713968\n",
      "l1 norm: 36.95212480650797\n",
      "Rbeta: 26.47771402466795\n",
      "Rbeta2: 22.705958072505375\n",
      "\n",
      "Train set: Avg. loss: 0.003589070, Accuracy: 128/128 (100%)\n",
      "\n",
      "0.65688187 2.058106\n",
      "l2 norm: 734.5667455217404\n",
      "l1 norm: 34.40778122873459\n",
      "Rbeta: 25.062946482260973\n",
      "Rbeta2: 21.46701930409438\n",
      "\n",
      "Train set: Avg. loss: 0.002495538, Accuracy: 128/128 (100%)\n",
      "\n",
      "0.7044486 2.2127883\n",
      "l2 norm: 699.8399646947821\n",
      "l1 norm: 33.01603539945052\n",
      "Rbeta: 24.26833985585843\n",
      "Rbeta2: 20.772699155745325\n",
      "\n",
      "Train set: Avg. loss: 0.001887872, Accuracy: 128/128 (100%)\n",
      "\n",
      "0.74117136 2.33206\n",
      "l2 norm: 677.1699644514943\n",
      "l1 norm: 32.109499036567755\n",
      "Rbeta: 23.7419393798263\n",
      "Rbeta2: 20.31328245063388\n",
      "\n",
      "Train set: Avg. loss: 0.000803731, Accuracy: 128/128 (100%)\n",
      "\n",
      "0.8539253 2.6993184\n",
      "l2 norm: 618.7238295645723\n",
      "l1 norm: 29.77938497380287\n",
      "Rbeta: 22.403097840331387\n",
      "Rbeta2: 19.14476756403384\n",
      "\n",
      "Train set: Avg. loss: 0.000493155, Accuracy: 128/128 (100%)\n",
      "\n",
      "0.9189891 2.910646\n",
      "l2 norm: 596.7852194801192\n",
      "l1 norm: 28.909738590446835\n",
      "Rbeta: 21.852007911832914\n",
      "Rbeta2: 18.664390353551457\n",
      "\n",
      "Train set: Avg. loss: 0.000269282, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.0005895 3.1734986\n",
      "l2 norm: 572.7004205665148\n",
      "l1 norm: 27.961896223561734\n",
      "Rbeta: 21.266869088029694\n",
      "Rbeta2: 18.158043610016275\n",
      "\n",
      "Train set: Avg. loss: 0.000181946, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.0536892 3.3442893\n",
      "l2 norm: 561.0994477283664\n",
      "l1 norm: 27.508187561212967\n",
      "Rbeta: 20.96020886640079\n",
      "Rbeta2: 17.892897867877114\n",
      "\n",
      "Train set: Avg. loss: 0.000136254, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.0929512 3.4704652\n",
      "l2 norm: 553.4369865669944\n",
      "l1 norm: 27.209736782499593\n",
      "Rbeta: 20.755538676997666\n",
      "Rbeta2: 17.716100537289563\n",
      "\n",
      "Train set: Avg. loss: 0.000058647, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.2076796 3.8388317\n",
      "l2 norm: 530.3956554845565\n",
      "l1 norm: 26.31332765649072\n",
      "Rbeta: 20.190777401586182\n",
      "Rbeta2: 17.229283462108775\n",
      "\n",
      "Train set: Avg. loss: 0.000037093, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.2700424 4.0389614\n",
      "l2 norm: 522.8928544783162\n",
      "l1 norm: 26.026342357992174\n",
      "Rbeta: 19.958907755390012\n",
      "Rbeta2: 17.029335191760705\n",
      "\n",
      "Train set: Avg. loss: 0.000027201, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.3122154 4.1742673\n",
      "l2 norm: 518.1554778568375\n",
      "l1 norm: 25.84644333676918\n",
      "Rbeta: 19.811438164521544\n",
      "Rbeta2: 16.90225584690839\n",
      "\n",
      "Train set: Avg. loss: 0.000021582, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.3435994 4.274987\n",
      "l2 norm: 514.7620438498891\n",
      "l1 norm: 25.718311658294393\n",
      "Rbeta: 19.70482024602354\n",
      "Rbeta2: 16.810387820593178\n",
      "\n",
      "Train set: Avg. loss: 0.000017985, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.368265 4.35417\n",
      "l2 norm: 512.1513782077567\n",
      "l1 norm: 25.62026597157643\n",
      "Rbeta: 19.62202880785127\n",
      "Rbeta2: 16.739050634002613\n",
      "\n",
      "Train set: Avg. loss: 0.000013675, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.4051588 4.472645\n",
      "l2 norm: 507.27365409078266\n",
      "l1 norm: 25.436126178573122\n",
      "Rbeta: 19.487151830181027\n",
      "Rbeta2: 16.62295577275602\n",
      "\n",
      "Train set: Avg. loss: 0.000011211, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.4317422 4.55796\n",
      "l2 norm: 504.59672219827274\n",
      "l1 norm: 25.337894287351407\n",
      "Rbeta: 19.396585415944454\n",
      "Rbeta2: 16.54503641577383\n",
      "\n",
      "Train set: Avg. loss: 0.000009635, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.4518659 4.6225657\n",
      "l2 norm: 502.51333236643353\n",
      "l1 norm: 25.2622983164935\n",
      "Rbeta: 19.32509500223758\n",
      "Rbeta2: 16.48355270102119\n",
      "\n",
      "Train set: Avg. loss: 0.000008549, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.4675963 4.6731215\n",
      "l2 norm: 500.81541165313376\n",
      "l1 norm: 25.201316393395864\n",
      "Rbeta: 19.266080602144857\n",
      "Rbeta2: 16.432783177324357\n",
      "\n",
      "Train set: Avg. loss: 0.000007762, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.480169 4.713581\n",
      "l2 norm: 499.3869731452117\n",
      "l1 norm: 25.150551528670952\n",
      "Rbeta: 19.215863392599292\n",
      "Rbeta2: 16.389563177259507\n",
      "\n",
      "Train set: Avg. loss: 0.000007170, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.490378 4.7464848\n",
      "l2 norm: 498.1544475760001\n",
      "l1 norm: 25.10720379392367\n",
      "Rbeta: 19.172130186361624\n",
      "Rbeta2: 16.351909626124733\n",
      "\n",
      "Train set: Avg. loss: 0.000006712, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.4987608 4.7735586\n",
      "l2 norm: 497.07181888325897\n",
      "l1 norm: 25.069532008844494\n",
      "Rbeta: 19.133387346262634\n",
      "Rbeta2: 16.31852918585003\n",
      "\n",
      "Train set: Avg. loss: 0.000006350, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5057039 4.796034\n",
      "l2 norm: 496.1041651879354\n",
      "l1 norm: 25.036214789032776\n",
      "Rbeta: 19.098578464500527\n",
      "Rbeta2: 16.28852097631028\n",
      "\n",
      "Train set: Avg. loss: 0.000006059, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5114877 4.8148155\n",
      "l2 norm: 495.22950741702283\n",
      "l1 norm: 25.00641247094615\n",
      "Rbeta: 19.066960323933188\n",
      "Rbeta2: 16.26123642073835\n",
      "\n",
      "Train set: Avg. loss: 0.000005821, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5163263 4.830587\n",
      "l2 norm: 494.4298332751747\n",
      "l1 norm: 24.97945341563983\n",
      "Rbeta: 19.03797835112308\n",
      "Rbeta2: 16.23620492960118\n",
      "\n",
      "Train set: Avg. loss: 0.000005460, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5238441 4.8552275\n",
      "l2 norm: 492.2722713301637\n",
      "l1 norm: 24.903135515382115\n",
      "Rbeta: 18.97877793889796\n",
      "Rbeta2: 16.185115947577714\n",
      "\n",
      "Train set: Avg. loss: 0.000005205, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5291547 4.872862\n",
      "l2 norm: 491.02244998757794\n",
      "l1 norm: 24.86239151113665\n",
      "Rbeta: 18.933347703942616\n",
      "Rbeta2: 16.1457559456967\n",
      "\n",
      "Train set: Avg. loss: 0.000005021, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5328844 4.885475\n",
      "l2 norm: 489.9020739695009\n",
      "l1 norm: 24.82657938416158\n",
      "Rbeta: 18.89300944911825\n",
      "Rbeta2: 16.11072917027627\n",
      "\n",
      "Train set: Avg. loss: 0.000004885, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5354642 4.894437\n",
      "l2 norm: 488.87881362380836\n",
      "l1 norm: 24.79446533181713\n",
      "Rbeta: 18.85671311989003\n",
      "Rbeta2: 16.07914382061292\n",
      "\n",
      "Train set: Avg. loss: 0.000004784, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5371971 4.9007173\n",
      "l2 norm: 487.92923237626314\n",
      "l1 norm: 24.765170763038512\n",
      "Rbeta: 18.82373508821566\n",
      "Rbeta2: 16.05037710793095\n",
      "\n",
      "Train set: Avg. loss: 0.000004708, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5383002 4.9050093\n",
      "l2 norm: 487.03706177389404\n",
      "l1 norm: 24.738064817420753\n",
      "Rbeta: 18.79351495115361\n",
      "Rbeta2: 16.02394452460172\n",
      "\n",
      "Train set: Avg. loss: 0.000004650, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5389328 4.907825\n",
      "l2 norm: 486.189487260991\n",
      "l1 norm: 24.712660681755047\n",
      "Rbeta: 18.765652557253652\n",
      "Rbeta2: 15.999509333572467\n",
      "\n",
      "Train set: Avg. loss: 0.000004606, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.539212 4.90954\n",
      "l2 norm: 485.3778753152545\n",
      "l1 norm: 24.688622892981744\n",
      "Rbeta: 18.739825443144806\n",
      "Rbeta2: 15.97678818722813\n",
      "\n",
      "Train set: Avg. loss: 0.000004572, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5392493 4.910436\n",
      "l2 norm: 484.5955891971746\n",
      "l1 norm: 24.665755777558694\n",
      "Rbeta: 18.715862212515688\n",
      "Rbeta2: 15.955698423372134\n",
      "\n",
      "Train set: Avg. loss: 0.000004546, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5391176 4.9107294\n",
      "l2 norm: 483.83717324550753\n",
      "l1 norm: 24.643850451376924\n",
      "Rbeta: 18.693570824151156\n",
      "Rbeta2: 15.936088625383022\n",
      "\n",
      "Train set: Avg. loss: 0.000004526, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5388416 4.9105763\n",
      "l2 norm: 483.0982707895917\n",
      "l1 norm: 24.622672659037697\n",
      "Rbeta: 18.672692367769592\n",
      "Rbeta2: 15.917675367675406\n",
      "\n",
      "Train set: Avg. loss: 0.000004511, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5384572 4.910099\n",
      "l2 norm: 482.37606409166705\n",
      "l1 norm: 24.602106658423978\n",
      "Rbeta: 18.653091605260766\n",
      "Rbeta2: 15.900332375682943\n",
      "\n",
      "Train set: Avg. loss: 0.000004499, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.537988 4.9093895\n",
      "l2 norm: 481.6679879355669\n",
      "l1 norm: 24.582042521177197\n",
      "Rbeta: 18.634640366140868\n",
      "Rbeta2: 15.883936388434597\n",
      "\n",
      "Train set: Avg. loss: 0.000004489, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5374486 4.9085217\n",
      "l2 norm: 480.9703377956166\n",
      "l1 norm: 24.56231550408736\n",
      "Rbeta: 18.617223387868115\n",
      "Rbeta2: 15.868368474939794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000004482, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5368769 4.907544\n",
      "l2 norm: 480.2833716537341\n",
      "l1 norm: 24.54296239333733\n",
      "Rbeta: 18.600755103535498\n",
      "Rbeta2: 15.853615736094692\n",
      "\n",
      "Train set: Avg. loss: 0.000004476, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5365398 4.9064965\n",
      "l2 norm: 479.60893273822467\n",
      "l1 norm: 24.524713949737333\n",
      "Rbeta: 18.5860730678624\n",
      "Rbeta2: 15.841078516701709\n",
      "\n",
      "Train set: Avg. loss: 0.000004471, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5362297 4.905409\n",
      "l2 norm: 478.94590545389343\n",
      "l1 norm: 24.506950708020543\n",
      "Rbeta: 18.57232457728781\n",
      "Rbeta2: 15.82944261187247\n",
      "\n",
      "Train set: Avg. loss: 0.000004468, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5359116 4.9043055\n",
      "l2 norm: 478.2899882375964\n",
      "l1 norm: 24.489424912681873\n",
      "Rbeta: 18.559368788125184\n",
      "Rbeta2: 15.818473020116441\n",
      "\n",
      "Train set: Avg. loss: 0.000004465, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5355897 4.9032044\n",
      "l2 norm: 477.6400345023373\n",
      "l1 norm: 24.472076070498666\n",
      "Rbeta: 18.54711813688819\n",
      "Rbeta2: 15.808095101080525\n",
      "\n",
      "Train set: Avg. loss: 0.000004462, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5352793 4.902116\n",
      "l2 norm: 476.99602157985123\n",
      "l1 norm: 24.4549138956654\n",
      "Rbeta: 18.535532197419187\n",
      "Rbeta2: 15.798300306122497\n",
      "\n",
      "Train set: Avg. loss: 0.000004460, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5349803 4.90104\n",
      "l2 norm: 476.3577278150953\n",
      "l1 norm: 24.437918754370546\n",
      "Rbeta: 18.52453415981621\n",
      "Rbeta2: 15.789028581384176\n",
      "\n",
      "Train set: Avg. loss: 0.000004458, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5346891 4.899988\n",
      "l2 norm: 475.72397405477477\n",
      "l1 norm: 24.42104606828527\n",
      "Rbeta: 18.514092226809186\n",
      "Rbeta2: 15.780230797840971\n",
      "\n",
      "Train set: Avg. loss: 0.000004457, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5343996 4.898962\n",
      "l2 norm: 475.09553148139673\n",
      "l1 norm: 24.404336198477704\n",
      "Rbeta: 18.50424119307698\n",
      "Rbeta2: 15.771920688787278\n",
      "\n",
      "Train set: Avg. loss: 0.000004456, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.534115 4.89797\n",
      "l2 norm: 474.47108997179373\n",
      "l1 norm: 24.387725940129595\n",
      "Rbeta: 18.494894843721156\n",
      "Rbeta2: 15.76402378949373\n",
      "\n",
      "Train set: Avg. loss: 0.000004454, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5338445 4.897009\n",
      "l2 norm: 473.8517768884211\n",
      "l1 norm: 24.371270040744257\n",
      "Rbeta: 18.4860088506553\n",
      "Rbeta2: 15.75653173407879\n",
      "\n",
      "Train set: Avg. loss: 0.000004453, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.533585 4.896083\n",
      "l2 norm: 473.2362778443024\n",
      "l1 norm: 24.35492019239924\n",
      "Rbeta: 18.477577413129605\n",
      "Rbeta2: 15.749427841450952\n",
      "\n",
      "Train set: Avg. loss: 0.000004453, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5333245 4.8951926\n",
      "l2 norm: 472.6253213452637\n",
      "l1 norm: 24.33869849575482\n",
      "Rbeta: 18.46962106871711\n",
      "Rbeta2: 15.742695296723083\n",
      "\n",
      "Train set: Avg. loss: 0.000004452, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5330848 4.894338\n",
      "l2 norm: 472.0176798215922\n",
      "l1 norm: 24.322556729969808\n",
      "Rbeta: 18.462032637820066\n",
      "Rbeta2: 15.736302520374139\n",
      "\n",
      "Train set: Avg. loss: 0.000004451, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5328655 4.8935204\n",
      "l2 norm: 471.41311972244625\n",
      "l1 norm: 24.306465607761197\n",
      "Rbeta: 18.45475297190954\n",
      "Rbeta2: 15.73019858031682\n",
      "\n",
      "Train set: Avg. loss: 0.000004450, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5326332 4.8927336\n",
      "l2 norm: 470.81165701419684\n",
      "l1 norm: 24.29041476215179\n",
      "Rbeta: 18.44785884585292\n",
      "Rbeta2: 15.72435650547249\n",
      "\n",
      "Train set: Avg. loss: 0.000004449, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.532397 4.8919754\n",
      "l2 norm: 470.21456981801975\n",
      "l1 norm: 24.274461153456528\n",
      "Rbeta: 18.441311107732634\n",
      "Rbeta2: 15.718780344019878\n",
      "\n",
      "Train set: Avg. loss: 0.000004449, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.532161 4.8912516\n",
      "l2 norm: 469.6207786163691\n",
      "l1 norm: 24.258549313881524\n",
      "Rbeta: 18.435059575268394\n",
      "Rbeta2: 15.71342653692938\n",
      "\n",
      "Train set: Avg. loss: 0.000004448, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5319647 4.8905544\n",
      "l2 norm: 469.0315416991045\n",
      "l1 norm: 24.24275680460882\n",
      "Rbeta: 18.429041159296098\n",
      "Rbeta2: 15.70835714630601\n",
      "\n",
      "Train set: Avg. loss: 0.000004447, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5317582 4.8898835\n",
      "l2 norm: 468.4459925926651\n",
      "l1 norm: 24.22700652502786\n",
      "Rbeta: 18.42328796959956\n",
      "Rbeta2: 15.70346316343096\n",
      "\n",
      "Train set: Avg. loss: 0.000004447, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5315516 4.889241\n",
      "l2 norm: 467.86449364370293\n",
      "l1 norm: 24.21134728450598\n",
      "Rbeta: 18.417838823862855\n",
      "Rbeta2: 15.698805397894972\n",
      "\n",
      "Train set: Avg. loss: 0.000004446, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.531345 4.888626\n",
      "l2 norm: 467.2862695759961\n",
      "l1 norm: 24.195709918754506\n",
      "Rbeta: 18.41256610518571\n",
      "Rbeta2: 15.694272855718257\n",
      "\n",
      "Train set: Avg. loss: 0.000004446, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5311385 4.8880377\n",
      "l2 norm: 466.7110146473496\n",
      "l1 norm: 24.180097381503423\n",
      "Rbeta: 18.40752264633817\n",
      "Rbeta2: 15.689914595418799\n",
      "\n",
      "Train set: Avg. loss: 0.000004445, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5309103 4.8874655\n",
      "l2 norm: 466.1408614224766\n",
      "l1 norm: 24.16452542689398\n",
      "Rbeta: 18.402580676915\n",
      "Rbeta2: 15.685569346419648\n",
      "\n",
      "Train set: Avg. loss: 0.000004445, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5306742 4.8869185\n",
      "l2 norm: 465.572871264798\n",
      "l1 norm: 24.148943583739708\n",
      "Rbeta: 18.39782766196358\n",
      "Rbeta2: 15.681344687550316\n",
      "\n",
      "Train set: Avg. loss: 0.000004444, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5304383 4.8863955\n",
      "l2 norm: 465.00930848661244\n",
      "l1 norm: 24.13345005151158\n",
      "Rbeta: 18.39328434811592\n",
      "Rbeta2: 15.67727958882659\n",
      "\n",
      "Train set: Avg. loss: 0.000004444, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5302256 4.885893\n",
      "l2 norm: 464.45042014238214\n",
      "l1 norm: 24.11811742187652\n",
      "Rbeta: 18.388998974332743\n",
      "Rbeta2: 15.673478797789306\n",
      "\n",
      "Train set: Avg. loss: 0.000004443, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5300436 4.8854094\n",
      "l2 norm: 463.8963930666302\n",
      "l1 norm: 24.102932705374702\n",
      "Rbeta: 18.38491238450654\n",
      "Rbeta2: 15.669913937477999\n",
      "\n",
      "Train set: Avg. loss: 0.000004442, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5298667 4.8849435\n",
      "l2 norm: 463.34751477034155\n",
      "l1 norm: 24.08785884042932\n",
      "Rbeta: 18.380974864940963\n",
      "Rbeta2: 15.66647474435821\n",
      "\n",
      "Train set: Avg. loss: 0.000004442, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.52967 4.8844967\n",
      "l2 norm: 462.8028749411008\n",
      "l1 norm: 24.072799736814474\n",
      "Rbeta: 18.377092896362566\n",
      "Rbeta2: 15.663014597943453\n",
      "\n",
      "Train set: Avg. loss: 0.000004441, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5294636 4.8840604\n",
      "l2 norm: 462.2622014160152\n",
      "l1 norm: 24.05778129500598\n",
      "Rbeta: 18.373298352537613\n",
      "Rbeta2: 15.659592678694281\n",
      "\n",
      "Train set: Avg. loss: 0.000004441, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5292814 4.883643\n",
      "l2 norm: 461.72474101279846\n",
      "l1 norm: 24.042835220963003\n",
      "Rbeta: 18.36963829867153\n",
      "Rbeta2: 15.65633490735494\n",
      "\n",
      "Train set: Avg. loss: 0.000004440, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5291045 4.8832297\n",
      "l2 norm: 461.1935674301873\n",
      "l1 norm: 24.028065178269685\n",
      "Rbeta: 18.366126801584\n",
      "Rbeta2: 15.653215659442994\n",
      "\n",
      "Train set: Avg. loss: 0.000004439, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5289276 4.8828335\n",
      "l2 norm: 460.6664712566849\n",
      "l1 norm: 24.013370793960334\n",
      "Rbeta: 18.362769050957162\n",
      "Rbeta2: 15.650212665535285\n",
      "\n",
      "Train set: Avg. loss: 0.000004439, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5287507 4.882448\n",
      "l2 norm: 460.1433506858721\n",
      "l1 norm: 23.998758914864837\n",
      "Rbeta: 18.359518968960593\n",
      "Rbeta2: 15.647292814716941\n",
      "\n",
      "Train set: Avg. loss: 0.000004438, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5285739 4.882077\n",
      "l2 norm: 459.6250774488968\n",
      "l1 norm: 23.984253009786\n",
      "Rbeta: 18.35638655922777\n",
      "Rbeta2: 15.644458057837543\n",
      "\n",
      "Train set: Avg. loss: 0.000004438, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.528397 4.8817163\n",
      "l2 norm: 459.1125949919276\n",
      "l1 norm: 23.969892662421586\n",
      "Rbeta: 18.353371479125624\n",
      "Rbeta2: 15.641720375827862\n",
      "\n",
      "Train set: Avg. loss: 0.000004437, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5282164 4.881371\n",
      "l2 norm: 458.60464423975446\n",
      "l1 norm: 23.955615078001287\n",
      "Rbeta: 18.350444694248814\n",
      "Rbeta2: 15.639031284191152\n",
      "\n",
      "Train set: Avg. loss: 0.000004437, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.52801 4.8810472\n",
      "l2 norm: 458.0993165635324\n",
      "l1 norm: 23.94131303130768\n",
      "Rbeta: 18.347566902004466\n",
      "Rbeta2: 15.636297069852557\n",
      "\n",
      "Train set: Avg. loss: 0.000004437, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5278037 4.880745\n",
      "l2 norm: 457.59808081554587\n",
      "l1 norm: 23.927098383903942\n",
      "Rbeta: 18.344815348526122\n",
      "Rbeta2: 15.633655321192863\n",
      "\n",
      "Train set: Avg. loss: 0.000004436, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5276136 4.8804474\n",
      "l2 norm: 457.10279702117066\n",
      "l1 norm: 23.913064068866905\n",
      "Rbeta: 18.34215957864258\n",
      "Rbeta2: 15.63113094473787\n",
      "\n",
      "Train set: Avg. loss: 0.000004436, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5274369 4.8801546\n",
      "l2 norm: 456.61257923027773\n",
      "l1 norm: 23.899149549573234\n",
      "Rbeta: 18.339543943889453\n",
      "Rbeta2: 15.628674103437634\n",
      "\n",
      "Train set: Avg. loss: 0.000004435, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5272601 4.8798757\n",
      "l2 norm: 456.1276455070005\n",
      "l1 norm: 23.885346551072267\n",
      "Rbeta: 18.336974974714252\n",
      "Rbeta2: 15.626244151290305\n",
      "\n",
      "Train set: Avg. loss: 0.000004435, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5270833 4.879605\n",
      "l2 norm: 455.64775594985053\n",
      "l1 norm: 23.871666339464777\n",
      "Rbeta: 18.334492528619563\n",
      "Rbeta2: 15.623882900234637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000004434, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5269065 4.8793373\n",
      "l2 norm: 455.17402560118114\n",
      "l1 norm: 23.858143360009386\n",
      "Rbeta: 18.332050333764748\n",
      "Rbeta2: 15.621552950392369\n",
      "\n",
      "Train set: Avg. loss: 0.000004434, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5267297 4.879077\n",
      "l2 norm: 454.7046429193076\n",
      "l1 norm: 23.84470399485418\n",
      "Rbeta: 18.329659285727722\n",
      "Rbeta2: 15.619261026908953\n",
      "\n",
      "Train set: Avg. loss: 0.000004433, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.526553 4.8788233\n",
      "l2 norm: 454.24057362747385\n",
      "l1 norm: 23.831394463612433\n",
      "Rbeta: 18.32733543761081\n",
      "Rbeta2: 15.617019120914938\n",
      "\n",
      "Train set: Avg. loss: 0.000004433, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5263762 4.878578\n",
      "l2 norm: 453.7804882959487\n",
      "l1 norm: 23.818170894917245\n",
      "Rbeta: 18.325072559577592\n",
      "Rbeta2: 15.61482465303825\n",
      "\n",
      "Train set: Avg. loss: 0.000004433, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5261995 4.8783417\n",
      "l2 norm: 453.3260763058853\n",
      "l1 norm: 23.80510061720681\n",
      "Rbeta: 18.32288627479105\n",
      "Rbeta2: 15.612686399036049\n",
      "\n",
      "Train set: Avg. loss: 0.000004432, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5260228 4.87811\n",
      "l2 norm: 452.87697383601113\n",
      "l1 norm: 23.79215993622541\n",
      "Rbeta: 18.32075093715142\n",
      "Rbeta2: 15.610587269749056\n",
      "\n",
      "Train set: Avg. loss: 0.000004432, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5258461 4.8778853\n",
      "l2 norm: 452.43411961892025\n",
      "l1 norm: 23.779399299299563\n",
      "Rbeta: 18.31869382107999\n",
      "Rbeta2: 15.608550385236848\n",
      "\n",
      "Train set: Avg. loss: 0.000004432, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5256693 4.87767\n",
      "l2 norm: 451.995829354755\n",
      "l1 norm: 23.76674983116314\n",
      "Rbeta: 18.31669452612949\n",
      "Rbeta2: 15.6065530711\n",
      "\n",
      "Train set: Avg. loss: 0.000004431, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5255393 4.877467\n",
      "l2 norm: 451.5631155833348\n",
      "l1 norm: 23.75436184148917\n",
      "Rbeta: 18.314896666576065\n",
      "Rbeta2: 15.604843218609897\n",
      "\n",
      "Train set: Avg. loss: 0.000004431, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5254214 4.8772783\n",
      "l2 norm: 451.1352225296748\n",
      "l1 norm: 23.742123615250286\n",
      "Rbeta: 18.3131815418667\n",
      "Rbeta2: 15.603224618152547\n",
      "\n",
      "Train set: Avg. loss: 0.000004431, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5252825 4.8770895\n",
      "l2 norm: 450.71484987701876\n",
      "l1 norm: 23.730070400676112\n",
      "Rbeta: 18.311484633894917\n",
      "Rbeta2: 15.601563115014029\n",
      "\n",
      "Train set: Avg. loss: 0.000004431, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5251353 4.876911\n",
      "l2 norm: 450.29722174551364\n",
      "l1 norm: 23.71803823425995\n",
      "Rbeta: 18.30981294104571\n",
      "Rbeta2: 15.59989653983449\n",
      "\n",
      "Train set: Avg. loss: 0.000004431, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.524988 4.8767476\n",
      "l2 norm: 449.885031743441\n",
      "l1 norm: 23.706160914803874\n",
      "Rbeta: 18.308211771813156\n",
      "Rbeta2: 15.598273125797776\n",
      "\n",
      "Train set: Avg. loss: 0.000004431, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5248408 4.8765955\n",
      "l2 norm: 449.4774480482861\n",
      "l1 norm: 23.694406588415916\n",
      "Rbeta: 18.306681320038546\n",
      "Rbeta2: 15.596703492016877\n",
      "\n",
      "Train set: Avg. loss: 0.000004431, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5246936 4.8764553\n",
      "l2 norm: 449.0749537588153\n",
      "l1 norm: 23.682760490752006\n",
      "Rbeta: 18.305138715044137\n",
      "Rbeta2: 15.595113025470825\n",
      "\n",
      "Train set: Avg. loss: 0.000004430, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5245464 4.8763165\n",
      "l2 norm: 448.6760919556731\n",
      "l1 norm: 23.67119095934525\n",
      "Rbeta: 18.303616366320593\n",
      "Rbeta2: 15.593538058005551\n",
      "\n",
      "Train set: Avg. loss: 0.000004430, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5243993 4.876184\n",
      "l2 norm: 448.2849755167272\n",
      "l1 norm: 23.659861213615972\n",
      "Rbeta: 18.302163133190394\n",
      "Rbeta2: 15.59201585299228\n",
      "\n",
      "Train set: Avg. loss: 0.000004430, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.524252 4.8760586\n",
      "l2 norm: 447.898601801055\n",
      "l1 norm: 23.648649770612366\n",
      "Rbeta: 18.3007362213175\n",
      "Rbeta2: 15.590511633858096\n",
      "\n",
      "Train set: Avg. loss: 0.000004430, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5241048 4.875941\n",
      "l2 norm: 447.5172901857187\n",
      "l1 norm: 23.637566343192574\n",
      "Rbeta: 18.299340135128265\n",
      "Rbeta2: 15.589026913376612\n",
      "\n",
      "Train set: Avg. loss: 0.000004429, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5239577 4.875826\n",
      "l2 norm: 447.1407413888977\n",
      "l1 norm: 23.62661090517984\n",
      "Rbeta: 18.29796972678931\n",
      "Rbeta2: 15.587559342243976\n",
      "\n",
      "Train set: Avg. loss: 0.000004429, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5238105 4.8757153\n",
      "l2 norm: 446.76832076401115\n",
      "l1 norm: 23.615757409019814\n",
      "Rbeta: 18.296629916800345\n",
      "Rbeta2: 15.58611868149454\n",
      "\n",
      "Train set: Avg. loss: 0.000004429, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5236634 4.8756104\n",
      "l2 norm: 446.4014380032827\n",
      "l1 norm: 23.605047420548527\n",
      "Rbeta: 18.295302572843415\n",
      "Rbeta2: 15.584681993594591\n",
      "\n",
      "Train set: Avg. loss: 0.000004429, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5235163 4.875508\n",
      "l2 norm: 446.0388923467382\n",
      "l1 norm: 23.59443899560145\n",
      "Rbeta: 18.293959393942284\n",
      "Rbeta2: 15.583227208648168\n",
      "\n",
      "Train set: Avg. loss: 0.000004429, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5233691 4.8754053\n",
      "l2 norm: 445.6804915111633\n",
      "l1 norm: 23.583933097603136\n",
      "Rbeta: 18.29265014900906\n",
      "Rbeta2: 15.581806413795112\n",
      "\n",
      "Train set: Avg. loss: 0.000004428, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.523222 4.8753123\n",
      "l2 norm: 445.3264103625204\n",
      "l1 norm: 23.57354492247262\n",
      "Rbeta: 18.29138386254934\n",
      "Rbeta2: 15.58041132181517\n",
      "\n",
      "Train set: Avg. loss: 0.000004428, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5231001 4.875225\n",
      "l2 norm: 444.97809162590124\n",
      "l1 norm: 23.563356503615545\n",
      "Rbeta: 18.29015147023186\n",
      "Rbeta2: 15.579106972732857\n",
      "\n",
      "Train set: Avg. loss: 0.000004428, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5229825 4.875141\n",
      "l2 norm: 444.63414852510994\n",
      "l1 norm: 23.553293433694506\n",
      "Rbeta: 18.28895479910258\n",
      "Rbeta2: 15.577842819197365\n",
      "\n",
      "Train set: Avg. loss: 0.000004427, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5228648 4.875056\n",
      "l2 norm: 444.2940970206368\n",
      "l1 norm: 23.54333334714962\n",
      "Rbeta: 18.287791926798896\n",
      "Rbeta2: 15.576607078418231\n",
      "\n",
      "Train set: Avg. loss: 0.000004427, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.522747 4.8749733\n",
      "l2 norm: 443.95961028960136\n",
      "l1 norm: 23.533542881684433\n",
      "Rbeta: 18.28667647211153\n",
      "Rbeta2: 15.575411513446246\n",
      "\n",
      "Train set: Avg. loss: 0.000004427, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5226294 4.8748903\n",
      "l2 norm: 443.6287476891949\n",
      "l1 norm: 23.523842639136408\n",
      "Rbeta: 18.285586422451264\n",
      "Rbeta2: 15.574237090273348\n",
      "\n",
      "Train set: Avg. loss: 0.000004426, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5225117 4.874806\n",
      "l2 norm: 443.30356701603614\n",
      "l1 norm: 23.51430981293412\n",
      "Rbeta: 18.284504388353355\n",
      "Rbeta2: 15.573070158073214\n",
      "\n",
      "Train set: Avg. loss: 0.000004426, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5223941 4.8747263\n",
      "l2 norm: 442.98363498802183\n",
      "l1 norm: 23.50492636545435\n",
      "Rbeta: 18.283445358453125\n",
      "Rbeta2: 15.571920803613354\n",
      "\n",
      "Train set: Avg. loss: 0.000004426, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5222764 4.874649\n",
      "l2 norm: 442.66763445461453\n",
      "l1 norm: 23.49565448099988\n",
      "Rbeta: 18.28242167445832\n",
      "Rbeta2: 15.57079745381537\n",
      "\n",
      "Train set: Avg. loss: 0.000004426, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5221587 4.874567\n",
      "l2 norm: 442.3559615537956\n",
      "l1 norm: 23.486495595826764\n",
      "Rbeta: 18.281405769259223\n",
      "Rbeta2: 15.569685632361669\n",
      "\n",
      "Train set: Avg. loss: 0.000004426, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5220411 4.874488\n",
      "l2 norm: 442.04853116593256\n",
      "l1 norm: 23.477454188811155\n",
      "Rbeta: 18.280408349558808\n",
      "Rbeta2: 15.56858649590815\n",
      "\n",
      "Train set: Avg. loss: 0.000004425, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5219234 4.874413\n",
      "l2 norm: 441.74601136876646\n",
      "l1 norm: 23.46854062367567\n",
      "Rbeta: 18.279409424350806\n",
      "Rbeta2: 15.567483539158742\n",
      "\n",
      "Train set: Avg. loss: 0.000004425, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5218058 4.874343\n",
      "l2 norm: 441.44616583984305\n",
      "l1 norm: 23.45968482713424\n",
      "Rbeta: 18.278424174833514\n",
      "Rbeta2: 15.566387369776757\n",
      "\n",
      "Train set: Avg. loss: 0.000004425, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5216881 4.874276\n",
      "l2 norm: 441.1511175341352\n",
      "l1 norm: 23.45095149165456\n",
      "Rbeta: 18.27742894351104\n",
      "Rbeta2: 15.565279640738156\n",
      "\n",
      "Train set: Avg. loss: 0.000004425, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5215704 4.8742123\n",
      "l2 norm: 440.8594286316806\n",
      "l1 norm: 23.442304152879018\n",
      "Rbeta: 18.276449054010428\n",
      "Rbeta2: 15.56418644004412\n",
      "\n",
      "Train set: Avg. loss: 0.000004425, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5214529 4.8741493\n",
      "l2 norm: 440.57284765195493\n",
      "l1 norm: 23.433811710024855\n",
      "Rbeta: 18.275496424478515\n",
      "Rbeta2: 15.563109631813258\n",
      "\n",
      "Train set: Avg. loss: 0.000004424, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5213352 4.874094\n",
      "l2 norm: 440.2899567197032\n",
      "l1 norm: 23.42542767246256\n",
      "Rbeta: 18.274580285552393\n",
      "Rbeta2: 15.562059958771584\n",
      "\n",
      "Train set: Avg. loss: 0.000004424, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5212176 4.874036\n",
      "l2 norm: 440.0111738522139\n",
      "l1 norm: 23.417160151600637\n",
      "Rbeta: 18.273675576544196\n",
      "Rbeta2: 15.56102531924071\n",
      "\n",
      "Train set: Avg. loss: 0.000004424, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5210999 4.8739705\n",
      "l2 norm: 439.7359732871971\n",
      "l1 norm: 23.408973575380294\n",
      "Rbeta: 18.272755389272955\n",
      "Rbeta2: 15.559978637399995\n",
      "\n",
      "Train set: Avg. loss: 0.000004424, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5209824 4.873909\n",
      "l2 norm: 439.465318090703\n",
      "l1 norm: 23.40091689162486\n",
      "Rbeta: 18.271839444729423\n",
      "Rbeta2: 15.558935868300003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000004424, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5208647 4.8738546\n",
      "l2 norm: 439.19906469396165\n",
      "l1 norm: 23.39299672087174\n",
      "Rbeta: 18.27096175734654\n",
      "Rbeta2: 15.557920054294048\n",
      "\n",
      "Train set: Avg. loss: 0.000004423, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5207472 4.873802\n",
      "l2 norm: 438.93601265252477\n",
      "l1 norm: 23.385157812525698\n",
      "Rbeta: 18.270089141047446\n",
      "Rbeta2: 15.55690277592061\n",
      "\n",
      "Train set: Avg. loss: 0.000004423, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5206295 4.873753\n",
      "l2 norm: 438.67619544142894\n",
      "l1 norm: 23.37740730336451\n",
      "Rbeta: 18.26923718055698\n",
      "Rbeta2: 15.55590454358279\n",
      "\n",
      "Train set: Avg. loss: 0.000004423, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.520512 4.873705\n",
      "l2 norm: 438.4192309263515\n",
      "l1 norm: 23.369720237789814\n",
      "Rbeta: 18.268382231304432\n",
      "Rbeta2: 15.554900303659256\n",
      "\n",
      "Train set: Avg. loss: 0.000004423, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.520382 4.8736563\n",
      "l2 norm: 438.1667992346335\n",
      "l1 norm: 23.362152486075395\n",
      "Rbeta: 18.26754072275829\n",
      "Rbeta2: 15.553876846575353\n",
      "\n",
      "Train set: Avg. loss: 0.000004423, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5202351 4.8736105\n",
      "l2 norm: 437.9173266553281\n",
      "l1 norm: 23.354637477067804\n",
      "Rbeta: 18.266691732703414\n",
      "Rbeta2: 15.552798498113065\n",
      "\n",
      "Train set: Avg. loss: 0.000004423, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5200881 4.8735704\n",
      "l2 norm: 437.67145934720827\n",
      "l1 norm: 23.347226603480657\n",
      "Rbeta: 18.26587333636015\n",
      "Rbeta2: 15.551740695710684\n",
      "\n",
      "Train set: Avg. loss: 0.000004423, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5199411 4.873534\n",
      "l2 norm: 437.4296107812367\n",
      "l1 norm: 23.33992127565468\n",
      "Rbeta: 18.265039204786444\n",
      "Rbeta2: 15.55066527387427\n",
      "\n",
      "Train set: Avg. loss: 0.000004423, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5197941 4.873499\n",
      "l2 norm: 437.1920366372897\n",
      "l1 norm: 23.332740530177386\n",
      "Rbeta: 18.264204686707146\n",
      "Rbeta2: 15.54959174003157\n",
      "\n",
      "Train set: Avg. loss: 0.000004423, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5196471 4.873463\n",
      "l2 norm: 436.95735434498187\n",
      "l1 norm: 23.3256270877809\n",
      "Rbeta: 18.263364866158685\n",
      "Rbeta2: 15.548512030496346\n",
      "\n",
      "Train set: Avg. loss: 0.000004422, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5195001 4.8734264\n",
      "l2 norm: 436.7261113070701\n",
      "l1 norm: 23.318619274026272\n",
      "Rbeta: 18.262545450299726\n",
      "Rbeta2: 15.547448671379406\n",
      "\n",
      "Train set: Avg. loss: 0.000004422, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5193533 4.8733935\n",
      "l2 norm: 436.49877030896084\n",
      "l1 norm: 23.311732965321003\n",
      "Rbeta: 18.261758279735947\n",
      "Rbeta2: 15.546411898834725\n",
      "\n",
      "Train set: Avg. loss: 0.000004422, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5192063 4.8733635\n",
      "l2 norm: 436.27576394006144\n",
      "l1 norm: 23.30497643349661\n",
      "Rbeta: 18.2609832735212\n",
      "Rbeta2: 15.545384736583939\n",
      "\n",
      "Train set: Avg. loss: 0.000004422, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5190594 4.8733373\n",
      "l2 norm: 436.05627089082\n",
      "l1 norm: 23.29831857419502\n",
      "Rbeta: 18.260223687788628\n",
      "Rbeta2: 15.544364157837494\n",
      "\n",
      "Train set: Avg. loss: 0.000004422, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5189124 4.873314\n",
      "l2 norm: 435.8387120188207\n",
      "l1 norm: 23.291710983947468\n",
      "Rbeta: 18.259480231621758\n",
      "Rbeta2: 15.543357722517582\n",
      "\n",
      "Train set: Avg. loss: 0.000004422, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5187656 4.873294\n",
      "l2 norm: 435.62321415870633\n",
      "l1 norm: 23.285157631009028\n",
      "Rbeta: 18.258758351334592\n",
      "Rbeta2: 15.54236542753658\n",
      "\n",
      "Train set: Avg. loss: 0.000004422, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5186186 4.873272\n",
      "l2 norm: 435.4109551553913\n",
      "l1 norm: 23.27869158408427\n",
      "Rbeta: 18.25803814748359\n",
      "Rbeta2: 15.541375853363721\n",
      "\n",
      "Train set: Avg. loss: 0.000004422, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5184717 4.873255\n",
      "l2 norm: 435.2020686816191\n",
      "l1 norm: 23.272313803203893\n",
      "Rbeta: 18.257310268090592\n",
      "Rbeta2: 15.54037487917388\n",
      "\n",
      "Train set: Avg. loss: 0.000004422, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5183249 4.8732386\n",
      "l2 norm: 434.99619496447\n",
      "l1 norm: 23.26603604184732\n",
      "Rbeta: 18.25661177535481\n",
      "Rbeta2: 15.539399969451095\n",
      "\n",
      "Train set: Avg. loss: 0.000004422, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.518178 4.8732257\n",
      "l2 norm: 434.79444014063324\n",
      "l1 norm: 23.259874885952645\n",
      "Rbeta: 18.255926386520382\n",
      "Rbeta2: 15.538433144896949\n",
      "\n",
      "Train set: Avg. loss: 0.000004422, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5180331 4.873217\n",
      "l2 norm: 434.59454494392276\n",
      "l1 norm: 23.253762748301973\n",
      "Rbeta: 18.255237409243275\n",
      "Rbeta2: 15.53746544340699\n",
      "\n",
      "Train set: Avg. loss: 0.000004422, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5179156 4.873212\n",
      "l2 norm: 434.39727752014244\n",
      "l1 norm: 23.24778560186557\n",
      "Rbeta: 18.254619219888866\n",
      "Rbeta2: 15.53662581735046\n",
      "\n",
      "Train set: Avg. loss: 0.000004421, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5177982 4.873206\n",
      "l2 norm: 434.20349925155836\n",
      "l1 norm: 23.241910817951606\n",
      "Rbeta: 18.2540014625326\n",
      "Rbeta2: 15.535791083887693\n",
      "\n",
      "Train set: Avg. loss: 0.000004421, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5176806 4.873198\n",
      "l2 norm: 434.0125643421943\n",
      "l1 norm: 23.236119253757256\n",
      "Rbeta: 18.253398635701156\n",
      "Rbeta2: 15.534969310257207\n",
      "\n",
      "Train set: Avg. loss: 0.000004421, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5175631 4.87319\n",
      "l2 norm: 433.8241259625958\n",
      "l1 norm: 23.230393046775447\n",
      "Rbeta: 18.252801151784585\n",
      "Rbeta2: 15.534152093013555\n",
      "\n",
      "Train set: Avg. loss: 0.000004421, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5174551 4.8731833\n",
      "l2 norm: 433.6375176995974\n",
      "l1 norm: 23.224727659262754\n",
      "Rbeta: 18.252216837899795\n",
      "Rbeta2: 15.533369734385984\n",
      "\n",
      "Train set: Avg. loss: 0.000004421, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5174551 4.8731747\n",
      "l2 norm: 433.45578907143283\n",
      "l1 norm: 23.21945401529594\n",
      "Rbeta: 18.251853203626915\n",
      "Rbeta2: 15.53306770812371\n",
      "\n",
      "Train set: Avg. loss: 0.000004420, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5174551 4.8731556\n",
      "l2 norm: 433.2775869393514\n",
      "l1 norm: 23.214283284859476\n",
      "Rbeta: 18.251482702597436\n",
      "Rbeta2: 15.532766881874089\n",
      "\n",
      "Train set: Avg. loss: 0.000004420, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5174292 4.8731327\n",
      "l2 norm: 433.1035409594907\n",
      "l1 norm: 23.20920061563695\n",
      "Rbeta: 18.251097019722963\n",
      "Rbeta2: 15.532389360860524\n",
      "\n",
      "Train set: Avg. loss: 0.000004420, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5173861 4.873103\n",
      "l2 norm: 432.9315370074266\n",
      "l1 norm: 23.204128723236117\n",
      "Rbeta: 18.250666749153105\n",
      "Rbeta2: 15.53193257680995\n",
      "\n",
      "Train set: Avg. loss: 0.000004420, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5173326 4.8730745\n",
      "l2 norm: 432.76149495961647\n",
      "l1 norm: 23.199095334266207\n",
      "Rbeta: 18.25023369281335\n",
      "Rbeta2: 15.531442837319235\n",
      "\n",
      "Train set: Avg. loss: 0.000004420, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5172777 4.873049\n",
      "l2 norm: 432.5947024492614\n",
      "l1 norm: 23.194158342742632\n",
      "Rbeta: 18.249813606995712\n",
      "Rbeta2: 15.530959857939443\n",
      "\n",
      "Train set: Avg. loss: 0.000004420, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5172198 4.873027\n",
      "l2 norm: 432.4303545285162\n",
      "l1 norm: 23.18928949846798\n",
      "Rbeta: 18.249403767158565\n",
      "Rbeta2: 15.530474444891\n",
      "\n",
      "Train set: Avg. loss: 0.000004420, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5171652 4.873001\n",
      "l2 norm: 432.2691533042528\n",
      "l1 norm: 23.18452226166847\n",
      "Rbeta: 18.248992160415497\n",
      "Rbeta2: 15.529999033559669\n",
      "\n",
      "Train set: Avg. loss: 0.000004420, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.51711 4.8729773\n",
      "l2 norm: 432.110495192729\n",
      "l1 norm: 23.1798280171372\n",
      "Rbeta: 18.248590385313648\n",
      "Rbeta2: 15.529528346488757\n",
      "\n",
      "Train set: Avg. loss: 0.000004420, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5170522 4.872957\n",
      "l2 norm: 431.9538705055229\n",
      "l1 norm: 23.175187148551366\n",
      "Rbeta: 18.248208677464287\n",
      "Rbeta2: 15.52906444591326\n",
      "\n",
      "Train set: Avg. loss: 0.000004420, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5169948 4.8729377\n",
      "l2 norm: 431.7993444505939\n",
      "l1 norm: 23.170610680718717\n",
      "Rbeta: 18.247849223386055\n",
      "Rbeta2: 15.528620174934808\n",
      "\n",
      "Train set: Avg. loss: 0.000004420, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5169386 4.872921\n",
      "l2 norm: 431.64630630008713\n",
      "l1 norm: 23.166072441207636\n",
      "Rbeta: 18.24749356912042\n",
      "Rbeta2: 15.528181088010188\n",
      "\n",
      "Train set: Avg. loss: 0.000004420, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5168828 4.8729067\n",
      "l2 norm: 431.4961130653242\n",
      "l1 norm: 23.161614503957534\n",
      "Rbeta: 18.24712656814326\n",
      "Rbeta2: 15.527730078962216\n",
      "\n",
      "Train set: Avg. loss: 0.000004419, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5168252 4.872896\n",
      "l2 norm: 431.3480626478161\n",
      "l1 norm: 23.15721472906022\n",
      "Rbeta: 18.246770344770837\n",
      "Rbeta2: 15.527281300279691\n",
      "\n",
      "Train set: Avg. loss: 0.000004419, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5167688 4.872886\n",
      "l2 norm: 431.2016851657885\n",
      "l1 norm: 23.152860408923555\n",
      "Rbeta: 18.246405376939265\n",
      "Rbeta2: 15.526828137474558\n",
      "\n",
      "Train set: Avg. loss: 0.000004419, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5167154 4.8728733\n",
      "l2 norm: 431.0578725432856\n",
      "l1 norm: 23.148584520275392\n",
      "Rbeta: 18.246038641512047\n",
      "Rbeta2: 15.526383471972089\n",
      "\n",
      "Train set: Avg. loss: 0.000004419, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.516661 4.872862\n",
      "l2 norm: 430.9158335043328\n",
      "l1 norm: 23.144360878162676\n",
      "Rbeta: 18.24568392228896\n",
      "Rbeta2: 15.525943367499517\n",
      "\n",
      "Train set: Avg. loss: 0.000004419, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5166055 4.8728523\n",
      "l2 norm: 430.7762484883921\n",
      "l1 norm: 23.14020898757518\n",
      "Rbeta: 18.24534299284426\n",
      "Rbeta2: 15.52551286086409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000004419, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5165495 4.8728447\n",
      "l2 norm: 430.6389855233302\n",
      "l1 norm: 23.136127393148538\n",
      "Rbeta: 18.245007515630615\n",
      "Rbeta2: 15.525084386783329\n",
      "\n",
      "Train set: Avg. loss: 0.000004419, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5164943 4.872836\n",
      "l2 norm: 430.50448943313046\n",
      "l1 norm: 23.132124769403486\n",
      "Rbeta: 18.244672456920064\n",
      "Rbeta2: 15.524655231679972\n",
      "\n",
      "Train set: Avg. loss: 0.000004419, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5164373 4.8728294\n",
      "l2 norm: 430.3710988515993\n",
      "l1 norm: 23.12815651294936\n",
      "Rbeta: 18.244354734864622\n",
      "Rbeta2: 15.524239039525746\n",
      "\n",
      "Train set: Avg. loss: 0.000004419, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5163785 4.87283\n",
      "l2 norm: 430.23966654657426\n",
      "l1 norm: 23.124235603696974\n",
      "Rbeta: 18.244041761693808\n",
      "Rbeta2: 15.523814671488854\n",
      "\n",
      "Train set: Avg. loss: 0.000004419, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5163199 4.8728304\n",
      "l2 norm: 430.11039532592355\n",
      "l1 norm: 23.120370582302062\n",
      "Rbeta: 18.24371596072451\n",
      "Rbeta2: 15.523379909017049\n",
      "\n",
      "Train set: Avg. loss: 0.000004419, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5162611 4.872832\n",
      "l2 norm: 429.98261849185593\n",
      "l1 norm: 23.116553231629773\n",
      "Rbeta: 18.243403534064868\n",
      "Rbeta2: 15.522954212128136\n",
      "\n",
      "Train set: Avg. loss: 0.000004419, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5162024 4.8728323\n",
      "l2 norm: 429.85741432702275\n",
      "l1 norm: 23.11281016796271\n",
      "Rbeta: 18.243084841005174\n",
      "Rbeta2: 15.522523826358738\n",
      "\n",
      "Train set: Avg. loss: 0.000004419, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5161449 4.872831\n",
      "l2 norm: 429.7334061256638\n",
      "l1 norm: 23.109088541962983\n",
      "Rbeta: 18.242756333576466\n",
      "Rbeta2: 15.522091744092686\n",
      "\n",
      "Train set: Avg. loss: 0.000004419, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5160906 4.87283\n",
      "l2 norm: 429.6112332830724\n",
      "l1 norm: 23.105434583731096\n",
      "Rbeta: 18.24245073028336\n",
      "Rbeta2: 15.521685256748933\n",
      "\n",
      "Train set: Avg. loss: 0.000004419, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5160398 4.8728237\n",
      "l2 norm: 429.4909057016509\n",
      "l1 norm: 23.1018480153543\n",
      "Rbeta: 18.24215961323611\n",
      "Rbeta2: 15.521306540501257\n",
      "\n",
      "Train set: Avg. loss: 0.000004419, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5159881 4.87282\n",
      "l2 norm: 429.3716331977334\n",
      "l1 norm: 23.09828122817734\n",
      "Rbeta: 18.241857980030026\n",
      "Rbeta2: 15.520913089695835\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5159423 4.872813\n",
      "l2 norm: 429.25550533980663\n",
      "l1 norm: 23.094818200921402\n",
      "Rbeta: 18.24154855469221\n",
      "Rbeta2: 15.52053443895078\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5158978 4.8728\n",
      "l2 norm: 429.14256243298996\n",
      "l1 norm: 23.091463612990722\n",
      "Rbeta: 18.241242310296332\n",
      "Rbeta2: 15.520163321819227\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5158528 4.872785\n",
      "l2 norm: 429.03305962961264\n",
      "l1 norm: 23.088226315634085\n",
      "Rbeta: 18.240946177357003\n",
      "Rbeta2: 15.519802782559028\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5158056 4.8727736\n",
      "l2 norm: 428.9240208469981\n",
      "l1 norm: 23.084978873334634\n",
      "Rbeta: 18.240650997578637\n",
      "Rbeta2: 15.519435423205625\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5157565 4.8727636\n",
      "l2 norm: 428.8163977414335\n",
      "l1 norm: 23.08178332448704\n",
      "Rbeta: 18.240381067743538\n",
      "Rbeta2: 15.519081691260055\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5157057 4.8727555\n",
      "l2 norm: 428.710438032221\n",
      "l1 norm: 23.07863605813993\n",
      "Rbeta: 18.240125331184835\n",
      "Rbeta2: 15.51873437325345\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5156573 4.8727446\n",
      "l2 norm: 428.6060539341911\n",
      "l1 norm: 23.075539124454686\n",
      "Rbeta: 18.23986959706333\n",
      "Rbeta2: 15.518395727967976\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5156118 4.872733\n",
      "l2 norm: 428.50348987871723\n",
      "l1 norm: 23.07249593863286\n",
      "Rbeta: 18.23961221624627\n",
      "Rbeta2: 15.518066076560425\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.515569 4.8727207\n",
      "l2 norm: 428.4026755219191\n",
      "l1 norm: 23.069504662939483\n",
      "Rbeta: 18.239362057485582\n",
      "Rbeta2: 15.51774643361007\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5155246 4.8727107\n",
      "l2 norm: 428.3036237901719\n",
      "l1 norm: 23.066560181705032\n",
      "Rbeta: 18.23910752949059\n",
      "Rbeta2: 15.51741994950063\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5154794 4.8727026\n",
      "l2 norm: 428.20525564245816\n",
      "l1 norm: 23.063623904927248\n",
      "Rbeta: 18.238851767660265\n",
      "Rbeta2: 15.517086110708854\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5154331 4.872695\n",
      "l2 norm: 428.1083197897357\n",
      "l1 norm: 23.060728038102834\n",
      "Rbeta: 18.23860392744594\n",
      "Rbeta2: 15.516756995098321\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5153855 4.8726897\n",
      "l2 norm: 428.01277616822745\n",
      "l1 norm: 23.057877424630373\n",
      "Rbeta: 18.238365906791113\n",
      "Rbeta2: 15.516431260128371\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5153403 4.8726816\n",
      "l2 norm: 427.91854710668264\n",
      "l1 norm: 23.055064390493797\n",
      "Rbeta: 18.238126527010305\n",
      "Rbeta2: 15.516112521583722\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5152925 4.872677\n",
      "l2 norm: 427.8251732697951\n",
      "l1 norm: 23.052263024731275\n",
      "Rbeta: 18.23788100484761\n",
      "Rbeta2: 15.5157794227184\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5152456 4.872674\n",
      "l2 norm: 427.7330351548262\n",
      "l1 norm: 23.04949561530304\n",
      "Rbeta: 18.237640518709053\n",
      "Rbeta2: 15.515451002305383\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5151972 4.87267\n",
      "l2 norm: 427.6419824064527\n",
      "l1 norm: 23.046759050641555\n",
      "Rbeta: 18.237396363073426\n",
      "Rbeta2: 15.515116581487243\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5151516 4.8726664\n",
      "l2 norm: 427.55216183987\n",
      "l1 norm: 23.044064250217573\n",
      "Rbeta: 18.237150149891136\n",
      "Rbeta2: 15.514787970769195\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5151064 4.872663\n",
      "l2 norm: 427.46395945662255\n",
      "l1 norm: 23.041418516004196\n",
      "Rbeta: 18.236915117020352\n",
      "Rbeta2: 15.514468576744557\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5150589 4.8726625\n",
      "l2 norm: 427.37666652246185\n",
      "l1 norm: 23.038795989884417\n",
      "Rbeta: 18.236690726602678\n",
      "Rbeta2: 15.514149568872565\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5150105 4.8726635\n",
      "l2 norm: 427.290899758603\n",
      "l1 norm: 23.03621687186472\n",
      "Rbeta: 18.236469472962185\n",
      "Rbeta2: 15.513830916313994\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5149583 4.872669\n",
      "l2 norm: 427.2058615396449\n",
      "l1 norm: 23.033645168319257\n",
      "Rbeta: 18.236245212344883\n",
      "Rbeta2: 15.513495077848006\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5149055 4.8726773\n",
      "l2 norm: 427.12159278415913\n",
      "l1 norm: 23.031090877905243\n",
      "Rbeta: 18.236016171826336\n",
      "Rbeta2: 15.513152046169917\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5148515 4.872687\n",
      "l2 norm: 427.03828083365727\n",
      "l1 norm: 23.02856049062326\n",
      "Rbeta: 18.23579736117175\n",
      "Rbeta2: 15.51281054993708\n",
      "\n",
      "Train set: Avg. loss: 0.000004418, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5147977 4.8726974\n",
      "l2 norm: 426.9562513688274\n",
      "l1 norm: 23.026072290197288\n",
      "Rbeta: 18.23559014122402\n",
      "Rbeta2: 15.51248312751357\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5147431 4.8727098\n",
      "l2 norm: 426.8748391962777\n",
      "l1 norm: 23.023596099943806\n",
      "Rbeta: 18.23538523395829\n",
      "Rbeta2: 15.512149691601467\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5146885 4.872722\n",
      "l2 norm: 426.793982863222\n",
      "l1 norm: 23.021129425282574\n",
      "Rbeta: 18.235171874837434\n",
      "Rbeta2: 15.511810224910041\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5146363 4.8727307\n",
      "l2 norm: 426.71423483005645\n",
      "l1 norm: 23.01869536142386\n",
      "Rbeta: 18.23495047602601\n",
      "Rbeta2: 15.511474905331784\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5145829 4.8727403\n",
      "l2 norm: 426.6360453263457\n",
      "l1 norm: 23.016308881242846\n",
      "Rbeta: 18.234731264399894\n",
      "Rbeta2: 15.511136595371555\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5145298 4.8727503\n",
      "l2 norm: 426.55861279216396\n",
      "l1 norm: 23.013937310573457\n",
      "Rbeta: 18.234522281436966\n",
      "Rbeta2: 15.510807331761281\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.514477 4.872759\n",
      "l2 norm: 426.48225340531064\n",
      "l1 norm: 23.01160509282272\n",
      "Rbeta: 18.234319442856286\n",
      "Rbeta2: 15.51048543865452\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5144271 4.8727636\n",
      "l2 norm: 426.4068859115913\n",
      "l1 norm: 23.009306884209618\n",
      "Rbeta: 18.234122336585315\n",
      "Rbeta2: 15.510179579186106\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5143805 4.872765\n",
      "l2 norm: 426.3338759143893\n",
      "l1 norm: 23.007088834770503\n",
      "Rbeta: 18.233920324650523\n",
      "Rbeta2: 15.509880531893344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5143332 4.8727665\n",
      "l2 norm: 426.26221841093616\n",
      "l1 norm: 23.004910184649493\n",
      "Rbeta: 18.233724044741574\n",
      "Rbeta2: 15.50958451420994\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5142863 4.8727665\n",
      "l2 norm: 426.1914839481585\n",
      "l1 norm: 23.002763901329665\n",
      "Rbeta: 18.233537865636414\n",
      "Rbeta2: 15.509302065132964\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5142385 4.872768\n",
      "l2 norm: 426.1217433758414\n",
      "l1 norm: 23.000646550270446\n",
      "Rbeta: 18.23335086434295\n",
      "Rbeta2: 15.509012094317587\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5141895 4.8727713\n",
      "l2 norm: 426.05285336939824\n",
      "l1 norm: 22.998546198554678\n",
      "Rbeta: 18.23317013237771\n",
      "Rbeta2: 15.5087224814503\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5141411 4.8727746\n",
      "l2 norm: 425.98561628653965\n",
      "l1 norm: 22.996504822322294\n",
      "Rbeta: 18.23299294708863\n",
      "Rbeta2: 15.508440565768385\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.514105 4.872778\n",
      "l2 norm: 425.9185239317359\n",
      "l1 norm: 22.994472096089304\n",
      "Rbeta: 18.23280860770423\n",
      "Rbeta2: 15.50818388912178\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5140756 4.872778\n",
      "l2 norm: 425.85261520026677\n",
      "l1 norm: 22.992491411458904\n",
      "Rbeta: 18.232638705568\n",
      "Rbeta2: 15.507958086373002\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5140463 4.8727736\n",
      "l2 norm: 425.78783962784246\n",
      "l1 norm: 22.99055035308659\n",
      "Rbeta: 18.232472920305522\n",
      "Rbeta2: 15.507744014179213\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.514017 4.872765\n",
      "l2 norm: 425.72394561290184\n",
      "l1 norm: 22.988629091921336\n",
      "Rbeta: 18.232298019244976\n",
      "Rbeta2: 15.50752140197096\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5139877 4.872756\n",
      "l2 norm: 425.66148438841367\n",
      "l1 norm: 22.986757004058955\n",
      "Rbeta: 18.232123942454677\n",
      "Rbeta2: 15.507303482146636\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5139583 4.8727446\n",
      "l2 norm: 425.6002935836284\n",
      "l1 norm: 22.984926275156944\n",
      "Rbeta: 18.231949740081003\n",
      "Rbeta2: 15.507085062713116\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.513929 4.872734\n",
      "l2 norm: 425.53955255214686\n",
      "l1 norm: 22.983105355759793\n",
      "Rbeta: 18.231773354710807\n",
      "Rbeta2: 15.50686530445303\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5138997 4.8727245\n",
      "l2 norm: 425.47964230829666\n",
      "l1 norm: 22.981308555183805\n",
      "Rbeta: 18.23160991735537\n",
      "Rbeta2: 15.506653086489361\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5138702 4.8727145\n",
      "l2 norm: 425.4210924082544\n",
      "l1 norm: 22.979549686165157\n",
      "Rbeta: 18.231429577605837\n",
      "Rbeta2: 15.506430483543651\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5138409 4.872702\n",
      "l2 norm: 425.36410355725536\n",
      "l1 norm: 22.977834304709802\n",
      "Rbeta: 18.231236704041194\n",
      "Rbeta2: 15.50619850239009\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5138116 4.872688\n",
      "l2 norm: 425.30828395470377\n",
      "l1 norm: 22.976157121514486\n",
      "Rbeta: 18.231045888928634\n",
      "Rbeta2: 15.505969037243762\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5137823 4.872673\n",
      "l2 norm: 425.2533171437557\n",
      "l1 norm: 22.974511319181584\n",
      "Rbeta: 18.23086925552637\n",
      "Rbeta2: 15.505751634888764\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5137529 4.8726606\n",
      "l2 norm: 425.19850981892796\n",
      "l1 norm: 22.97286296320278\n",
      "Rbeta: 18.23068743272481\n",
      "Rbeta2: 15.505530213026233\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5137236 4.8726473\n",
      "l2 norm: 425.14436603941726\n",
      "l1 norm: 22.97123159530419\n",
      "Rbeta: 18.230506433799043\n",
      "Rbeta2: 15.505308458687928\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5136943 4.8726335\n",
      "l2 norm: 425.0912715556311\n",
      "l1 norm: 22.969629598597983\n",
      "Rbeta: 18.230320118616465\n",
      "Rbeta2: 15.505080508991723\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.513665 4.8726196\n",
      "l2 norm: 425.03844209881413\n",
      "l1 norm: 22.96803418395448\n",
      "Rbeta: 18.230139122083465\n",
      "Rbeta2: 15.504858758443941\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5136356 4.8726063\n",
      "l2 norm: 424.9870001621817\n",
      "l1 norm: 22.966485301983383\n",
      "Rbeta: 18.22996521641735\n",
      "Rbeta2: 15.504644714197022\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5136063 4.872596\n",
      "l2 norm: 424.9363375862488\n",
      "l1 norm: 22.96498659559519\n",
      "Rbeta: 18.22985087861214\n",
      "Rbeta2: 15.504477736383045\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.513577 4.8725915\n",
      "l2 norm: 424.88644121141226\n",
      "l1 norm: 22.963520224426446\n",
      "Rbeta: 18.22977686601266\n",
      "Rbeta2: 15.504338895636524\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5135477 4.8725915\n",
      "l2 norm: 424.83714499018254\n",
      "l1 norm: 22.962072582564165\n",
      "Rbeta: 18.229703108146445\n",
      "Rbeta2: 15.5041977085831\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5135183 4.872596\n",
      "l2 norm: 424.78845996135465\n",
      "l1 norm: 22.960643099379347\n",
      "Rbeta: 18.2296394170704\n",
      "Rbeta2: 15.50406020487416\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.513489 4.8725996\n",
      "l2 norm: 424.74059483566975\n",
      "l1 norm: 22.959204975806003\n",
      "Rbeta: 18.229507425289416\n",
      "Rbeta2: 15.503866927751481\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5134597 4.8726044\n",
      "l2 norm: 424.6930248851048\n",
      "l1 norm: 22.957772741326828\n",
      "Rbeta: 18.229383062556717\n",
      "Rbeta2: 15.503674991913106\n",
      "\n",
      "Train set: Avg. loss: 0.000004417, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5134304 4.8726063\n",
      "l2 norm: 424.64665962063845\n",
      "l1 norm: 22.95638176621309\n",
      "Rbeta: 18.22925407833622\n",
      "Rbeta2: 15.503486071797605\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5134009 4.872609\n",
      "l2 norm: 424.6008754391044\n",
      "l1 norm: 22.95500681486326\n",
      "Rbeta: 18.229135317639837\n",
      "Rbeta2: 15.503303014706383\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5133716 4.872611\n",
      "l2 norm: 424.5556662453668\n",
      "l1 norm: 22.953645334034583\n",
      "Rbeta: 18.2290085182599\n",
      "Rbeta2: 15.503117111011234\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5133423 4.872614\n",
      "l2 norm: 424.51076668265057\n",
      "l1 norm: 22.952295034027912\n",
      "Rbeta: 18.228889347573112\n",
      "Rbeta2: 15.502932548275478\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5133129 4.8726163\n",
      "l2 norm: 424.46646473898016\n",
      "l1 norm: 22.95095352834768\n",
      "Rbeta: 18.22875859378198\n",
      "Rbeta2: 15.50274212458073\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5132836 4.8726206\n",
      "l2 norm: 424.4234640123865\n",
      "l1 norm: 22.9496618819106\n",
      "Rbeta: 18.228647052280916\n",
      "Rbeta2: 15.50256225245463\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5132543 4.8726244\n",
      "l2 norm: 424.3806875023555\n",
      "l1 norm: 22.948373582547863\n",
      "Rbeta: 18.22853456125856\n",
      "Rbeta2: 15.502385562630149\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.513225 4.8726296\n",
      "l2 norm: 424.3390493872516\n",
      "l1 norm: 22.94712751512766\n",
      "Rbeta: 18.22842574301032\n",
      "Rbeta2: 15.502207366249468\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5131956 4.872633\n",
      "l2 norm: 424.2976203967339\n",
      "l1 norm: 22.945881213582727\n",
      "Rbeta: 18.228310119604338\n",
      "Rbeta2: 15.502029170535923\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5131663 4.872636\n",
      "l2 norm: 424.25694949591906\n",
      "l1 norm: 22.944655861846357\n",
      "Rbeta: 18.228201585537217\n",
      "Rbeta2: 15.501855329721895\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.513137 4.8726387\n",
      "l2 norm: 424.21655180465837\n",
      "l1 norm: 22.943439993339396\n",
      "Rbeta: 18.228098779518657\n",
      "Rbeta2: 15.50168584362911\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5131077 4.8726425\n",
      "l2 norm: 424.17605470702625\n",
      "l1 norm: 22.94221417875743\n",
      "Rbeta: 18.22800005716714\n",
      "Rbeta2: 15.501519706556666\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5130783 4.872645\n",
      "l2 norm: 424.1362486277747\n",
      "l1 norm: 22.941014188589705\n",
      "Rbeta: 18.22790652364005\n",
      "Rbeta2: 15.501359263806775\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.513049 4.8726482\n",
      "l2 norm: 424.09671625023185\n",
      "l1 norm: 22.939818590444958\n",
      "Rbeta: 18.227809984997545\n",
      "Rbeta2: 15.501194467251102\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5130197 4.872651\n",
      "l2 norm: 424.05768004098167\n",
      "l1 norm: 22.938635847211103\n",
      "Rbeta: 18.22771249651092\n",
      "Rbeta2: 15.50103117806671\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5129904 4.8726554\n",
      "l2 norm: 424.0192870030211\n",
      "l1 norm: 22.937469764850036\n",
      "Rbeta: 18.227615136308952\n",
      "Rbeta2: 15.50086504228613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.512961 4.872663\n",
      "l2 norm: 423.98097607072964\n",
      "l1 norm: 22.936307009355133\n",
      "Rbeta: 18.22752239882672\n",
      "Rbeta2: 15.500702589644193\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5129317 4.87267\n",
      "l2 norm: 423.94307701426544\n",
      "l1 norm: 22.935153725186915\n",
      "Rbeta: 18.22743143376282\n",
      "Rbeta2: 15.50053829590758\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5129024 4.872677\n",
      "l2 norm: 423.9054664213847\n",
      "l1 norm: 22.934000992717326\n",
      "Rbeta: 18.227328886266918\n",
      "Rbeta2: 15.500368141761449\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.512873 4.872685\n",
      "l2 norm: 423.8682112292271\n",
      "l1 norm: 22.93286096228297\n",
      "Rbeta: 18.227231783613703\n",
      "Rbeta2: 15.50019798808295\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128437 4.8726926\n",
      "l2 norm: 423.8311427344058\n",
      "l1 norm: 22.93172121073534\n",
      "Rbeta: 18.22712841500729\n",
      "Rbeta2: 15.500024820995867\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128399 4.872697\n",
      "l2 norm: 423.7954777946594\n",
      "l1 norm: 22.930691401527465\n",
      "Rbeta: 18.22708193030724\n",
      "Rbeta2: 15.499972975031978\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128387 4.8727026\n",
      "l2 norm: 423.7606511028102\n",
      "l1 norm: 22.92969504505495\n",
      "Rbeta: 18.227053011877413\n",
      "Rbeta2: 15.499939170378532\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128393 4.8727064\n",
      "l2 norm: 423.7257610640504\n",
      "l1 norm: 22.92869012108516\n",
      "Rbeta: 18.22701210020711\n",
      "Rbeta2: 15.49990366379141\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128399 4.8727093\n",
      "l2 norm: 423.691396330208\n",
      "l1 norm: 22.92770172039744\n",
      "Rbeta: 18.226968466598944\n",
      "Rbeta2: 15.499864809203363\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128387 4.8727155\n",
      "l2 norm: 423.6574712769118\n",
      "l1 norm: 22.926722484802365\n",
      "Rbeta: 18.2269409097551\n",
      "Rbeta2: 15.49983267917199\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128385 4.872721\n",
      "l2 norm: 423.6236715112448\n",
      "l1 norm: 22.92574283535259\n",
      "Rbeta: 18.226906264950628\n",
      "Rbeta2: 15.499799028440112\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128381 4.872725\n",
      "l2 norm: 423.5909856099954\n",
      "l1 norm: 22.92480038823596\n",
      "Rbeta: 18.226873264078435\n",
      "Rbeta2: 15.499768057246326\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128387 4.8727264\n",
      "l2 norm: 423.5590027102266\n",
      "l1 norm: 22.923878587726268\n",
      "Rbeta: 18.226826909176392\n",
      "Rbeta2: 15.49972752937132\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128393 4.872727\n",
      "l2 norm: 423.52781854107764\n",
      "l1 norm: 22.922982367900293\n",
      "Rbeta: 18.22678533131091\n",
      "Rbeta2: 15.499692861997294\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128417 4.872727\n",
      "l2 norm: 423.49681406151734\n",
      "l1 norm: 22.922091684065066\n",
      "Rbeta: 18.226742520887036\n",
      "Rbeta2: 15.499662687566788\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128423 4.872727\n",
      "l2 norm: 423.46663005272137\n",
      "l1 norm: 22.92122851056471\n",
      "Rbeta: 18.22670312633524\n",
      "Rbeta2: 15.499631034241613\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128437 4.8727264\n",
      "l2 norm: 423.4367087523551\n",
      "l1 norm: 22.920369219492912\n",
      "Rbeta: 18.226658005136255\n",
      "Rbeta2: 15.499596186245805\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128452 4.872727\n",
      "l2 norm: 423.4069425341473\n",
      "l1 norm: 22.91951699801497\n",
      "Rbeta: 18.226617378250033\n",
      "Rbeta2: 15.499566193413369\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128467 4.8727264\n",
      "l2 norm: 423.37807441577263\n",
      "l1 norm: 22.918689749615023\n",
      "Rbeta: 18.22656830270445\n",
      "Rbeta2: 15.499526825261423\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128473 4.872726\n",
      "l2 norm: 423.3491632378351\n",
      "l1 norm: 22.91786117344542\n",
      "Rbeta: 18.226531091787002\n",
      "Rbeta2: 15.499498186222231\n",
      "\n",
      "Train set: Avg. loss: 0.000004416, Accuracy: 128/128 (100%)\n",
      "\n",
      "1.5128479 4.872725\n",
      "l2 norm: 423.3205920800908\n",
      "l1 norm: 22.91704143679286\n",
      "Rbeta: 18.226499607726478\n",
      "Rbeta2: 15.499473900532319\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-87dfdedcde17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-dbfe0f9486c4>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(k, T, Cout)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUinit_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVinit_torch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mRbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrbetavals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mell1s\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mrbetas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mbetas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-46bf24234d01>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(ker_size1, ker_size2, output_channels, initialization)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start training:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mlossv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f258a9e1a11c>\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(network, optimizer)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run experiments \n",
    "T = 3\n",
    "Cout =[1,2,4,8]\n",
    "\n",
    "for k in [1, 3, 8, 16, 28]:\n",
    "    run_experiment(k, T, Cout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "solar-continent",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-5f8da24706d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Write betas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mrbetavals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrbetavals_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Write data to a CSV\n",
    "import pandas as pd\n",
    "    \n",
    "# Write rbetas \n",
    "name =  \"experiments-data/\" + str(k) + \"rbeta\" + str(Cout) + \".csv\"\n",
    "pd.DataFrame(rbetas).to_csv(name, header=False, index=False)\n",
    "\n",
    "# Write betas\n",
    "for i in range(len(pairs)):\n",
    "    beta = betas[i]\n",
    "    losses = losses_all[i]\n",
    "    rbetavals = rbetavals_all[i]\n",
    "    ell1s = ell1s_all[i]\n",
    "    print(str(pairs[i]))\n",
    "    name = \"experiments-data/\" + str(pairs[i]) + \".csv\"\n",
    "    pd.DataFrame(beta).to_csv(name, header=False, index=False)\n",
    "    name = \"experiments-data/\" + \"loss\" + str(pairs[i]) + \".csv\"\n",
    "    pd.DataFrame(losses).to_csv(name, header=False, index=False)\n",
    "    name = \"experiments-data/\" + \"rbetas\" + str(pairs[i]) + \".csv\"\n",
    "    pd.DataFrame(rbetavals).to_csv(name, header=False, index=False)\n",
    "    name = \"experiments-data/\" + \"ell1s\" + str(pairs[i]) + \".csv\"\n",
    "    pd.DataFrame(ell1s).to_csv(name, header=False, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "unavailable-edmonton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 1)\n",
      "(28, 3)\n",
      "(28, 5)\n",
      "(28, 7)\n"
     ]
    }
   ],
   "source": [
    "# Write data to a CSV\n",
    "import pandas as pd\n",
    "    \n",
    "# Write rbetas \n",
    "name =  \"experiments-data/\" + str(k) + \"rbeta\" + str(Cout) + \".csv\"\n",
    "pd.DataFrame(rbetas).to_csv(name, header=False, index=False)\n",
    "\n",
    "# Write betas, losses, and ell1s\n",
    "for i in range(len(pairs)):\n",
    "    beta = betas[i]\n",
    "#     losses = losses_all[i]\n",
    "    rbetavals = rbetavals_all[i]\n",
    "    ell1s = ell1s_all[i]\n",
    "    print(str(pairs[i]))\n",
    "    name = \"experiments-data/\" + str(pairs[i]) + \".csv\"\n",
    "    pd.DataFrame(beta).to_csv(name, header=False, index=False)\n",
    "#     name = \"experiments-data/\" + \"loss\" + str(pairs[i]) + \".csv\"\n",
    "#     pd.DataFrame(losses).to_csv(name, header=False, index=False)\n",
    "    name = \"experiments-data/\" + \"rbetavals\" + str(pairs[i]) + \".csv\"\n",
    "    pd.DataFrame(rbetavals).to_csv(name, header=False, index=False)\n",
    "    name = \"experiments-data/\" + \"ell1s\" + str(pairs[i]) + \".csv\"\n",
    "    pd.DataFrame(ell1s).to_csv(name, header=False, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mounted-aviation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "meaningful-hepatitis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6596820989735943, 2.6352740575188918, 2.591987834198255, 2.584869061040482]\n"
     ]
    }
   ],
   "source": [
    "print(rbetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-typing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
